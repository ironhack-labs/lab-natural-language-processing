{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1000 non-null   object\n",
      " 1   label   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "def clean_html(raw_html: str) -> str:\n",
    "    if not isinstance(raw_html, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Parse HTML safely\n",
    "    soup = BeautifulSoup(raw_html, \"lxml\")\n",
    "\n",
    "    # Get visible text only (removes <script>, <style>, comments, tags)\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove prefixed 'b' (from byte strings like b'hello')\n",
    "    text = re.sub(r\"^b\\s+\", \"\", text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "\n",
    "    # Remove all single characters (isolated letters)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r\"^[a-zA-Z]\\s+\", \"\", text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_html: str) -> str:\n",
    "    # Step 1: Strip HTML/JS/CSS\n",
    "    text = clean_html(raw_html)\n",
    "\n",
    "    # Step 2: Normalize plain text\n",
    "    text = normalize_text(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying final processing to X_train and X_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anirudh Unni\\AppData\\Local\\Temp\\ipykernel_17932\\1703161904.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(raw_html, \"lxml\")\n",
      "C:\\Users\\Anirudh Unni\\AppData\\Local\\Temp\\ipykernel_17932\\1703161904.py:10: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(raw_html, \"lxml\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "\n",
      "Example of processed text:\n",
      "dear friend propos surpris person contact howev sincer seek confid transact propos person transpar honesti high calib let first start introduc proper name ron sinclear person assist haitian presid apolog infring privaci may interest know former presid haiti fight serious war unit democraci quit year back along foreign power countri quit long ago american govern order presid jean bertrand arist leav haiti forc power rabel forc know fulli well capabl america power member unit nation presid arist decid left seat power exil south africa countri seek asylum new govern place light sad happen deposit made secur compani europ year may took place still power presid haiti aid person assist ron sinclear loyalist suceed secret move sum usd nine million five hunder thousand unit state dollar privat secur compani vault europ need servic high reliabl foreign receiv fund bank account futur surviv famili arist present oper foreign bank account name fund us usd aros various money receiv sale diamond deal presid charl taylor liberia abound great quantiti around coastal area liberia decid abl assist us shall compens commiss fund money shall set asid take care local foreign expens might incur along line famili remain fund might also interest note secur arrang regard transact put place therefor noth whatsoev fear worri everi arrang co handl former person assist ron sinclear present europ netherland possibl conclus arrang part share famili also invest lucrat busi courtesi full advic assist connect importat part transact indic interest assist famili pleas kind send us telephon fax number discuss clarif expect receiv urgent respons us proceed detail pend transact\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def process_text_final(raw_html: str) -> str:\n",
    "    \n",
    "    #  Use the clean_html function\n",
    "    text = clean_html(raw_html)\n",
    "\n",
    "    # Use the normalize_text function\n",
    "    text = normalize_text(text)\n",
    "\n",
    "    # Tokenize, remove stopwords, and stem\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [snowball.stem(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return \" \".join(stemmed_tokens)\n",
    "    \n",
    "\n",
    "print(\"Applying final processing to X_train and X_test...\")\n",
    "X_train_processed = X_train.apply(process_text_final)\n",
    "X_test_processed = X_test.apply(process_text_final)\n",
    "print(\"Processing complete!\")\n",
    "print(\"\\nExample of processed text:\")\n",
    "print(X_train_processed.iloc[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying lemmatization to X_train and X_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anirudh Unni\\AppData\\Local\\Temp\\ipykernel_17932\\1703161904.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(raw_html, \"lxml\")\n",
      "C:\\Users\\Anirudh Unni\\AppData\\Local\\Temp\\ipykernel_17932\\1703161904.py:10: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(raw_html, \"lxml\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization complete!\n",
      "\n",
      "Example of lemmatized text:\n",
      "dear friend proposal surprising personal contact however sincerely seek confidence transaction propose person transparency honesty high caliber let first start introducing properly name ron sinclear personal assistance haitian president apologize infringed privacy may interest know former president haiti fighting serious war united democracy quite year backed along foreign powerful country quite long ago american government ordered president jean bertrand aristed leave haiti forced power rabel force knowing fully well capability america powerful member united nation president aristed decided left seat power exile south africa country seek asylum new government place light sad happening deposit made security company europe year may took place still power president haiti aid personal assistant ron sinclear loyalist suceeded secretely move sum usd nine million five hundered thousand united state dollar private security company vault europe need service highly reliable foreigner receive fund bank account future survival family aristed presently operate foreign bank account name fund u usd arose various money received sale diamond deal president charles taylor liberia abound great quantity around coastal area liberia decided able assist u shall compensated commission fund money shall set aside take care local foreign expense might incurred along line family remaining fund might also interest note security arrangement regarding transaction put place therefore nothing whatsoever fear worry every arrangement co handled former personal assistant ron sinclear presentely europe netherlands possible conclusion arrangement part share family also invested lucrative business courtesy full advice assistance connection importat part transaction indicate interest assist family please kindly send u telephone fax number discussion clarification expecting receive urgent response u proceed detail pending transaction\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(raw_html: str) -> str:\n",
    "    \n",
    "    # Step 1: Use your clean_html function\n",
    "    text = clean_html(raw_html)\n",
    "\n",
    "    # Step 2: Use your normalize_text function\n",
    "    text = normalize_text(text)\n",
    "\n",
    "    # Step 3: Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Step 4: Remove stopwords and lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Step 5: Join tokens back into a string\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# --- Apply the new lemmatization function to your data ---\n",
    "print(\"Applying lemmatization to X_train and X_test...\")\n",
    "# We will use this lemmatized version for the rest of the notebook\n",
    "X_train_processed = X_train.apply(lemmatize_text)\n",
    "X_test_processed = X_test.apply(lemmatize_text)\n",
    "print(\"Lemmatization complete!\")\n",
    "\n",
    "# Display an example to verify the output\n",
    "print(\"\\nExample of lemmatized text:\")\n",
    "print(X_train_processed.iloc[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in HAM messages\n",
      "[('state', 116), ('pm', 97), ('would', 93), ('president', 89), ('mr', 89), ('time', 81), ('percent', 80), ('obama', 77), ('call', 74), ('secretary', 74)]\n",
      "Top 10 words in SPAM messages\n",
      "[('money', 847), ('account', 742), ('bank', 645), ('u', 631), ('fund', 626), ('e', 510), ('transaction', 471), ('business', 424), ('mr', 423), ('country', 422)]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from collections import Counter\n",
    "\n",
    "# Combine the processed text with its label to do this analysis\n",
    "train_df = pd.concat([X_train_processed, y_train], axis=1)\n",
    "\n",
    "# Separate ham (0) and spam (1) messages\n",
    "ham_messages = train_df[train_df['label'] == 0]['text']\n",
    "spam_messages = train_df[train_df['label'] == 1]['text']\n",
    "\n",
    "# Create a counter for ham words by joining all messages into one string\n",
    "ham_word_counts = Counter(\" \".join(ham_messages).split())\n",
    "\n",
    "# Create a counter for spam words\n",
    "spam_word_counts = Counter(\" \".join(spam_messages).split())\n",
    "\n",
    "print(\"Top 10 words in HAM messages\")\n",
    "print(ham_word_counts.most_common(10))\n",
    "\n",
    "print(\"Top 10 words in SPAM messages\")\n",
    "print(spam_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF training data: (800, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "# Initialize the vectorizer to find the top 5000 word features\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit on the training data and transform both train and test data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_processed)\n",
    "X_test_tfidf = vectorizer.transform(X_test_processed)\n",
    "\n",
    "print(f\"Shape of the TF-IDF training data: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Model with TF-IDF only ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       125\n",
      "           1       0.97      0.97      0.97        75\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.98      0.98      0.98       200\n",
      "weighted avg       0.98      0.98      0.98       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Using TF-IDF\n",
    "print(\"--- Training Model with TF-IDF only ---\")\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
