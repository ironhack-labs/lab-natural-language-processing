{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zk/jxl4jn3s41b8q_1hhx9_8k940000gn/T/ipykernel_3202/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "# Load the train and test datasets\n",
    "train_data = pd.read_csv('data/kg_train.csv')\n",
    "test_data = pd.read_csv('data/kg_test.csv')\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "train_data = train_data.head(1000)\n",
    "print(train_data.shape)\n",
    "train_data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "test_data = test_data.head(1000)\n",
    "print(test_data.shape)\n",
    "test_data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels from training data\n",
    "X_train = train_data['text']\n",
    "y_train = train_data['label']\n",
    "\n",
    "# Test data only has the text column\n",
    "X_test = test_data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean HTML content\n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean HTML content and preprocess text\n",
    "def clean_html_and_text(text):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove all special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\s*[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victorg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean HTML content and preprocess text\n",
    "def clean_html_and_text(text):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove all special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\s*[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_html_and_text(text):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove all special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\s*[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Lemmatize words\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the HTML content and preprocess the text columns\n",
    "train_data['text'] = train_data['text'].apply(clean_html_and_text)\n",
    "test_data['text'] = test_data['text'].apply(clean_html_and_text)\n",
    "\n",
    "# Remove rows with empty text after preprocessing\n",
    "train_data = train_data[train_data['text'].str.strip() != '']\n",
    "test_data = test_data[test_data['text'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (983, 2)\n",
      "Test Data Shape: (987, 1)\n",
      "Train Data Sample:\n",
      "                                                text  label\n",
      "0  dear sir strictly private business proposal mi...      1\n",
      "2  noracheryl emailed dozen memo haiti weekend pl...      0\n",
      "3  dear sirfmadamc know proposal might surprise e...      1\n",
      "4                                                fyi      0\n",
      "5  sure bottom line need special security code ge...      0\n",
      "Test Data Sample:\n",
      "                                                text\n",
      "0  usiness fact deceased man foreigner authorized...\n",
      "1  happy adjust afternoon going suggest pm start ...\n",
      "2  lael brainard confirmed afternoonmiguel rodrig...\n",
      "3      friday march amsbwhoeop rei extended congrats\n",
      "4  dear good friendi happy inform successin getti...\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about the datasets\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "print(\"Train Data Sample:\")\n",
    "print(train_data.head())\n",
    "print(\"Test Data Sample:\")\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for 'ham' messages after preprocessing.\n",
      "No data available for 'spam' messages after preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Check if categories contain any data\n",
    "if ham_words.empty:\n",
    "    print(\"No data available for 'ham' messages after preprocessing.\")\n",
    "else:\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    ham_bow = count_vectorizer.fit_transform(ham_words)\n",
    "    ham_word_counts = pd.DataFrame({\n",
    "        'word': count_vectorizer.get_feature_names_out(),\n",
    "        'count': ham_bow.sum(axis=0).A1\n",
    "    }).sort_values(by='count', ascending=False).head(10)\n",
    "    print(\"Top 10 words in ham messages:\")\n",
    "    print(ham_word_counts)\n",
    "\n",
    "if spam_words.empty:\n",
    "    print(\"No data available for 'spam' messages after preprocessing.\")\n",
    "else:\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    spam_bow = count_vectorizer.fit_transform(spam_words)\n",
    "    spam_word_counts = pd.DataFrame({\n",
    "        'word': count_vectorizer.get_feature_names_out(),\n",
    "        'count': spam_bow.sum(axis=0).A1\n",
    "    }).sort_values(by='count', ascending=False).head(10)\n",
    "    print(\"Top 10 words in spam messages:\")\n",
    "    print(spam_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>noracheryl emailed dozen memo haiti weekend pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear sirfmadamc know proposal might surprise e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sure bottom line need special security code ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  money_mark  \\\n",
       "0  dear sir strictly private business proposal mi...      1           1   \n",
       "2  noracheryl emailed dozen memo haiti weekend pl...      0           1   \n",
       "3  dear sirfmadamc know proposal might surprise e...      1           1   \n",
       "4                                                fyi      0           1   \n",
       "5  sure bottom line need special security code ge...      0           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1      1498  \n",
       "2                 0       110  \n",
       "3                 1      1378  \n",
       "4                 0         3  \n",
       "5                 0       311  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "train_data['money_mark'] = train_data['text'].str.contains(money_simbol_list)*1\n",
    "train_data['suspicious_words'] = train_data['text'].str.contains(suspicious_words)*1\n",
    "train_data['text_len'] = train_data['text'].apply(lambda x: len(x)) \n",
    "\n",
    "test_data['money_mark'] = test_data['text'].str.contains(money_simbol_list)*1\n",
    "test_data['suspicious_words'] = test_data['text'].str.contains(suspicious_words)*1\n",
    "test_data['text_len'] = test_data['text'].apply(lambda x: len(x)) \n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m spam_words \u001b[38;5;241m=\u001b[39m train_data[train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspam\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m----> 6\u001b[0m ham_bow \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(ham_words)\n\u001b[1;32m      7\u001b[0m spam_bow \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(spam_words)\n\u001b[1;32m      9\u001b[0m ham_word_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m: count_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out(),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m: ham_bow\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mA1\n\u001b[1;32m     12\u001b[0m })\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1297\u001b[0m         )\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Exploratory Data Analysis: Get top 10 words in ham and spam messages\n",
    "ham_words = train_data[train_data['label'] == 'ham']['text']\n",
    "spam_words = train_data[train_data['label'] == 'spam']['text']\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "ham_bow = count_vectorizer.fit_transform(ham_words)\n",
    "spam_bow = count_vectorizer.fit_transform(spam_words)\n",
    "\n",
    "ham_word_counts = pd.DataFrame({\n",
    "    'word': count_vectorizer.get_feature_names_out(),\n",
    "    'count': ham_bow.sum(axis=0).A1\n",
    "}).sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "spam_word_counts = pd.DataFrame({\n",
    "    'word': count_vectorizer.get_feature_names_out(),\n",
    "    'count': spam_bow.sum(axis=0).A1\n",
    "}).sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "print(\"Top 10 words in ham messages:\")\n",
    "print(ham_word_counts)\n",
    "\n",
    "print(\"Top 10 words in spam messages:\")\n",
    "print(spam_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data into numerical features using TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "# Initialize a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the training set for validation purposes\n",
    "train_predictions = model.predict(X_train_tfidf)\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Predictions:\n",
      "[1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0\n",
      " 1 0 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 0 0\n",
      " 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 0 0\n",
      " 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0\n",
      " 0 0 0 0 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0\n",
      " 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1\n",
      " 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0\n",
      " 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1\n",
      " 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1\n",
      " 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1\n",
      " 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0\n",
      " 1 1 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
      " 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1\n",
      " 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
      " 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 0 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 0\n",
      " 0 1 0 0 1 0 0 1 1 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0\n",
      " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0\n",
      " 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(X_test_tfidf)\n",
    "print(\"Test Predictions:\")\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
