{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cw/hgl5p7pd36gfh8z1d954r9bw0000gn/T/ipykernel_60556/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/data/kg_train.csv'\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"/data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "data_train = train_data.copy()\n",
    "data_test = test_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marinacastilloariza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(raw_html):\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    return str(soup)\n",
    "\n",
    "def remove_html_comments(cleaned_html):\n",
    "    cleaned_html = re.sub(r'<!--.*?-->', '', cleaned_html, flags=re.DOTALL)\n",
    "    \n",
    "    return cleaned_html\n",
    "\n",
    "def remove_html_tags(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cw/hgl5p7pd36gfh8z1d954r9bw0000gn/T/ipykernel_60556/1538888025.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(raw_html, \"html.parser\")\n",
      "/var/folders/cw/hgl5p7pd36gfh8z1d954r9bw0000gn/T/ipykernel_60556/1538888025.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(raw_html, \"html.parser\")\n",
      "/var/folders/cw/hgl5p7pd36gfh8z1d954r9bw0000gn/T/ipykernel_60556/1538888025.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n",
      "/var/folders/cw/hgl5p7pd36gfh8z1d954r9bw0000gn/T/ipykernel_60556/1538888025.py:13: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) \n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    text = re.sub(r'^\\b\\w\\b\\s*', '', text) \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    text = re.sub(r'^\\b(b)', '', text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['cleaned_html'] = data['text'].apply(clean_html)\n",
    "data['cleaned_html'] = data['cleaned_html'].apply(remove_html_comments)\n",
    "data['cleaned_text'] = data['cleaned_html'].apply(remove_html_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Removing specific prefixes, like a prefixed 'b', is not a universal standard procedure in natural language processing (NLP). Instead, the decision to remove such prefixes depends on the specific context of the data being processed and the goals of the analysis. Here are some points to consider regarding the standard practices in NLP preprocessing:\n",
    "\n",
    "Informal Language: In datasets containing informal language, such as text messages, tweets, or user-generated content, practitioners often choose to clean up non-standard usages (like slang or prefixes) to maintain the integrity of the analysis.\n",
    "Formal Language: In more structured datasets (e.g., news articles or academic texts), such prefixes are less likely to appear, making this cleaning step unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                   preprocessed_text  \\\n",
      "0  dear sir strictly private business proposal am...   \n",
      "1                                            will do   \n",
      "2  noracheryl has emailed dozens of memos about h...   \n",
      "3  dear sirfmadamc know that this proposal might ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                          final_text  \n",
      "0  dear sir strictly private business proposal mi...  \n",
      "1                                                     \n",
      "2  noracheryl emailed dozens memos haiti weekend ...  \n",
      "3  dear sirfmadamc know proposal might surprise e...  \n",
      "4                                                fyi  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marinacastilloariza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "data['preprocessed_text'] = data['cleaned_text'].apply(clean_text)\n",
    "nltk.download('stopwords')  # Ensure that stopwords are downloaded\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['final_text'] = data['preprocessed_text'].apply(\n",
    "    lambda text: ' '.join(word for word in text.split() if word not in stop_words)\n",
    ")\n",
    "print(data[['text', 'preprocessed_text', 'final_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marinacastilloariza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                   preprocessed_text  \\\n",
      "0  dear sir strictly private business proposal am...   \n",
      "1                                            will do   \n",
      "2  noracheryl has emailed dozens of memos about h...   \n",
      "3  dear sirfmadamc know that this proposal might ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                          final_text  \\\n",
      "0  dear sir strictly private business proposal mi...   \n",
      "1                                                      \n",
      "2  noracheryl emailed dozens memos haiti weekend ...   \n",
      "3  dear sirfmadamc know proposal might surprise e...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        stemmed_text  \\\n",
      "0  dear sir strictli privat busi propos mike chuk...   \n",
      "1                                                      \n",
      "2  noracheryl email dozen memo haiti weekend plea...   \n",
      "3  dear sirfmadamc know propos might surpris emer...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  dear sir strictly private business proposal mi...  \n",
      "1                                                     \n",
      "2  noracheryl emailed dozen memo haiti weekend pl...  \n",
      "3  dear sirfmadamc know proposal might surprise e...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['stemmed_text'] = data['final_text'].apply(\n",
    "    lambda text: ' '.join(porter_stemmer.stem(word) for word in text.split())\n",
    ")\n",
    "nltk.download('wordnet') \n",
    "data['lemmatized_text'] = data['final_text'].apply(\n",
    "    lambda text: ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    ")\n",
    "print(data[['text', 'preprocessed_text', 'final_text', 'stemmed_text', 'lemmatized_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words in Ham Messages:\n",
      "           count\n",
      "pm           115\n",
      "us           115\n",
      "would        106\n",
      "president     86\n",
      "state         82\n",
      "percent       77\n",
      "call          75\n",
      "work          67\n",
      "mr            66\n",
      "obama         62\n",
      "\n",
      "Top 10 Words in Spam Messages:\n",
      "             count\n",
      "money          920\n",
      "account        741\n",
      "bank           725\n",
      "us             550\n",
      "business       474\n",
      "fund           429\n",
      "transaction    409\n",
      "transfer       392\n",
      "country        367\n",
      "million        366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ham_messages = data[data['label'] == 0]\n",
    "spam_messages = data[data['label'] == 1]\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "ham_word_counts = vectorizer.fit_transform(ham_messages['final_text'])\n",
    "ham_word_sum = ham_word_counts.sum(axis=0)\n",
    "ham_word_freq = pd.DataFrame(ham_word_sum.T, index=vectorizer.get_feature_names_out(), columns=['count']).sort_values(by='count', ascending=False)\n",
    "top_10_ham_words = ham_word_freq.head(10)\n",
    "\n",
    "spam_word_counts = vectorizer.fit_transform(spam_messages['final_text'])\n",
    "spam_word_sum = spam_word_counts.sum(axis=0)\n",
    "spam_word_freq = pd.DataFrame(spam_word_sum.T, index=vectorizer.get_feature_names_out(), columns=['count']).sort_values(by='count', ascending=False)\n",
    "top_10_spam_words = spam_word_freq.head(10)\n",
    "\n",
    "print(\"Top 10 Words in Ham Messages:\")\n",
    "print(top_10_ham_words)\n",
    "\n",
    "print(\"\\nTop 10 Words in Spam Messages:\")\n",
    "print(top_10_spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['preprocessed_text'] = data_train['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
      "535  I have not been able to reach oscar this am. W...      0   \n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
      "557  I can have it announced here on Monday - can't...      0   \n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
      "\n",
      "                                     preprocessed_text  money_mark  \\\n",
      "29   regards mr nelson smithkindly reply me on my p...           1   \n",
      "535  have not been able to reach oscar this am we a...           1   \n",
      "695  huma abedin bim checking with pat on the will ...           1   \n",
      "557    can have it announced here on monday cant today           1   \n",
      "836  bank of africaagence san pedro bp san pedro co...           1   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "29                  0        85  \n",
      "535                 0        91  \n",
      "695                 0       128  \n",
      "557                 0        47  \n",
      "836                 1      1569  \n"
     ]
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_symbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "# Adding money mark and suspicious words\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_symbol_list) * 1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words) * 1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Bag of Words matrix: (800, 18350)\n",
      "Feature names: ['aac' 'aaclocated' 'aae' 'aag' 'aaronovitchon' 'abacha' 'abachabefore'\n",
      " 'abachac' 'abachace' 'abachaco']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_train_array = X_train_counts.toarray()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Shape of the Bag of Words matrix:\", X_train_array.shape)  # (num_samples, num_features)\n",
    "print(\"Feature names:\", feature_names[:10])  # Display the first 10 words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF matrix: (800, 18350)\n",
      "Feature names: ['aac' 'aaclocated' 'aae' 'aag' 'aaronovitchon' 'abacha' 'abachabefore'\n",
      " 'abachac' 'abachace' 'abachaco']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_train_tfidf_array = X_train_tfidf.toarray()\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"Shape of the TF-IDF matrix:\", X_train_tfidf_array.shape)\n",
    "print(\"Feature names:\", feature_names[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.74      0.85       125\n",
      "           1       0.70      1.00      0.82        75\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.85      0.87      0.84       200\n",
      "weighted avg       0.89      0.84      0.84       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[93 32]\n",
      " [ 0 75]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "def preprocess_text(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    return cleaned_text\n",
    "data['preprocessed_text'] = data['text'].apply(preprocess_text)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['preprocessed_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['preprocessed_text'])\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, train_data['label'])\n",
    "predictions = classifier.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(test_data['label'], predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_data['label'], predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_data['label'], predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
