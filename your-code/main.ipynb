{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/q0js_gcd29lfhsshk693bdyw0000gn/T/ipykernel_53329/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n",
      "label\n",
      "0    558\n",
      "1    442\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "print(data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                           Will do.      0\n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                fyi      0\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strip_html(text):\n",
    "    text = re.sub(r'(?is)<(script|style).*?>.*?(</\\1>)', '', str(text))\n",
    "    text = re.sub(r'(?is)<!--.*?-->', '', text)\n",
    "    text = re.sub(r'(?s)<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\s+', '', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)      # Keep only letters and spaces\n",
    "    text = re.sub(r'\\b\\w\\b', ' ', text)           # Remove single characters\n",
    "    text = re.sub(r'\\s+', ' ', text)              # Replace multiple spaces with one\n",
    "    text = text.strip().lower()                   # Trim and lowercase\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patrikbobcsok/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join(word for word in text.split() if word.lower() not in STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sizes: 790 790\n"
     ]
    }
   ],
   "source": [
    "def preprocess_series(series: pd.Series) -> pd.Series:\n",
    "    return (series.astype(str)\n",
    "            .map(strip_html)\n",
    "            .map(clean_text)\n",
    "            .map(remove_stopwords)\n",
    "            .map(lemmatize_text))\n",
    "\n",
    "X_train_cleaned = preprocess_series(X_train)\n",
    "X_test_cleaned  = preprocess_series(X_test)\n",
    "\n",
    "mask_tr = X_train_cleaned.str.strip().astype(bool)\n",
    "X_train_cleaned = X_train_cleaned[mask_tr]\n",
    "y_train = y_train[mask_tr]\n",
    "\n",
    "mask_te = X_test_cleaned.str.strip().astype(bool)\n",
    "X_test_cleaned = X_test_cleaned[mask_te]\n",
    "y_test = y_test[mask_te]\n",
    "\n",
    "print(\"Train sizes:\", len(X_train_cleaned), len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 HAM words:\n",
      "              word  count\n",
      "948           fyi     40\n",
      "3425  unfavorable     27\n",
      "2701      soffice     27\n",
      "3488          vol     26\n",
      "2319      refused     26\n",
      "3274   thrdenough     25\n",
      "1194          hvn     24\n",
      "2809        state     23\n",
      "1841           na     22\n",
      "160   amsecretary     21 \n",
      "\n",
      "Top 10 SPAM words:\n",
      "           word  count\n",
      "15415     nbsp    360\n",
      "4023       com    106\n",
      "14915       mr     88\n",
      "25423    yahoo     74\n",
      "22181        u     73\n",
      "9848      http     64\n",
      "24920      www     55\n",
      "4855   dearsir     49\n",
      "14620  million     46\n",
      "2030        aw     45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "ham_texts  = X_train_cleaned[y_train == 0]\n",
    "spam_texts = X_train_cleaned[y_train == 1]\n",
    "\n",
    "# Safety again\n",
    "ham_texts  = ham_texts[ham_texts.str.strip().astype(bool)]\n",
    "spam_texts = spam_texts[spam_texts.str.strip().astype(bool)]\n",
    "\n",
    "# Vectorize (letters only)\n",
    "vec_ham  = CountVectorizer(token_pattern=r'(?u)\\b[a-zA-Z]+\\b')\n",
    "vec_spam = CountVectorizer(token_pattern=r'(?u)\\b[a-zA-Z]+\\b')\n",
    "\n",
    "ham_bow  = vec_ham.fit_transform(ham_texts)\n",
    "spam_bow = vec_spam.fit_transform(spam_texts)\n",
    "\n",
    "ham_top = (pd.DataFrame({'word': vec_ham.get_feature_names_out(),\n",
    "                         'count': ham_bow.toarray().sum(axis=0)})\n",
    "           .sort_values('count', ascending=False).head(10))\n",
    "spam_top = (pd.DataFrame({'word': vec_spam.get_feature_names_out(),\n",
    "                          'count': spam_bow.toarray().sum(axis=0)})\n",
    "            .sort_values('count', ascending=False).head(10))\n",
    "\n",
    "print(\"Top 10 HAM words:\\n\", ham_top, \"\\n\")\n",
    "print(\"Top 10 SPAM words:\\n\", spam_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  money_mark  \\\n",
      "29   regard mrnelsonsmith kindlyreplymeonmyprivatee...           0   \n",
      "535  ihavenotbeenabletoreachoscarthisam wearesuppos...           0   \n",
      "695  humaabedinb mcheckingwithpatonthe kwillworkwit...           0   \n",
      "557             icanhaveitannouncedhereonmonday ttoday           0   \n",
      "836  bankofafricaagencesanpedro bp sanpedro coted i...           1   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "29                  0         6  \n",
      "535                 0         3  \n",
      "695                 0         5  \n",
      "557                 0         2  \n",
      "836                 1        60  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/q0js_gcd29lfhsshk693bdyw0000gn/T/ipykernel_53329/1790663120.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  money_mark_train = X_train_cleaned.str.contains(money_symbol_list, regex=True).astype(int)\n",
      "/var/folders/xx/q0js_gcd29lfhsshk693bdyw0000gn/T/ipykernel_53329/1790663120.py:7: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  suspicious_train = X_train_cleaned.str.contains(suspicious_words, regex=True).astype(int)\n",
      "/var/folders/xx/q0js_gcd29lfhsshk693bdyw0000gn/T/ipykernel_53329/1790663120.py:10: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  money_mark_test = X_test_cleaned.str.contains(money_symbol_list, regex=True).astype(int)\n",
      "/var/folders/xx/q0js_gcd29lfhsshk693bdyw0000gn/T/ipykernel_53329/1790663120.py:11: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  suspicious_test = X_test_cleaned.str.contains(suspicious_words, regex=True).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_symbol_list = r\"(euro|dollar|pound|€|£|\\$)\"\n",
    "\n",
    "suspicious_words  = r\"(free|cheap|sex|money|account|bank|fund|transfer|transaction|win|deposit|password)\"\n",
    "\n",
    "money_mark_train = X_train_cleaned.str.contains(money_symbol_list, regex=True).astype(int)\n",
    "suspicious_train = X_train_cleaned.str.contains(suspicious_words, regex=True).astype(int)\n",
    "text_len_train   = X_train_cleaned.apply(lambda x: len(x.split()))\n",
    "\n",
    "money_mark_test = X_test_cleaned.str.contains(money_symbol_list, regex=True).astype(int)\n",
    "suspicious_test = X_test_cleaned.str.contains(suspicious_words, regex=True).astype(int)\n",
    "text_len_test   = X_test_cleaned.apply(lambda x: len(x.split()))\n",
    "\n",
    "print(pd.DataFrame({\n",
    "    'text': X_train_cleaned.head(),\n",
    "    'money_mark': money_mark_train.head(),\n",
    "    'suspicious_words': suspicious_train.head(),\n",
    "    'text_len': text_len_train.head()\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30499\n",
      "   aa  aaa  aabeiawaeaambiqaceqedeqh  aac  aacute  aacw  aae  \\\n",
      "0   0    0                         0    0       0     0    0   \n",
      "1   0    0                         0    0       0     0    0   \n",
      "2   0    0                         0    0       0     0    0   \n",
      "3   0    0                         0    0       0     0    0   \n",
      "4   0    0                         0    0       0     0    0   \n",
      "\n",
      "   aaecaxeebsexbhjbuqdhcrmimoeifekrobhbcsmzuvavynlrchyknoel  aaegmdbsch  aaeh  \\\n",
      "0                                                  0                  0     0   \n",
      "1                                                  0                  0     0   \n",
      "2                                                  0                  0     0   \n",
      "3                                                  0                  0     0   \n",
      "4                                                  0                  0     0   \n",
      "\n",
      "   ...  zzwh  zzwqgb  zzwqgdg  zzwqgyw  zzwx  zzxh  zzxmsihdoawxlig  zzz  \\\n",
      "0  ...     0       0        0        0     0     0                0    0   \n",
      "1  ...     0       0        0        0     0     0                0    0   \n",
      "2  ...     0       0        0        0     0     0                0    0   \n",
      "3  ...     0       0        0        0     0     0                0    0   \n",
      "4  ...     0       0        0        0     0     0                0    0   \n",
      "\n",
      "   zzzahbxntxe  zzzj  \n",
      "0            0     0  \n",
      "1            0     0  \n",
      "2            0     0  \n",
      "3            0     0  \n",
      "4            0     0  \n",
      "\n",
      "[5 rows x 30499 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit on training set and transform it into BoW matrix\n",
    "X_train_bow = vectorizer.fit_transform(X_train_cleaned)\n",
    "\n",
    "# Convert sparse matrix to DataFrame for readability\n",
    "bow_df = pd.DataFrame(X_train_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"Vocabulary size:\", len(vectorizer.get_feature_names_out()))\n",
    "print(bow_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (790, 30499)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X_train_cleaned)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9346733668341709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       124\n",
      "           1       0.97      0.85      0.91        75\n",
      "\n",
      "    accuracy                           0.93       199\n",
      "   macro avg       0.94      0.92      0.93       199\n",
      "weighted avg       0.94      0.93      0.93       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_cleaned)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_cleaned)\n",
    "\n",
    "# 2. Train classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 3. Predict on test\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# 4. Evaluate\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW ONLY — accuracy: 0.9698492462311558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.98      0.98       124\n",
      "        spam       0.96      0.96      0.96        75\n",
      "\n",
      "    accuracy                           0.97       199\n",
      "   macro avg       0.97      0.97      0.97       199\n",
      "weighted avg       0.97      0.97      0.97       199\n",
      "\n",
      "\n",
      "Top SPAM words:\n",
      "nbsp             score=5.229\n",
      "yahoo            score=3.658\n",
      "dearsir          score=3.253\n",
      "million          score=3.191\n",
      "aw               score=3.169\n",
      "ig               score=3.004\n",
      "af               score=2.978\n",
      "bestregards      score=2.978\n",
      "iammr            score=2.896\n",
      "usd              score=2.896\n",
      "\n",
      "Top HAM words:\n",
      "fyi              score=-4.373\n",
      "soffice          score=-3.992\n",
      "unfavorable      score=-3.992\n",
      "refused          score=-3.955\n",
      "vol              score=-3.955\n",
      "thrdenough       score=-3.918\n",
      "hvn              score=-3.878\n",
      "amsecretary      score=-3.751\n",
      "pmsecretary      score=-3.704\n",
      "nov              score=-3.368\n",
      "TF-IDF ONLY — accuracy: 0.964824120603015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      0.98      0.97       124\n",
      "        spam       0.97      0.93      0.95        75\n",
      "\n",
      "    accuracy                           0.96       199\n",
      "   macro avg       0.97      0.96      0.96       199\n",
      "weighted avg       0.96      0.96      0.96       199\n",
      "\n",
      "\n",
      "Top SPAM words:\n",
      "nbsp             score=2.529\n",
      "yahoo            score=1.885\n",
      "dearsir          score=1.611\n",
      "million          score=1.505\n",
      "com              score=1.467\n",
      "bestregards      score=1.410\n",
      "mr               score=1.402\n",
      "iammr            score=1.373\n",
      "http             score=1.310\n",
      "usd              score=1.291\n",
      "\n",
      "Top HAM words:\n",
      "fyi              score=-3.588\n",
      "ok               score=-2.137\n",
      "state            score=-1.693\n",
      "yes              score=-1.675\n",
      "thx              score=-1.663\n",
      "gov              score=-1.586\n",
      "releaseinpartb   score=-1.521\n",
      "plsprint         score=-1.490\n",
      "willdo           score=-1.461\n",
      "pm               score=-1.375\n",
      "BoW + FLAGS — accuracy: 0.48743718592964824\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.18      0.30       124\n",
      "        spam       0.42      1.00      0.60        75\n",
      "\n",
      "    accuracy                           0.49       199\n",
      "   macro avg       0.71      0.59      0.45       199\n",
      "weighted avg       0.78      0.49      0.41       199\n",
      "\n",
      "\n",
      "Top SPAM words:\n",
      "nbsp             score=4.909\n",
      "yahoo            score=3.337\n",
      "dearsir          score=2.932\n",
      "million          score=2.870\n",
      "aw               score=2.849\n",
      "ig               score=2.684\n",
      "bestregards      score=2.658\n",
      "af               score=2.658\n",
      "usd              score=2.575\n",
      "iammr            score=2.575\n",
      "\n",
      "Top HAM words:\n",
      "fyi              score=-4.694\n",
      "soffice          score=-4.312\n",
      "unfavorable      score=-4.312\n",
      "refused          score=-4.276\n",
      "vol              score=-4.276\n",
      "thrdenough       score=-4.238\n",
      "hvn              score=-4.199\n",
      "amsecretary      score=-4.071\n",
      "pmsecretary      score=-4.025\n",
      "nov              score=-3.688\n",
      "TF-IDF + FLAGS — accuracy: 0.4371859296482412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.10      0.18       124\n",
      "        spam       0.40      1.00      0.57        75\n",
      "\n",
      "    accuracy                           0.44       199\n",
      "   macro avg       0.70      0.55      0.37       199\n",
      "weighted avg       0.77      0.44      0.33       199\n",
      "\n",
      "\n",
      "Top SPAM words:\n",
      "money_mark       score=2.491\n",
      "nbsp             score=1.900\n",
      "suspicious_words  score=1.455\n",
      "text_len         score=1.411\n",
      "yahoo            score=1.255\n",
      "dearsir          score=0.981\n",
      "million          score=0.875\n",
      "com              score=0.838\n",
      "bestregards      score=0.780\n",
      "mr               score=0.773\n",
      "\n",
      "Top HAM words:\n",
      "fyi              score=-4.218\n",
      "ok               score=-2.766\n",
      "state            score=-2.323\n",
      "yes              score=-2.305\n",
      "thx              score=-2.292\n",
      "gov              score=-2.216\n",
      "releaseinpartb   score=-2.151\n",
      "plsprint         score=-2.120\n",
      "willdo           score=-2.091\n",
      "pm               score=-2.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/q0js_gcd29lfhsshk693bdyw0000gn/T/ipykernel_53329/4069116629.py:55: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  money_tr = X_train_cleaned.str.contains(money_pat, flags=re.I, regex=True).astype(int).values.reshape(-1,1)\n",
      "/var/folders/xx/q0js_gcd29lfhsshk693bdyw0000gn/T/ipykernel_53329/4069116629.py:56: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  suspi_tr = X_train_cleaned.str.contains(suspi_pat, flags=re.I, regex=True).astype(int).values.reshape(-1,1)\n",
      "/var/folders/xx/q0js_gcd29lfhsshk693bdyw0000gn/T/ipykernel_53329/4069116629.py:60: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  money_te = X_test_cleaned.str.contains(money_pat, flags=re.I, regex=True).astype(int).values.reshape(-1,1)\n",
      "/var/folders/xx/q0js_gcd29lfhsshk693bdyw0000gn/T/ipykernel_53329/4069116629.py:61: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  suspi_te = X_test_cleaned.str.contains(suspi_pat, flags=re.I, regex=True).astype(int).values.reshape(-1,1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to show top words for SPAM and HAM classes\n",
    "def show_top_words(model, feature_names, top_k=10):\n",
    "    scores = model.feature_log_prob_[1] - model.feature_log_prob_[0]\n",
    "    idx_spam = np.argsort(scores)[::-1][:top_k]\n",
    "    idx_ham  = np.argsort(scores)[:top_k]\n",
    "\n",
    "    print(\"\\nTop SPAM words:\")\n",
    "    for i in idx_spam:\n",
    "        print(f\"{feature_names[i]:<15}  score={scores[i]:.3f}\")\n",
    "\n",
    "    print(\"\\nTop HAM words:\")\n",
    "    for i in idx_ham:\n",
    "        print(f\"{feature_names[i]:<15}  score={scores[i]:.3f}\")\n",
    "\n",
    "# Train BoW (default params)\n",
    "bow = CountVectorizer(token_pattern=r\"\\b[a-zA-Z]{2,}\\b\")\n",
    "Xtr_bow = bow.fit_transform(X_train_cleaned)\n",
    "Xte_bow = bow.transform(X_test_cleaned)\n",
    "\n",
    "# Train NB (default params)\n",
    "nb_bow = MultinomialNB()\n",
    "nb_bow.fit(Xtr_bow, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "pred_bow = nb_bow.predict(Xte_bow)\n",
    "print(\"BoW ONLY — accuracy:\", accuracy_score(y_test, pred_bow))\n",
    "print(classification_report(y_test, pred_bow, target_names=[\"ham\",\"spam\"]))\n",
    "\n",
    "# Show top words\n",
    "show_top_words(nb_bow, bow.get_feature_names_out(), top_k=10)\n",
    "\n",
    "# Train TF-IDF\n",
    "tfidf = TfidfVectorizer(token_pattern=r\"\\b[a-zA-Z]{2,}\\b\")\n",
    "Xtr_tfidf = tfidf.fit_transform(X_train_cleaned)\n",
    "Xte_tfidf = tfidf.transform(X_test_cleaned)\n",
    "\n",
    "# Train NB on TF-IDF\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(Xtr_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "pred_tfidf = nb_tfidf.predict(Xte_tfidf)\n",
    "print(\"TF-IDF ONLY — accuracy:\", accuracy_score(y_test, pred_tfidf))\n",
    "print(classification_report(y_test, pred_tfidf, target_names=[\"ham\",\"spam\"]))\n",
    "\n",
    "show_top_words(nb_tfidf, tfidf.get_feature_names_out(), top_k=10)\n",
    "\n",
    "# Add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_pat = r\"(euro|dollar|pound|€|£|\\$)\"\n",
    "suspi_pat = r\"(free|cheap|sex|money|account|bank|fund|transfer|transaction|win|deposit|password)\"\n",
    "\n",
    "# Train flags\n",
    "money_tr = X_train_cleaned.str.contains(money_pat, flags=re.I, regex=True).astype(int).values.reshape(-1,1)\n",
    "suspi_tr = X_train_cleaned.str.contains(suspi_pat, flags=re.I, regex=True).astype(int).values.reshape(-1,1)\n",
    "len_tr   = np.array([len(t.split()) for t in X_train_cleaned]).reshape(-1,1)\n",
    "\n",
    "# Test flags\n",
    "money_te = X_test_cleaned.str.contains(money_pat, flags=re.I, regex=True).astype(int).values.reshape(-1,1)\n",
    "suspi_te = X_test_cleaned.str.contains(suspi_pat, flags=re.I, regex=True).astype(int).values.reshape(-1,1)\n",
    "len_te   = np.array([len(t.split()) for t in X_test_cleaned]).reshape(-1,1)\n",
    "\n",
    "# Convert to sparse cols so we can stack with BoW/TF-IDF\n",
    "def col(x): return csr_matrix(x)\n",
    "extra_names = [\"money_mark\",\"suspicious_words\",\"text_len\"]\n",
    "\n",
    "# Stack BoW with flags\n",
    "Xtr_bow_extra = hstack([Xtr_bow, col(money_tr), col(suspi_tr), col(len_tr)])\n",
    "Xte_bow_extra = hstack([Xte_bow, col(money_te), col(suspi_te), col(len_te)])\n",
    "bow_extra_names = list(bow.get_feature_names_out()) + extra_names\n",
    "\n",
    "# Train NB on BoW + flags\n",
    "nb_bow_extra = MultinomialNB()\n",
    "nb_bow_extra.fit(Xtr_bow_extra, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "pred_bow_extra = nb_bow_extra.predict(Xte_bow_extra)\n",
    "print(\"BoW + FLAGS — accuracy:\", accuracy_score(y_test, pred_bow_extra))\n",
    "print(classification_report(y_test, pred_bow_extra, target_names=[\"ham\",\"spam\"]))\n",
    "\n",
    "show_top_words(nb_bow_extra, np.array(bow_extra_names), top_k=10)\n",
    "\n",
    "# Stack TF-IDF with flags\n",
    "Xtr_tfidf_extra = hstack([Xtr_tfidf, col(money_tr), col(suspi_tr), col(len_tr)])\n",
    "Xte_tfidf_extra = hstack([Xte_tfidf, col(money_te), col(suspi_te), col(len_te)])\n",
    "tfidf_extra_names = list(tfidf.get_feature_names_out()) + extra_names\n",
    "\n",
    "# Train NB on TF-IDF + flags\n",
    "nb_tfidf_extra = MultinomialNB()\n",
    "nb_tfidf_extra.fit(Xtr_tfidf_extra, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "pred_tfidf_extra = nb_tfidf_extra.predict(Xte_tfidf_extra)\n",
    "print(\"TF-IDF + FLAGS — accuracy:\", accuracy_score(y_test, pred_tfidf_extra))\n",
    "print(classification_report(y_test, pred_tfidf_extra, target_names=[\"ham\",\"spam\"]))\n",
    "\n",
    "show_top_words(nb_tfidf_extra, np.array(tfidf_extra_names), top_k=10)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
