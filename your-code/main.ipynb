{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)          \n",
    "# stratify=y = Keeps the spam/ham ratio the same in both sets\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove inline JavaScript/CSS\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_inline_js_css(text):\n",
    "    # Remove <script> ... </script> blocks (inline JavaScript)\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Remove <style> ... </style> blocks (inline CSS)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply to training and test data\n",
    "X_train_clean = X_train.apply(remove_inline_js_css)\n",
    "X_test_clean = X_test.apply(remove_inline_js_css)\n",
    "\n",
    "# removing  html comments\n",
    "\n",
    "def remove_html_comments(text):\n",
    "    # Remove HTML comments <!-- ... -->\n",
    "    return re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "\n",
    "# Apply to train and test sets *after* removing inline JS/CSS\n",
    "X_train_clean = X_train_clean.apply(remove_html_comments)\n",
    "X_test_clean = X_test_clean.apply(remove_html_comments)\n",
    "\n",
    "\n",
    "# we can remove the remaining tags\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    # Remove any remaining HTML tags like <div>, <p>, <a>, etc.\n",
    "    return re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "# Apply to training and test sets\n",
    "X_train_clean = X_train_clean.apply(remove_html_tags)\n",
    "X_test_clean = X_test_clean.apply(remove_html_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all special characters\n",
    "\n",
    "def remove_special_char(text):\n",
    "    # Keep only letters, numbers, and spaces\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "X_train_clean = X_train_clean.apply(remove_special_char)\n",
    "X_test_clean = X_test_clean.apply(remove_special_char)\n",
    "\n",
    "# Remove numbers\n",
    "\n",
    "def remove_num(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "X_train_clean = X_train_clean.apply(remove_num)\n",
    "X_test_clean = X_test_clean.apply(remove_num)\n",
    "\n",
    "# Remove all single characters\n",
    "\n",
    "def remove_single_char(text):\n",
    "    # \\b means word boundary, [a-zA-Z0-9] means any letter or digit, and \\b again ensures it's alone\n",
    "    return re.sub(r'\\b\\w\\b', '', text)\n",
    "X_train_clean = X_train_clean.apply(remove_single_char)\n",
    "X_test_clean = X_test_clean.apply(remove_single_char)\n",
    "\n",
    "# Remove single characters from the start\n",
    "\n",
    "def remove_single_char_start(text):\n",
    "    return re.sub(r'^\\w\\s+', '', text)\n",
    "X_train_clean = X_train_clean.apply(remove_single_char_start)\n",
    "X_test_clean = X_test_clean.apply(remove_single_char_start)\n",
    "\n",
    "# Substitute multiple spaces with single space\n",
    "\n",
    "def remove_multi_space(text):\n",
    "    return re.sub(r'\\s+', ' ' ,text)\n",
    "X_train_clean = X_train_clean.apply(remove_multi_space)\n",
    "X_test_clean = X_test_clean.apply(remove_multi_space)\n",
    "\n",
    "# Remove prefixed 'b'\n",
    "\n",
    "def remove_prefixed_b(text):\n",
    "    return re.sub(r\"^b['\\\"]\", '', text)\n",
    "X_train_clean = X_train_clean.apply(remove_prefixed_b)\n",
    "X_test_clean = X_test_clean.apply(remove_prefixed_b)\n",
    "\n",
    "# Convert to Lowercase\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "X_train_clean = X_train_clean.apply(to_lowercase)\n",
    "X_test_clean = X_test_clean.apply(to_lowercase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenization\n",
    "def do_tokenization(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "X_train_clean = X_train_clean.apply(do_tokenization)\n",
    "X_test_clean = X_test_clean.apply(do_tokenization)\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "X_train_clean = X_train_clean.apply(remove_stopwords)\n",
    "X_test_clean = X_test_clean.apply(remove_stopwords)\n",
    "X_train_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Mapping POS tag to first character lemmatize() accepts\"\"\"\n",
    "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
    "    if treebank_tag.startswith('V'): return wordnet.VERB\n",
    "    if treebank_tag.startswith('N'): return wordnet.NOUN\n",
    "    if treebank_tag.startswith('R'): return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    if not tokens: \n",
    "        return tokens\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return [lemmatizer.lemmatize(tok, get_wordnet_pos(pos)) for tok, pos in pos_tags]\n",
    "\n",
    "\n",
    "# Lemmatize\n",
    "X_train_lem = X_train_clean.apply(lemmatize_tokens)\n",
    "X_test_lem  = X_test_clean.apply(lemmatize_tokens)\n",
    "\n",
    "# Join back to strings for vectorizers/models\n",
    "X_train_ready = X_train_lem.apply(lambda toks: \" \".join(toks))\n",
    "X_test_ready  = X_test_lem.apply(lambda toks: \" \".join(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer(min_df=2, ngram_range=(1,1))\n",
    "\n",
    "X_train_bow = bow.fit_transform(X_train_ready)  # learn vocab on train and transform\n",
    "X_test_bow  = bow.transform(X_test_ready)       # transform test with same vocab\n",
    "\n",
    "print(X_train_bow.shape, X_test_bow.shape)      # (n_docs, n_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer(min_df=2, ngram_range=(1,1))\n",
    "\n",
    "X_train_bow = bow.fit_transform(X_train_ready)  # learn vocab on train and transform\n",
    "X_test_bow  = bow.transform(X_test_ready)       # transform test with same vocab\n",
    "\n",
    "print(X_train_bow.shape, X_test_bow.shape)      # (n_docs, n_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Load the vectorizer\n",
    "tfidf = TfidfVectorizer(min_df=2, ngram_range=(1,1))  # tweak min_df/ngrams as needed\n",
    "\n",
    "# 2. Vectorize all dataset\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_ready)  # learn vocab + transform training set\n",
    "X_test_tfidf  = tfidf.transform(X_test_ready)       # transform test set with same vocab\n",
    "\n",
    "# 3. Print shapes\n",
    "print(\"TF-IDF Train shape:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF Test shape:\", X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
