{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>IPython display is working!</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Test display\n",
    "display(HTML(\"<b>IPython display is working!</b>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(r\"C:\\Users\\User\\Desktop\\AI_Lab\\WK4-Labs\\lab-natural-language-processing\\data\\kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1000, 2)\n",
      "Training set size: 800\n",
      "Test set size: 200\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\n",
    "    r\"C:\\Users\\User\\Desktop\\AI_Lab\\WK4-Labs\\lab-natural-language-processing\\data\\kg_train.csv\",\n",
    "    encoding='latin-1'\n",
    ")\n",
    "\n",
    "# Reduce the training set to speed up development\n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "data.fillna(\"\", inplace=True)\n",
    "\n",
    "# Assuming 'EmailText' is the column containing the message and 'Label' the target\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Divide into training and test partitions (e.g., 80% train / 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800\n",
      "Test set size: 200\n"
     ]
    }
   ],
   "source": [
    "# Example: Suppose the columns are actually 'text' and 'label'\n",
    "X = data['text']     \n",
    "y = data['label']       \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World & welcome!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "# Patterns\n",
    "SCRIPT_STYLE_RE = re.compile(r\"(?is)<(script|style)[^>]*>.*?</\\1>\", re.MULTILINE)\n",
    "COMMENTS_RE     = re.compile(r\"(?is)<!--.*?-->\", re.MULTILINE)\n",
    "TAGS_RE         = re.compile(r\"(?is)<[^>]+>\")\n",
    "\n",
    "def clean_html_regex(html: str) -> str:\n",
    "    # 1) Remove inline JS/CSS\n",
    "    s = SCRIPT_STYLE_RE.sub(\" \", html)\n",
    "    # 2) Remove comments\n",
    "    s = COMMENTS_RE.sub(\" \", s)\n",
    "    # 3) Remove remaining tags\n",
    "    s = TAGS_RE.sub(\" \", s)\n",
    "    # 4) Unescape & normalize\n",
    "    s = unescape(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# --- Define demo HTML ---\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<style>.red {color: red;}</style>\n",
    "<script>alert('hi');</script>\n",
    "</head>\n",
    "<body>\n",
    "<!-- comment with > tricky char -->\n",
    "<p>Hello <b>World</b> &amp; welcome!</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Test\n",
    "print(clean_html_regex(html_doc))\n",
    "# Output: \"Hello World & welcome!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN : \"b'This is a TEST! With #hashtags, numbers 123 and single letters a I t.'\"\n",
      "OUT: this is test with hashtags numbers and single letters \n",
      "\n",
      "IN : b'Bytes input: Hello!! 42 times... a b c?'\n",
      "OUT: bytes input hello times \n",
      "\n",
      "IN : '  x  Leading single letters should go.  '\n",
      "OUT: leading single letters should go \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\"\n",
    "    Clean text by applying (in order):\n",
    "      1) Remove special characters\n",
    "      2) Remove numbers\n",
    "      3) Remove all single characters\n",
    "      4) Remove single characters from the start\n",
    "      5) Substitute multiple spaces with a single space\n",
    "      6) Remove prefixed 'b' (from byte-string representations)\n",
    "      7) Convert to lowercase\n",
    "    \"\"\"\n",
    "    # Ensure string (handle bytes safely)\n",
    "    if isinstance(s, bytes):\n",
    "        s = s.decode('utf-8', errors='ignore')\n",
    "    else:\n",
    "        s = str(s)\n",
    "\n",
    "    # 6) Remove prefixed 'b' if present like b'...'\n",
    "    #    (Do this early so quotes don't interfere with later steps)\n",
    "    #    Matches:  b'...content...'  or  b\"...content...\"\n",
    "    m = re.match(r\"^\\s*b([\\\"'])(.*)\\1\\s*$\", s)\n",
    "    if m:\n",
    "        s = m.group(2)\n",
    "\n",
    "    # 7) Convert to lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    # 1) Remove special characters (keep letters and spaces only)\n",
    "    #    Note: If you want to keep punctuation, adjust this.\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "\n",
    "    # 2) Remove numbers (already excluded above, but keep explicitly)\n",
    "    s = re.sub(r\"\\d+\", \" \", s)\n",
    "\n",
    "    # 3) Remove all single characters (isolated letters like 'a', 'I')\n",
    "    s = re.sub(r\"\\b[a-z]\\b\", \" \", s)\n",
    "\n",
    "    # 4) Remove single characters from the start of the string\n",
    "    s = re.sub(r\"^\\s*\\b[a-z]\\b\\s*\", \" \", s)\n",
    "\n",
    "    # 5) Substitute multiple spaces with single space\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "# ---- Demo ----\n",
    "examples = [\n",
    "    \"b'This is a TEST! With #hashtags, numbers 123 and single letters a I t.'\",\n",
    "    b\"Bytes input: Hello!! 42 times... a b c?\",\n",
    "    \"  x  Leading single letters should go.  \",\n",
    "]\n",
    "for e in examples:\n",
    "    print(\"IN :\", repr(e))\n",
    "    print(\"OUT:\", clean_text(e), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: this is an example sentence showing off the stop words filtration\n",
      "No stopwords: example sentence showing stop words filtration\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords once\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stopwords into a set for fast lookup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove English stopwords from a given text string.\n",
    "    Assumes the text is already lowercased and cleaned.\n",
    "    \"\"\"\n",
    "    # Tokenize into words (split on whitespace)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Keep words not in stopwords\n",
    "    filtered_words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    # Join back into a string\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Clean the text first\n",
    "data[\"clean_text\"] = data[\"text\"].apply(clean_text)  # Or clean_html_regex if HTML cleaning\n",
    "\n",
    "\n",
    "# --- Demo ---\n",
    "example = \"this is an example sentence showing off the stop words filtration\"\n",
    "print(\"Original:\", example)\n",
    "print(\"No stopwords:\", remove_stopwords(example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# POS tagger (name changed in newer NLTK)\n",
    "# Try both so it works across versions:\n",
    "for pkg in ['averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng']:\n",
    "    try:\n",
    "        nltk.download(pkg)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not download {pkg}: {e}\")\n",
    "\n",
    "# Lemmatizer data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat be run faster than the dog .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "def _ensure_nltk_data():\n",
    "    needed = [\n",
    "        ('tokenizers/punkt', 'punkt'),\n",
    "        ('corpora/wordnet', 'wordnet'),\n",
    "        ('corpora/omw-1.4', 'omw-1.4'),\n",
    "    ]\n",
    "    # POS tagger: handle both names across NLTK versions\n",
    "    pos_candidates = [\n",
    "        ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),\n",
    "        ('taggers/averaged_perceptron_tagger_eng', 'averaged_perceptron_tagger_eng'),\n",
    "    ]\n",
    "    for path, pkg in needed + pos_candidates:\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "        except LookupError:\n",
    "            nltk.download(pkg, quiet=True)\n",
    "\n",
    "_ensure_nltk_data()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def _wn_pos(tag):\n",
    "    if tag.startswith('J'): return wordnet.ADJ\n",
    "    if tag.startswith('V'): return wordnet.VERB\n",
    "    if tag.startswith('N'): return wordnet.NOUN\n",
    "    if tag.startswith('R'): return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmas = [lemmatizer.lemmatize(w, _wn_pos(t)) for w, t in tagged]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Test\n",
    "print(lemmatize_text(\"The cats are running faster than the dogs.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in HAM messages:\n",
      "the             1773\n",
      "to              1065\n",
      "and             833\n",
      "of              791\n",
      "in              616\n",
      "that            414\n",
      "is              385\n",
      "for             369\n",
      "on              329\n",
      "you             311\n",
      "\n",
      "Top 10 words in SPAM messages:\n",
      "the             7046\n",
      "to              5593\n",
      "of              4984\n",
      "and             3985\n",
      "in              3289\n",
      "you             3229\n",
      "this            2675\n",
      "my              2143\n",
      "your            2078\n",
      "for             2030\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# --- Assumes you have a DataFrame `data`\n",
    "# Replace these with your actual column names:\n",
    "text_col = \"text\"   # column with the message text\n",
    "label_col = \"label\"      # column with 'ham'/'spam' or 0/1\n",
    "\n",
    "# Simple tokenizer (lowercase, keep only letters)\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b[a-z]{2,}\\b', text)  # only words with >=2 letters\n",
    "    return words\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_texts = data[data[label_col].isin([\"ham\", 0])][text_col].apply(tokenize)\n",
    "spam_texts = data[data[label_col].isin([\"spam\", 1])][text_col].apply(tokenize)\n",
    "\n",
    "# Flatten token lists\n",
    "ham_words = [word for tokens in ham_texts for word in tokens]\n",
    "spam_words = [word for tokens in spam_texts for word in tokens]\n",
    "\n",
    "# Count and get top 10\n",
    "top_ham = Counter(ham_words).most_common(10)\n",
    "top_spam = Counter(spam_words).most_common(10)\n",
    "\n",
    "# Display\n",
    "print(\"Top 10 words in HAM messages:\")\n",
    "for word, freq in top_ham:\n",
    "    print(f\"{word:15s} {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 words in SPAM messages:\")\n",
    "for word, freq in top_spam:\n",
    "    print(f\"{word:15s} {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0  Dear=2C Good day hope fine=2Cdear am writting ...      1   \n",
      "1  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...      1   \n",
      "2                                           Will do.      0   \n",
      "3  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...      1   \n",
      "4  Dear Friend, My name is LOI C.ESTRADA,The wife...      1   \n",
      "\n",
      "                                          clean_text  \\\n",
      "0  dear good day hope fine cdear am writting this...   \n",
      "1  from mr henry kaborethe chief auditor incharge...   \n",
      "2                                            will do   \n",
      "3  from the desk of dr adamu ismalerauditing and ...   \n",
      "4  dear friend my name is loi estrada the wife of...   \n",
      "\n",
      "                                   preprocessed_text  money_mark  \\\n",
      "0  dearc good day hope finecdear am writting this...           1   \n",
      "1  from mr henry kaborethe chief auditor incharge...           0   \n",
      "2                                            will do           0   \n",
      "3  from the desk of dradamu ismalerauditing and a...           1   \n",
      "4  dear friend my name is loi cestradathe wife of...           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 1      1558  \n",
      "1                 1      2936  \n",
      "2                 0         7  \n",
      "3                 1       497  \n",
      "4                 1      2050  \n"
     ]
    }
   ],
   "source": [
    "# Example cleaning function\n",
    "def clean_text_basic(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters/numbers\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# 1. Create 'preprocessed_text' before splitting\n",
    "data[\"preprocessed_text\"] = data[\"text\"].apply(clean_text_basic)\n",
    "\n",
    "# 2. Split into train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_val = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=data['label']  # replace with your actual target column\n",
    ")\n",
    "\n",
    "data_train = data_train.reset_index(drop=True)\n",
    "data_val = data_val.reset_index(drop=True)\n",
    "\n",
    "# 3. Add extra features\n",
    "money_simbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\n",
    "    \"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\",\n",
    "    \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"\n",
    "])\n",
    "\n",
    "for df in [data_train, data_val]:\n",
    "    df['money_mark'] = df['preprocessed_text'].str.contains(money_simbol_list, case=False, regex=True) * 1\n",
    "    df['suspicious_words'] = df['preprocessed_text'].str.contains(suspicious_words, case=False, regex=True) * 1\n",
    "    df['text_len'] = df['preprocessed_text'].apply(len)\n",
    "\n",
    "print(data_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear good day hope fine cdear am writting this...</td>\n",
       "      <td>dearc good day hope finecdear am writting this...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>1</td>\n",
       "      <td>from mr henry kaborethe chief auditor incharge...</td>\n",
       "      <td>from mr henry kaborethe chief auditor incharge...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>will do</td>\n",
       "      <td>will do</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>1</td>\n",
       "      <td>from the desk of dr adamu ismalerauditing and ...</td>\n",
       "      <td>from the desk of dradamu ismalerauditing and a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear friend my name is loi estrada the wife of...</td>\n",
       "      <td>dear friend my name is loi cestradathe wife of...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Dear=2C Good day hope fine=2Cdear am writting ...      1   \n",
       "1  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...      1   \n",
       "2                                           Will do.      0   \n",
       "3  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...      1   \n",
       "4  Dear Friend, My name is LOI C.ESTRADA,The wife...      1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  dear good day hope fine cdear am writting this...   \n",
       "1  from mr henry kaborethe chief auditor incharge...   \n",
       "2                                            will do   \n",
       "3  from the desk of dr adamu ismalerauditing and ...   \n",
       "4  dear friend my name is loi estrada the wife of...   \n",
       "\n",
       "                                   preprocessed_text  money_mark  \\\n",
       "0  dearc good day hope finecdear am writting this...           1   \n",
       "1  from mr henry kaborethe chief auditor incharge...           0   \n",
       "2                                            will do           0   \n",
       "3  from the desk of dradamu ismalerauditing and a...           1   \n",
       "4  dear friend my name is loi cestradathe wife of...           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1      1558  \n",
       "1                 1      2936  \n",
       "2                 0         7  \n",
       "3                 1       497  \n",
       "4                 1      2050  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   preprocessed_text  label  money_mark  \\\n",
      "0  dearc good day hope finecdear am writting this...      1           1   \n",
      "1  from mr henry kaborethe chief auditor incharge...      1           0   \n",
      "2                                            will do      0           0   \n",
      "3  from the desk of dradamu ismalerauditing and a...      1           1   \n",
      "4  dear friend my name is loi cestradathe wife of...      1           1   \n",
      "\n",
      "   suspicious_words  text_len  token_count  exclam_count  \n",
      "0                 1      1558          286             0  \n",
      "1                 1      2936          492             0  \n",
      "2                 0         7            2             0  \n",
      "3                 1       497           76             0  \n",
      "4                 1      2050          320             0  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 0) Locate columns ---\n",
    "def detect_col(cols, candidates):\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    raise KeyError(f\"None of these columns found: {candidates}\")\n",
    "\n",
    "# Adjust candidate lists if needed\n",
    "text_col  = detect_col(\n",
    "    set(data.columns),\n",
    "    [\"preprocessed_text\",\"EmailText\",\"text\",\"message\",\"content\",\"sms\",\"Email\",\"body\"]\n",
    ")\n",
    "label_col = detect_col(\n",
    "    set(data.columns),\n",
    "    [\"label\",\"Label\",\"target\",\"y\",\"spam\",\"is_spam\"]\n",
    ")\n",
    "\n",
    "# --- 1) Minimal text cleaner to build `preprocessed_text` if missing ---\n",
    "def basic_clean(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    # remove urls, mentions, digits\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n",
    "    s = re.sub(r\"@\\w+|#\\w+\", \" \", s)\n",
    "    s = re.sub(r\"\\d+\", \" \", s)\n",
    "    # strip html tags\n",
    "    s = re.sub(r\"(?is)<(script|style)[^>]*>.*?</\\1>\", \" \", s)\n",
    "    s = re.sub(r\"(?is)<!--.*?-->\", \" \", s)\n",
    "    s = re.sub(r\"(?is)<[^>]+>\", \" \", s)\n",
    "    # keep letters and spaces only\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "    # collapse spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "if \"preprocessed_text\" not in data.columns:\n",
    "    data[\"preprocessed_text\"] = (\n",
    "        data[text_col].fillna(\"\").astype(str).apply(basic_clean)\n",
    "    )\n",
    "\n",
    "# --- 2) Split into train/val ---\n",
    "data_train, data_val = train_test_split(\n",
    "    data,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=data[label_col] if data[label_col].nunique() > 1 else None\n",
    ")\n",
    "data_train = data_train.reset_index(drop=True)\n",
    "data_val   = data_val.reset_index(drop=True)\n",
    "\n",
    "# --- 3) Extra features (your indicators + a couple helpful ones) ---\n",
    "def add_extra_features(df: pd.DataFrame, text_col: str = \"preprocessed_text\") -> pd.DataFrame:\n",
    "    df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "    # your original patterns (case-insensitive)\n",
    "    money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "    suspicious_words  = \"|\".join([\n",
    "        \"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\n",
    "        \"transaction\",\"win\",\"deposit\",\"password\"\n",
    "    ])\n",
    "\n",
    "    df['money_mark'] = df[text_col].str.contains(money_simbol_list, case=False, regex=True, na=False).astype('int8')\n",
    "    df['suspicious_words'] = df[text_col].str.contains(suspicious_words, case=False, regex=True, na=False).astype('int8')\n",
    "    df['text_len'] = df[text_col].str.len().astype('int32')\n",
    "\n",
    "    # extras\n",
    "    df['token_count']  = df[text_col].str.split().apply(len).astype('int32')\n",
    "    df['exclam_count'] = df[text_col].str.count(r\"!\").astype('int16')\n",
    "    return df\n",
    "\n",
    "data_train = add_extra_features(data_train, text_col=\"preprocessed_text\")\n",
    "data_val   = add_extra_features(data_val,   text_col=\"preprocessed_text\")\n",
    "\n",
    "print(data_train[[text_col if text_col!='preprocessed_text' else 'preprocessed_text',\n",
    "                  label_col,'money_mark','suspicious_words','text_len','token_count','exclam_count']].head())\n",
    "\n",
    "# (Optional) sanity check:\n",
    "# print(\"Columns:\", list(data.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['coding' 'fun' 'in' 'is' 'language' 'love' 'natural' 'processing'\n",
      " 'python']\n",
      "\n",
      "Bag of Words Representation:\n",
      "    coding  fun  in  is  language  love  natural  processing  python\n",
      "0       0    0   0   0         1     1        1           1       0\n",
      "1       0    1   0   1         1     0        0           1       0\n",
      "2       1    0   1   0         0     1        0           0       1\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example small corpus (list of text documents)\n",
    "corpus = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Language processing is fun\",\n",
    "    \"I love coding in Python\"\n",
    "]\n",
    "\n",
    "# Step 1: Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit the vectorizer and transform the corpus into BoW matrix\n",
    "X_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 3: Inspect vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Step 4: Convert to DataFrame for better readability\n",
    "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nBag of Words Representation:\\n\", bow_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix: (4, 20)\n",
      "\n",
      "TF-IDF DataFrame:\n",
      "     amazing    coding  coding python      data  data processing  data science  \\\n",
      "0  0.000000  0.000000       0.000000  0.000000         0.000000      0.000000   \n",
      "1  0.000000  0.000000       0.000000  0.000000         0.000000      0.000000   \n",
      "2  0.000000  0.426931       0.426931  0.336597         0.000000      0.426931   \n",
      "3  0.435516  0.000000       0.000000  0.343366         0.435516      0.000000   \n",
      "\n",
      "        fun  fun interesting  interesting  language  language processing  \\\n",
      "0  0.000000         0.000000     0.000000  0.343366             0.343366   \n",
      "1  0.420681         0.420681     0.420681  0.331670             0.331670   \n",
      "2  0.000000         0.000000     0.000000  0.000000             0.000000   \n",
      "3  0.000000         0.000000     0.000000  0.000000             0.000000   \n",
      "\n",
      "       love  love natural   natural  natural language  processing  \\\n",
      "0  0.343366      0.435516  0.435516          0.435516    0.277984   \n",
      "1  0.000000      0.000000  0.000000          0.000000    0.268515   \n",
      "2  0.336597      0.000000  0.000000          0.000000    0.000000   \n",
      "3  0.000000      0.000000  0.000000          0.000000    0.277984   \n",
      "\n",
      "   processing amazing  processing fun    python  python data  \n",
      "0            0.000000        0.000000  0.000000     0.000000  \n",
      "1            0.000000        0.420681  0.000000     0.000000  \n",
      "2            0.000000        0.000000  0.336597     0.336597  \n",
      "3            0.435516        0.000000  0.343366     0.343366  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords once\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Example dataset (replace with your actual text column)\n",
    "data = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"I love natural language processing\",\n",
    "        \"Language processing is fun and interesting\",\n",
    "        \"I love coding in Python for data science\",\n",
    "        \"Python and data processing are amazing\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Step 1: Load the vectorizer\n",
    "# Includes stopword removal, unigrams + bigrams, and a vocabulary size limit\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=20\n",
    ")\n",
    "\n",
    "# Step 2: Vectorize the entire dataset\n",
    "X_tfidf = vectorizer.fit_transform(data[\"text\"])\n",
    "\n",
    "# Step 3: Print shape\n",
    "print(\"Shape of TF-IDF matrix:\", X_tfidf.shape)\n",
    "\n",
    "# (Optional) Convert to DataFrame to inspect\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF DataFrame:\\n\", tfidf_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     66\u001b[39m     label_col = \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m y_raw = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_col\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Normalize to 0/1\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y_raw.dtype == \u001b[38;5;28mobject\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def detect_col(cols, candidates):\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def guess_label_col(df: pd.DataFrame, text_like=(\"text\",\"message\",\"content\",\"email\",\"body\",\"sms\")):\n",
    "    \"\"\"\n",
    "    Try to guess a binary label column:\n",
    "      - exclude text-like columns\n",
    "      - pick a column with exactly 2 unique values (ignoring NaNs)\n",
    "    \"\"\"\n",
    "    excluded = set([c for c in df.columns if any(t in c.lower() for t in text_like)])\n",
    "    candidates = []\n",
    "    for c in df.columns:\n",
    "        if c in excluded: \n",
    "            continue\n",
    "        uniq = df[c].dropna().unique()\n",
    "        # small set of uniques (exactly 2 preferred)\n",
    "        if len(uniq) == 2:\n",
    "            candidates.append(c)\n",
    "    if candidates:\n",
    "        return candidates[0]  # take the first; you can change this if needed\n",
    "    # fallback: any small-cardinality column (<=3 uniques)\n",
    "    for c in df.columns:\n",
    "        if c in excluded: \n",
    "            continue\n",
    "        uniq = df[c].dropna().unique()\n",
    "        if 1 < len(uniq) <= 3:\n",
    "            return c\n",
    "    raise KeyError(\"Could not infer a binary label column. Please set label_col manually.\")\n",
    "\n",
    "# ---------- 1) ensure preprocessed_text exists ----------\n",
    "text_col = (\"preprocessed_text\" \n",
    "            if \"preprocessed_text\" in data.columns \n",
    "            else detect_col(set(data.columns), [\"EmailText\",\"text\",\"message\",\"content\",\"sms\",\"Email\",\"body\"]))\n",
    "\n",
    "if text_col is None:\n",
    "    raise KeyError(f\"No text column found. Available columns: {list(data.columns)}\")\n",
    "\n",
    "if text_col != \"preprocessed_text\":\n",
    "    def basic_clean(s: str) -> str:\n",
    "        s = str(s).lower()\n",
    "        s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n",
    "        s = re.sub(r\"@\\w+|#\\w+\", \" \", s)\n",
    "        s = re.sub(r\"(?is)<(script|style)[^>]*>.*?</\\1>\", \" \", s)\n",
    "        s = re.sub(r\"(?is)<!--.*?-->\", \" \", s)\n",
    "        s = re.sub(r\"(?is)<[^>]+>\", \" \", s)\n",
    "        s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "    data[\"preprocessed_text\"] = data[text_col].fillna(\"\").astype(str).apply(basic_clean)\n",
    "text_col = \"preprocessed_text\"\n",
    "\n",
    "# ---------- 2) find/normalize label column ----------\n",
    "label_col = detect_col(set(data.columns), [\"label\",\"Label\",\"target\",\"y\",\"spam\",\"is_spam\"])\n",
    "if label_col is None:\n",
    "    label_col = \"label\"\n",
    "\n",
    "y_raw = data[label_col]\n",
    "\n",
    "# Normalize to 0/1\n",
    "if y_raw.dtype == object:\n",
    "    y = y_raw.str.strip().str.lower().map({\"ham\": 0, \"spam\": 1})\n",
    "    if y.isna().any():\n",
    "        y, _ = pd.factorize(y_raw)\n",
    "else:\n",
    "    y = pd.to_numeric(y_raw, errors=\"coerce\").astype(\"Int64\")\n",
    "    if y.dropna().nunique() != 2:\n",
    "        y, _ = pd.factorize(y_raw)\n",
    "\n",
    "# text features\n",
    "X_text = data[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "# ---------- 3) split ----------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42,\n",
    "    stratify=y if pd.Series(y).dropna().nunique()>1 else None\n",
    ")\n",
    "\n",
    "print(\"Detected text column:\", text_col)\n",
    "print(\"Detected label column:\", label_col)\n",
    "print(\"Train/Val sizes:\", len(X_train), len(X_val))\n",
    "print(\"Label classes (train):\", pd.Series(y_train).dropna().unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'preprocessed_text'], dtype='object')\n",
      "                                                    0  \\\n",
      "text               I love natural language processing   \n",
      "preprocessed_text  i love natural language processing   \n",
      "\n",
      "                                                            1  \\\n",
      "text               Language processing is fun and interesting   \n",
      "preprocessed_text  language processing is fun and interesting   \n",
      "\n",
      "                                                          2  \n",
      "text               I love coding in Python for data science  \n",
      "preprocessed_text  i love coding in python for data science  \n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "print(data.head(3).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of these columns found among ['preprocessed_text', 'text']: ['label', 'Label', 'target', 'y', 'spam', 'is_spam']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Detect text and label columns\u001b[39;00m\n\u001b[32m     23\u001b[39m text_col = (\u001b[33m\"\u001b[39m\u001b[33mpreprocessed_text\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mpreprocessed_text\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data.columns \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m     24\u001b[39m             detect_col(\u001b[38;5;28mset\u001b[39m(data.columns), [\u001b[33m\"\u001b[39m\u001b[33mEmailText\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msms\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mEmail\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m label_col = \u001b[43mdetect_col\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspam\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mis_spam\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Create preprocessed_text if needed\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_col != \u001b[33m\"\u001b[39m\u001b[33mpreprocessed_text\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mdetect_col\u001b[39m\u001b[34m(cols, candidates)\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols:\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m c\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of these columns found among \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidates\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of these columns found among ['preprocessed_text', 'text']: ['label', 'Label', 'target', 'y', 'spam', 'is_spam']\""
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "#impact needed libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ====== 1) Detect columns & basic preprocessing ======\n",
    "def detect_col(cols, candidates):\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    raise KeyError(f\"None of these columns found among {list(cols)}: {candidates}\")\n",
    "\n",
    "# Detect text and label columns\n",
    "text_col = (\"preprocessed_text\" if \"preprocessed_text\" in data.columns else\n",
    "            detect_col(set(data.columns), [\"EmailText\",\"text\",\"message\",\"content\",\"sms\",\"Email\",\"body\"]))\n",
    "label_col = detect_col(set(data.columns), [\"label\",\"Label\",\"target\",\"y\",\"spam\",\"is_spam\"])\n",
    "\n",
    "# Create preprocessed_text if needed\n",
    "if text_col != \"preprocessed_text\":\n",
    "    def basic_clean(s: str) -> str:\n",
    "        s = str(s).lower()\n",
    "        s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)       # URLs\n",
    "        s = re.sub(r\"@\\w+|#\\w+\", \" \", s)              # mentions/hashtags\n",
    "        s = re.sub(r\"(?is)<(script|style)[^>]*>.*?</\\1>\", \" \", s)  # inline JS/CSS\n",
    "        s = re.sub(r\"(?is)<!--.*?-->\", \" \", s)        # comments\n",
    "        s = re.sub(r\"(?is)<[^>]+>\", \" \", s)           # HTML tags\n",
    "        s = re.sub(r\"[^a-z\\s]\", \" \", s)               # letters/spaces only\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "    data[\"preprocessed_text\"] = data[text_col].fillna(\"\").astype(str).apply(basic_clean)\n",
    "    text_col = \"preprocessed_text\"\n",
    "\n",
    "# Normalize labels to 0/1\n",
    "y_raw = data[label_col]\n",
    "if y_raw.dtype == object:\n",
    "    y = y_raw.str.lower().map({\"ham\":0, \"spam\":1})\n",
    "    if y.isna().any():\n",
    "        y, _ = pd.factorize(y_raw)   # fallback\n",
    "else:\n",
    "    y = y_raw.astype(int)\n",
    "\n",
    "X_text = data[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "# Train/val split\n",
    "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y if y.nunique()>1 else None\n",
    ")\n",
    "\n",
    "# ====== 2) Extra flags (money_mark, suspicious_words, text_len) ======\n",
    "# We compute these ON THE FLY from raw preprocessed text and return a sparse matrix to keep NB happy.\n",
    "money_pattern = re.compile(r\"(?i)(euro|dollar|pound|€|\\$|£)\")\n",
    "suspicious_re = re.compile(r\"(?i)\\b(free|cheap|sex|money|account|bank|fund|transfer|transaction|win|deposit|password)\\b\")\n",
    "\n",
    "flag_names = np.array([\"money_mark\", \"suspicious_words\", \"text_len\"])\n",
    "\n",
    "def flags_from_text(X_iterable):\n",
    "    # X_iterable will be an array-like of strings\n",
    "    mm, sw, tl = [], [], []\n",
    "    for s in X_iterable:\n",
    "        s = \"\" if s is None else str(s)\n",
    "        mm.append(1 if money_pattern.search(s) else 0)\n",
    "        sw.append(1 if suspicious_re.search(s) else 0)\n",
    "        tl.append(len(s))\n",
    "    arr = np.vstack([mm, sw, tl]).T  # shape (n_samples, 3)\n",
    "    return sparse.csr_matrix(arr, dtype=np.float64)   # sparse to hstack nicely\n",
    "\n",
    "flags_transformer = FunctionTransformer(flags_from_text, accept_sparse=False, validate=False)\n",
    "\n",
    "# ====== 3) Build four feature configurations (always MultinomialNB with defaults) ======\n",
    "# Common vectorizer settings that usually work well for spam/ham\n",
    "bow = CountVectorizer(ngram_range=(1,2), min_df=3, max_features=30000, stop_words=\"english\")\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_features=30000, stop_words=\"english\")\n",
    "\n",
    "# A) Bag of Words only\n",
    "pipe_bow = Pipeline([\n",
    "    (\"vect\", bow),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# B) TF-IDF only\n",
    "pipe_tfidf = Pipeline([\n",
    "    (\"vect\", tfidf),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# C) BoW + flags (FeatureUnion)\n",
    "union_bow_flags = FeatureUnion([\n",
    "    (\"bow\", bow),\n",
    "    (\"flags\", flags_transformer),\n",
    "])\n",
    "pipe_bow_flags = Pipeline([\n",
    "    (\"features\", union_bow_flags),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# D) TF-IDF + flags (FeatureUnion)\n",
    "union_tfidf_flags = FeatureUnion([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"flags\", flags_transformer),\n",
    "])\n",
    "pipe_tfidf_flags = Pipeline([\n",
    "    (\"features\", union_tfidf_flags),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "configs = [\n",
    "    (\"BoW only\", pipe_bow),\n",
    "    (\"TF-IDF only\", pipe_tfidf),\n",
    "    (\"BoW + flags\", pipe_bow_flags),\n",
    "    (\"TF-IDF + flags\", pipe_tfidf_flags),\n",
    "]\n",
    "\n",
    "# ====== 4) Fit, validate, and cross-validate (F1) ======\n",
    "results = []\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, pipe in configs:\n",
    "    # CV on full text+labels (uses only X_text here)\n",
    "    cv_f1 = cross_val_score(pipe, X_text, y, cv=skf, scoring=\"f1\").mean()\n",
    "    # Train/val split scoring\n",
    "    pipe.fit(X_train_text, y_train)\n",
    "    y_pred = pipe.predict(X_val_text)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1  = f1_score(y_val, y_pred)\n",
    "    results.append((name, pipe, cv_f1, acc, f1))\n",
    "    print(f\"{name:15s} | 5-fold CV F1: {cv_f1:.3f} | Val Acc: {acc:.3f} | Val F1: {f1:.3f}\")\n",
    "\n",
    "# Pick best by validation F1 (tie-breaker CV F1)\n",
    "best_name, best_pipe, best_cv_f1, best_acc, best_f1 = sorted(\n",
    "    results, key=lambda t: (t[4], t[2]), reverse=True\n",
    ")[0]\n",
    "\n",
    "print(f\"\\nBest config: {best_name}\")\n",
    "print(\"\\nValidation report:\\n\", classification_report(y_val, best_pipe.predict(X_val_text), digits=3))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_val, best_pipe.predict(X_val_text)))\n",
    "\n",
    "# ====== 5) Show most relevant features for the best model ======\n",
    "def get_feature_names_for_pipe(name, pipe):\n",
    "    \"\"\"Return feature names aligned with MultinomialNB input columns.\"\"\"\n",
    "    if name == \"BoW only\":\n",
    "        return pipe.named_steps[\"vect\"].get_feature_names_out()\n",
    "    if name == \"TF-IDF only\":\n",
    "        return pipe.named_steps[\"vect\"].get_feature_names_out()\n",
    "    if name == \"BoW + flags\":\n",
    "        bow_names = pipe.named_steps[\"features\"].transformer_list[0][1].get_feature_names_out()\n",
    "        return np.concatenate([bow_names, flag_names])\n",
    "    if name == \"TF-IDF + flags\":\n",
    "        tfidf_names = pipe.named_steps[\"features\"].transformer_list[0][1].get_feature_names_out()\n",
    "        return np.concatenate([tfidf_names, flag_names])\n",
    "    raise ValueError(\"Unknown configuration\")\n",
    "\n",
    "def top_nb_features(nb_clf, feat_names, top_k=20):\n",
    "    \"\"\"\n",
    "    For MultinomialNB:\n",
    "    feature_log_prob_ shape = (n_classes, n_features).\n",
    "    We'll rank by log P(feature|spam) - log P(feature|ham).\n",
    "    \"\"\"\n",
    "    logprob = nb_clf.feature_log_prob_\n",
    "    if logprob.shape[0] != 2:\n",
    "        # assume classes are [0,1] but check ordering\n",
    "        # Sort by class index\n",
    "        pass\n",
    "    # Find index of ham (0) and spam (1) according to classes_\n",
    "    classes = nb_clf.classes_\n",
    "    if set(classes) != {0,1}:\n",
    "        # Map class order to ham/spam indices\n",
    "        cls_to_idx = {cls:i for i, cls in enumerate(classes)}\n",
    "        ham_idx, spam_idx = cls_to_idx[0], cls_to_idx[1]\n",
    "    else:\n",
    "        # commonly [0,1]\n",
    "        ham_idx, spam_idx = 0, 1\n",
    "\n",
    "    score = logprob[spam_idx] - logprob[ham_idx]\n",
    "    top_spam_idx = np.argsort(score)[-top_k:][::-1]\n",
    "    top_ham_idx  = np.argsort(score)[:top_k]\n",
    "\n",
    "    return (feat_names[top_spam_idx], score[top_spam_idx],\n",
    "            feat_names[top_ham_idx],  score[top_ham_idx])\n",
    "\n",
    "# Extract names + NB\n",
    "if best_name in [\"BoW only\", \"TF-IDF only\"]:\n",
    "    nb = best_pipe.named_steps[\"clf\"]\n",
    "else:\n",
    "    nb = best_pipe.named_steps[\"clf\"]\n",
    "\n",
    "feat_names = get_feature_names_for_pipe(best_name, best_pipe)\n",
    "top_spam_words, spam_scores, top_ham_words, ham_scores = top_nb_features(nb, feat_names, top_k=20)\n",
    "\n",
    "print(\"\\nTop features indicating SPAM:\")\n",
    "for w, s in zip(top_spam_words, spam_scores):\n",
    "    print(f\"{w:25s} {s: .3f}\")\n",
    "\n",
    "print(\"\\nTop features indicating HAM:\")\n",
    "for w, s in zip(top_ham_words, ham_scores):\n",
    "    print(f\"{w:25s} {s: .3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
