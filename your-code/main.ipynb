{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crvid\\AppData\\Local\\Temp\\ipykernel_2668\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data_train = pd.read_csv(\"C:\\\\Users\\\\crvid\\\\Documents\\\\IRONHACK\\\\WEEK_7\\\\DAY1\\\\lab-natural-language-processing\\\\data\\\\kg_train.csv\",encoding='latin-1')\n",
    "data_test = pd.read_csv(\"C:\\\\Users\\\\crvid\\\\Documents\\\\IRONHACK\\\\WEEK_7\\\\DAY1\\\\lab-natural-language-processing\\\\data\\\\kg_test.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data_train = data_train.head(1000)\n",
    "print(data_train.shape)\n",
    "data_train.fillna(\"\",inplace=True)\n",
    "\n",
    "data_test = data_test.head(1000)\n",
    "print(data_test.shape)\n",
    "data_test.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Defining Features and Target\n",
    "X = data_train[\"text\"]\n",
    "y = data_train[\"label\"]\n",
    "\n",
    "# Splitting the data_train into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english') # Initializes the stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56     Tom harkin called office twice duringCell:Home:AM\n",
      "955    Dear sir=2C I got your contact in cause of a s...\n",
      "231                                                   18\n",
      "738                               She is on her cell.###\n",
      "740                         Pls keep the updates coming!\n",
      "Name: text, dtype: object\n",
      "518     DR,GAIUS OBASEKIFORMER GROUP DIRECTORNIGERIA ...\n",
      "871    R29vZCBEYXksDQpNYXkgaXQgbm90IGJlIGEgc3VycHJpc2...\n",
      "797    newmyer on behalf of Jackie NewmyerTuesday Sep...\n",
      "274    GREETINGS FROM MR. ABU M.SESAYDEAR FRIEND,THRO...\n",
      "325    FROM:RITA DOINK.INTERNATIONAL PROMOTIONS/PRIZE...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.DOTALL) # Removes everything inside <script> and <style> tags\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL) # Removes HTML comments, anything in between <!-- and -->\n",
    "    text = re.sub(r'<.*?>', '', text) # Removes regular HTML tags\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the X_train and X_test datasets\n",
    "X_train_clean = X_train.apply(clean_text)\n",
    "X_test_clean = X_test.apply(clean_text)\n",
    "\n",
    "# Check the cleaned data (just to see a small sample)\n",
    "print(X_train_clean.head())\n",
    "print(X_test_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's expand the clean_text function to include all the additional cleaning steps\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove inline JavaScript/CSS and HTML tags\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters (non-alphabetic characters except space)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove prefixed 'b' (sometimes added when converting byte data)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56     tom harkin called office twice duringcell home am\n",
      "955    dear sir got your contact in cause of seriouse...\n",
      "231                                                     \n",
      "738                                   she is on her cell\n",
      "740                          pls keep the updates coming\n",
      "Name: text, dtype: object\n",
      "518    dr gaius obasekiformer group directornigeria n...\n",
      "871    vzcbeyxksdqpnyxkgaxqgbm igjligegc vychjpc ugdg...\n",
      "797    newmyer on behalf of jackie newmyertuesday sep...\n",
      "274    greetings from mr abu sesaydear friend through...\n",
      "325    from rita doink international promotions prize...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply the updated clean_text function to the X_train and X_test datasets\n",
    "X_train_clean = X_train.apply(clean_text)\n",
    "X_test_clean = X_test.apply(clean_text)\n",
    "\n",
    "# Check the cleaned data (just to see a small sample)\n",
    "print(X_train_clean.head())\n",
    "print(X_test_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's expand the clean_text function to remove stopwords\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove inline JavaScript/CSS and HTML tags\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters (non-alphabetic characters except space)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove prefixed 'b' (sometimes added when converting byte data)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56        tom harkin called office twice duringcell home\n",
      "955    dear sir got contact cause seriouse search rel...\n",
      "231                                                     \n",
      "738                                                 cell\n",
      "740                              pls keep updates coming\n",
      "Name: text, dtype: object\n",
      "518    dr gaius obasekiformer group directornigeria n...\n",
      "871    vzcbeyxksdqpnyxkgaxqgbm igjligegc vychjpc ugdg...\n",
      "797    newmyer behalf jackie newmyertuesday september...\n",
      "274    greetings mr abu sesaydear friend courtesy bus...\n",
      "325    rita doink international promotions prize awar...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply the updated clean_text function to both X_train and X_test\n",
    "X_train_clean = X_train.apply(clean_text)\n",
    "X_test_clean = X_test.apply(clean_text)\n",
    "\n",
    "# Check the cleaned data (after removing stopwords)\n",
    "print(X_train_clean.head())\n",
    "print(X_test_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\crvid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\crvid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # To get additional support for languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove inline JavaScript/CSS and HTML tags\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters (non-alphabetic characters except space)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization (convert words to their base form)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Reconstruct the cleaned text\n",
    "    text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56        tom harkin called office twice duringcell home\n",
      "955    dear sir got contact cause seriouse search rel...\n",
      "231                                                     \n",
      "738                                                 cell\n",
      "740                               pls keep update coming\n",
      "Name: text, dtype: object\n",
      "518    dr gaius obasekiformer group directornigeria n...\n",
      "871    vzcbeyxksdqpnyxkgaxqgbm igjligegc vychjpc ugdg...\n",
      "797    newmyer behalf jackie newmyertuesday september...\n",
      "274    greeting mr abu sesaydear friend courtesy busi...\n",
      "325    rita doink international promotion prize award...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply the updated clean_text function with lemmatization to X_train and X_test\n",
    "X_train_clean = X_train.apply(clean_text)\n",
    "X_test_clean = X_test.apply(clean_text)\n",
    "\n",
    "# Check the cleaned and lemmatized data\n",
    "print(X_train_clean.head())\n",
    "print(X_test_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in HAM messages:\n",
      "state: 136\n",
      "pm: 127\n",
      "would: 107\n",
      "president: 99\n",
      "time: 95\n",
      "call: 94\n",
      "mr: 91\n",
      "obama: 84\n",
      "percent: 81\n",
      "secretary: 79\n",
      "Top 10 words in SPAM messages:\n",
      "money: 981\n",
      "account: 895\n",
      "bank: 800\n",
      "fund: 781\n",
      "u: 734\n",
      "transaction: 549\n",
      "business: 514\n",
      "country: 508\n",
      "mr: 489\n",
      "nbsp: 475\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_messages = data_train[data_train['label'] == 0]['text']\n",
    "spam_messages = data_train[data_train['label'] == 1]['text']\n",
    "\n",
    "# Clean the messages with out created function\n",
    "ham_cleaned = ham_messages.apply(clean_text)\n",
    "spam_cleaned = spam_messages.apply(clean_text)\n",
    "\n",
    "# Tokenize the words by splitting the cleaned text into words\n",
    "ham_words = \" \".join(ham_cleaned).split()\n",
    "spam_words = \" \".join(spam_cleaned).split()\n",
    "\n",
    "# Count word frequency using Counter from the collections module\n",
    "ham_word_count = Counter(ham_words)\n",
    "spam_word_count = Counter(spam_words)\n",
    "\n",
    "# Get the 10 most common words in ham and spam messages\n",
    "top_10_ham = ham_word_count.most_common(10)\n",
    "top_10_spam = spam_word_count.most_common(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 words in HAM messages:\")\n",
    "for word, count in top_10_ham:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"Top 10 words in SPAM messages:\")\n",
    "for word, count in top_10_spam:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  money_mark  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1           1   \n",
       "1                                           Will do.      0           1   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0           1   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1           1   \n",
       "4                                                fyi      0           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 0      2292  \n",
       "1                 0         8  \n",
       "2                 0       197  \n",
       "3                 1      2199  \n",
       "4                 0         3  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Bag of Words matrix: (800, 27204)\n",
      "   aa  aaa  aabeiawaeaambiqaceqedeqh  aac  aaccount  aacute  aacw  aae  \\\n",
      "0   0    0                         0    0         0       0     0    0   \n",
      "1   0    0                         0    0         0       0     0    0   \n",
      "2   0    0                         0    0         0       0     0    0   \n",
      "3   0    0                         0    0         0       0     0    0   \n",
      "4   0    0                         0    0         0       0     0    0   \n",
      "\n",
      "   aaecaxeebsexbhjbuqdhcrmimoeifekrobhbcsmzuvavynlrchyknoel  aaegmdbsch  ...  \\\n",
      "0                                                  0                  0  ...   \n",
      "1                                                  0                  0  ...   \n",
      "2                                                  0                  0  ...   \n",
      "3                                                  0                  0  ...   \n",
      "4                                                  0                  0  ...   \n",
      "\n",
      "   zzvpscw  zzwh  zzwqgb  zzwqgdg  zzwqgyw  zzwx  zzxh  zzz  zzzahbxntxe  zzzj  \n",
      "0        0     0       0        0        0     0     0    0            0     0  \n",
      "1        0     0       0        0        0     0     0    0            0     0  \n",
      "2        0     0       0        0        0     0     0    0            0     0  \n",
      "3        0     0       0        0        0     0     0    0            0     0  \n",
      "4        0     0       0        0        0     0     0    0            0     0  \n",
      "\n",
      "[5 rows x 27204 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer on the training data and transform it into a Bag of Words\n",
    "X_train_bow = vectorizer.fit_transform(X_train_clean)\n",
    "\n",
    "# Convert the BoW matrix to a DataFrame for easier inspection\n",
    "bow_df = pd.DataFrame(X_train_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Check the shape and a few samples of the Bag of Words representation\n",
    "print(f\"Shape of the Bag of Words matrix: {X_train_bow.shape}\")\n",
    "print(bow_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF matrix: (800, 27204)\n",
      "First message's TF-IDF representation:\n",
      "    aa  aaa  aabeiawaeaambiqaceqedeqh  aac  aaccount  aacute  aacw  aae  \\\n",
      "0  0.0  0.0                       0.0  0.0       0.0     0.0   0.0  0.0   \n",
      "\n",
      "   aaecaxeebsexbhjbuqdhcrmimoeifekrobhbcsmzuvavynlrchyknoel  aaegmdbsch  ...  \\\n",
      "0                                                0.0                0.0  ...   \n",
      "\n",
      "   zzvpscw  zzwh  zzwqgb  zzwqgdg  zzwqgyw  zzwx  zzxh  zzz  zzzahbxntxe  zzzj  \n",
      "0      0.0   0.0     0.0      0.0      0.0   0.0   0.0  0.0          0.0   0.0  \n",
      "\n",
      "[1 rows x 27204 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the cleaned training data and transform it into TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_clean)\n",
    "\n",
    "# Check the shape of the vectorized dataset (number of messages and features)\n",
    "print(f\"Shape of the TF-IDF matrix: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Optionally, print the first few feature names (words) and their corresponding TF-IDF values for the first message\n",
    "print(\"First message's TF-IDF representation:\")\n",
    "print(pd.DataFrame(X_train_tfidf[0].toarray(), columns=tfidf_vectorizer.get_feature_names_out()).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.50%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       117\n",
      "           1       0.85      1.00      0.92        83\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.92      0.94      0.92       200\n",
      "weighted avg       0.94      0.93      0.93       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Initialize the classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Step 2: Train the classifier using the training data\n",
    "# X_train_tfidf contains the TF-IDF vectorized training messages\n",
    "# y_train contains the labels (ham/spam)\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Step 3: Transform the test data using the same TF-IDF vectorizer\n",
    "# X_test_clean is your cleaned test set (from text preprocessing)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_clean)\n",
    "\n",
    "# Step 4: Predict the labels for the test data\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Step 5: Evaluate the classifier performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Detailed classification report (precision, recall, f1-score)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
