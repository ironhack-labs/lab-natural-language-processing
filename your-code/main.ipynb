{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/8kjgwj5916lcv_26jt1s3fg80000gn/T/ipykernel_24596/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_val = train_test_split(data, test_size=0.3, random_state=42, stratify=data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "#print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t3/8kjgwj5916lcv_26jt1s3fg80000gn/T/ipykernel_24596/1424907659.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_text, \"html.parser\")\n",
      "/var/folders/t3/8kjgwj5916lcv_26jt1s3fg80000gn/T/ipykernel_24596/1424907659.py:4: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "def clean_html(html_text):\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "data_train['preprocessed_text'] = data_train['text'].apply(clean_html)\n",
    "data_val['preprocessed_text'] = data_val['text'].apply(clean_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0   \n",
      "428  H <hrod17@clintonemail.com >Saturday January 2...      0   \n",
      "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1   \n",
      "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1   \n",
      "380  Hello,This is Dr.Clive Whittaker. I work for F...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "381  mills cheryl sunday january pmtravel schedulec...  \n",
      "428   saturday january pm sbwhoeopre fyi any foreig...  \n",
      "849  dear my name is mr mr ken edward former govern...  \n",
      "252  dear sir am engr victor chigoziem with the eng...  \n",
      "380  hello this is dr clive whittaker work for fide...  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Initialize the SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # 1. Remove all special characters\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "\n",
    "    # 2. Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 3. Remove single characters (isolated single letters)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # 4. Remove single characters from the start\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "    # 5. Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # 6. Remove prefixed 'b' (handling any byte-like artifacts)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "\n",
    "    # 7. Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage on X_train (assuming X_train is a list or pandas Series of text)\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(preprocess_text)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(preprocess_text)\n",
    "\n",
    "# Output one of the cleaned texts to verify\n",
    "print(data_train[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0   \n",
      "428  H <hrod17@clintonemail.com >Saturday January 2...      0   \n",
      "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1   \n",
      "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1   \n",
      "380  Hello,This is Dr.Clive Whittaker. I work for F...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "381  mills cheryl sunday january pmtravel schedulec...  \n",
      "428  saturday january pm sbwhoeopre fyi foreign nat...  \n",
      "849  dear name mr mr ken edward former government o...  \n",
      "252  dear sir engr victor chigoziem engineering sto...  \n",
      "380  hello dr clive whittaker work fidelity investm...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/weemoe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# Load the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords from a list of words\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text (split into words)\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Example usage on X_train (assuming X_train is a list or pandas Series of text)\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(remove_stopwords)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(remove_stopwords)\n",
    "\n",
    "# Output one of the stopword-removed texts to verify\n",
    "print(data_train[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/weemoe/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/weemoe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/weemoe/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381    mill cheryl sunday january pmtravel scheduleca...\n",
      "428    saturday january pm sbwhoeopre fyi foreign nat...\n",
      "849    dear name mr mr ken edward former government o...\n",
      "252    dear sir engr victor chigoziem engineering sto...\n",
      "380    hello dr clive whittaker work fidelity investm...\n",
      "Name: preprocessed_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download the necessary NLTK resources\n",
    "nltk.download('punkt_tab')  # This is needed for tokenization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize the text (split into words)\n",
    "    words = word_tokenize(text)\n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to X_train_no_stopwords\n",
    "data_train['preprocessed_text'] = data_train['preprocessed_text'].apply(lemmatize_text)\n",
    "data_val['preprocessed_text'] = data_val['preprocessed_text'].apply(lemmatize_text)\n",
    "\n",
    "# Output the lemmatized text to verify\n",
    "print(data_train['preprocessed_text'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('money', 634), ('account', 603), ('bank', 562), ('u', 556), ('fund', 534), ('mr', 405), ('country', 383), ('transaction', 377), ('business', 367), ('million', 327)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize the bag of words Counter\n",
    "bag_of_words = Counter()\n",
    "\n",
    "# Loop over each text in X_train_no_stopwords\n",
    "for text in data_train['preprocessed_text']:\n",
    "    # Split the text into words\n",
    "    words = text.split(' ')\n",
    "    # Update the Counter with the words\n",
    "    bag_of_words.update(words)\n",
    "\n",
    "# Get the 10 most common words\n",
    "top_10_words = bag_of_words.most_common(10)\n",
    "\n",
    "# Display the top 10 words and their counts\n",
    "print(top_10_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Mills Cheryl D &lt;MillsCD@state.gov&gt;Sunday Janua...</td>\n",
       "      <td>0</td>\n",
       "      <td>mill cheryl sunday january pmtravel scheduleca...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>H &lt;hrod17@clintonemail.com &gt;Saturday January 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>saturday january pm sbwhoeopre fyi foreign nat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>DEAR,     MY NAME IS MR MR Ken Edward,A former...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear name mr mr ken edward former government o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Dear Sir, I am Engr. Victor Chigoziem with the...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sir engr victor chigoziem engineering sto...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Hello,This is Dr.Clive Whittaker. I work for F...</td>\n",
       "      <td>1</td>\n",
       "      <td>hello dr clive whittaker work fidelity investm...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0   \n",
       "428  H <hrod17@clintonemail.com >Saturday January 2...      0   \n",
       "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1   \n",
       "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1   \n",
       "380  Hello,This is Dr.Clive Whittaker. I work for F...      1   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "381  mill cheryl sunday january pmtravel scheduleca...           1   \n",
       "428  saturday january pm sbwhoeopre fyi foreign nat...           1   \n",
       "849  dear name mr mr ken edward former government o...           1   \n",
       "252  dear sir engr victor chigoziem engineering sto...           1   \n",
       "380  hello dr clive whittaker work fidelity investm...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "381                 0       103  \n",
       "428                 0       116  \n",
       "849                 1       900  \n",
       "252                 1      2252  \n",
       "380                 1       858  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Transform the validation data using the trained vectorizer\n",
    "X_val = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Convert these into DataFrames to inspect\n",
    "bow_df_train = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "bow_df_val = pd.DataFrame(X_val.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of the LogisticRegression is: 0.9733333333333334\n",
      "Validation Accuracy of the MultinomialNB is: 0.95\n",
      "Validation Accuracy of the SVC is: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Extract features and labels from training and validation sets\n",
    "X_train = bow_df_train\n",
    "y_train = data_train['label']\n",
    "X_val = bow_df_val\n",
    "y_val = data_val['label']\n",
    "\n",
    "# Initialize a Logistic Regression classifier\n",
    "\n",
    "classifier_dict = {'LogisticRegression':LogisticRegression(),\n",
    "                   'MultinomialNB':MultinomialNB(),\n",
    "                   'SVC':SVC()}\n",
    "for key, classifier in classifier_dict.items():\n",
    "\n",
    "    # Train the classifier on the training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation data\n",
    "    y_pred = classifier.predict(X_val)\n",
    "\n",
    "    # Evaluate the classifier's performance\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f'Validation Accuracy of the {key} is: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
