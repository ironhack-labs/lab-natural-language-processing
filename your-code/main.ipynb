{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7j/c3cwb7d17c1cfkhmkrqtw4q00000gn/T/ipykernel_10450/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bshanmugam/Documents/AIENginnering/AIClasswork/lab-natural-language-processing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "print(os.getcwd())\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "import os\n",
    "os.chdir('/Users/bshanmugam/Documents/AIENginnering/AIClasswork/lab-natural-language-processing')\n",
    "\n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_html(text):\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "     # Remove HTML comments <!-- ... -->\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove prefixed 'b' (like from byte strings)\n",
    "    text = re.sub(r\"^b['\\\"]\", '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove all single characters surrounded by spaces\n",
    "    text = re.sub(r'\\s+[a-z]\\s+', ' ', text)\n",
    "    \n",
    "    # Remove single characters at the start\n",
    "    text = re.sub(r'^[a-z]\\s+', '', text)\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Strip leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['cleaned_html'] = data['text'].apply(clean_html)  \n",
    "data['final_cleaned'] = data['cleaned_html'].apply(clean_text)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_stem(text):\n",
    "    tokens = text.split()  # split text into words\n",
    "    filtered = [snowball.stem(word) for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "    return \" \".join(filtered)\n",
    "data['clean_no_stop_stem'] = data['final_cleaned'].apply(remove_stopwords_and_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tame_text(text):\n",
    "    tokens = text.lower().split()\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalpha()]\n",
    "    \n",
    "    # Join back to a single string\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "data['lemmatization_word']= data['final_cleaned'].apply(tame_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              word  count\n",
      "10953        money    924\n",
      "167        account    800\n",
      "1904          bank    754\n",
      "7118          fund    704\n",
      "2724      business    496\n",
      "4191       country    457\n",
      "17535  transaction    416\n",
      "10765      million    396\n",
      "17606     transfer    393\n",
      "3629       company    386\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "corpus = data['lemmatization_word'].tolist()\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(corpus)\n",
    "word_counts = X_bow.toarray().sum(axis=0)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "word_freq = pd.DataFrame({'word': words, 'count': word_counts})\n",
    "top10_words = word_freq.sort_values(by='count', ascending=False).head(10)\n",
    "print(top10_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_html</th>\n",
       "      <th>final_cleaned</th>\n",
       "      <th>clean_no_stop_stem</th>\n",
       "      <th>lemmatization_word</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>1</td>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>regards mr nelson smithkindly reply me on my p...</td>\n",
       "      <td>regard mr nelson smithkind repli privat email ...</td>\n",
       "      <td>regard mr nelson smithkindly reply private ema...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>0</td>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>have not been able to reach oscar this am we a...</td>\n",
       "      <td>abl reach oscar suppos send pdb receiv</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>huma abedin bim checking with pat on the will ...</td>\n",
       "      <td>huma abedin bim check pat work jack jake resta...</td>\n",
       "      <td>huma abedin bim checking pat work jack jake re...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>can have it announced here on monday cant today</td>\n",
       "      <td>announc monday cant today</td>\n",
       "      <td>announced monday cant today</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>1</td>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...</td>\n",
       "      <td>bank of africaagence san pedro bp san pedro co...</td>\n",
       "      <td>bank africaag san pedro bp san pedro cote divo...</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
       "535  I have not been able to reach oscar this am. W...      0   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
       "557  I can have it announced here on Monday - can't...      0   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
       "\n",
       "                                          cleaned_html  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...   \n",
       "535  I have not been able to reach oscar this am. W...   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...   \n",
       "557  I can have it announced here on Monday - can't...   \n",
       "836  BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...   \n",
       "\n",
       "                                         final_cleaned  \\\n",
       "29   regards mr nelson smithkindly reply me on my p...   \n",
       "535  have not been able to reach oscar this am we a...   \n",
       "695  huma abedin bim checking with pat on the will ...   \n",
       "557    can have it announced here on monday cant today   \n",
       "836  bank of africaagence san pedro bp san pedro co...   \n",
       "\n",
       "                                    clean_no_stop_stem  \\\n",
       "29   regard mr nelson smithkind repli privat email ...   \n",
       "535             abl reach oscar suppos send pdb receiv   \n",
       "695  huma abedin bim check pat work jack jake resta...   \n",
       "557                          announc monday cant today   \n",
       "836  bank africaag san pedro bp san pedro cote divo...   \n",
       "\n",
       "                                    lemmatization_word  money_mark  \\\n",
       "29   regard mr nelson smithkindly reply private ema...           1   \n",
       "535         able reach oscar supposed send pdb receive           1   \n",
       "695  huma abedin bim checking pat work jack jake re...           1   \n",
       "557                        announced monday cant today           1   \n",
       "836  bank africaagence san pedro bp san pedro cote ...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0        75  \n",
       "535                 0        42  \n",
       "695                 0        79  \n",
       "557                 0        27  \n",
       "836                 1      1067  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['lemmatization_word'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['lemmatization_word'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['lemmatization_word'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['lemmatization_word'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['lemmatization_word'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['lemmatization_word'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary words: ['aac' 'aaclocated' 'aae' ... 'zumadirector' 'zumae' 'zurich']\n",
      "Bag of Words matrix shape: (1000, 19352)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "data['lemmatization_word'] = data['lemmatization_word'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform your preprocessed text to get the Bag of Words matrix\n",
    "X_bow = vectorizer.fit_transform(data['lemmatization_word'])\n",
    "\n",
    "# Check the feature names (vocabulary words)\n",
    "print(\"Vocabulary words:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Convert the sparse matrix to array (optional, for visualization)\n",
    "bow_array = X_bow.toarray()\n",
    "print(\"Bag of Words matrix shape:\", bow_array.shape)\n",
    "\n",
    "# Example: Display Bag of Words for first 3 documents\n",
    "print(bow_array[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary words: ['aac' 'aaclocated' 'aae' ... 'zumadirector' 'zumae' 'zurich']\n",
      "TF-IDF matrix shape: (1000, 19352)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Make sure the column is string type (if it's a list, convert it to string)\n",
    "data['lemmatization_word'] = data['lemmatization_word'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform your preprocessed text to get the TF-IDF matrix\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['lemmatization_word'])\n",
    "\n",
    "# Check the feature names (vocabulary words)\n",
    "print(\"Vocabulary words:\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Convert sparse matrix to array (optional, for viewing)\n",
    "tfidf_array = X_tfidf.toarray()\n",
    "print(\"TF-IDF matrix shape:\", tfidf_array.shape)\n",
    "\n",
    "# Example: Display TF-IDF vectors for first 3 documents\n",
    "print(tfidf_array[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize the classifier\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m clf \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the classifier on training data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Your codefrom sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assume X = feature matrix (e.g., X_tfidf), y = labels (like data['label'])\n",
    "X = X_tfidf  # or X_bow if you prefer Bag of Words\n",
    "y = data['label']  # Your target column\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the classifier on training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
