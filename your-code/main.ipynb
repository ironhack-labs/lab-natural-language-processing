{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2f/9101t8nx1hn47xc4fzmkb13w0000gn/T/ipykernel_32117/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "                                                 text  label\n",
      "0   DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                            Will do.      0\n",
      "2   Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3   Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                 fyi      0\n",
      "..                                                ...    ...\n",
      "95  Subject to your satisfaction you will be given...      1\n",
      "96                    Also I should mention the G-20.      0\n",
      "97  RGVhciBmcmllbmQsIAoKR3JlZXRpbmdzIHRvIHlvdSBpbi...      1\n",
      "98  cheryl.mills B6Sunday August 2 2009 8:50 AMHRe...      0\n",
      "99  Thanks for the help. We're working to schedule...      0\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "text     object\n",
      "label     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)\n",
    "print(data.head(100))\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800\n",
      "Validation set size: 200\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the split\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Validation set size:\", X_val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                          clean_text  \n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...  \n",
      "1                                           Will do.  \n",
      "2  Nora--Cheryl has emailed dozens of memos about...  \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...  \n",
      "4                                                fyi  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2f/9101t8nx1hn47xc4fzmkb13w0000gn/T/ipykernel_32117/1516230239.py:14: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, 'html.parser').get_text()\n",
      "/var/folders/2f/9101t8nx1hn47xc4fzmkb13w0000gn/T/ipykernel_32117/1516230239.py:14: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  text = BeautifulSoup(text, 'html.parser').get_text()\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(text):\n",
    "    # Step 1: Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 2: Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "data['clean_text'] = data['text'].apply(clean_html)\n",
    "\n",
    "# Display the first few rows of the cleaned text\n",
    "print(data[['text', 'clean_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                          clean_text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  dear sir strictly private business proposal am...  \n",
      "1                                           will do   \n",
      "2  nora cheryl has emailed dozens of memos about ...  \n",
      "3  dear sir fmadam know that this proposal might ...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove all special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'\\b\\w\\b\\s?', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the clean_text column\n",
    "data['cleaned_text'] = data['clean_text'].apply(clean_text)\n",
    "\n",
    "# Display the first few rows of the cleaned text\n",
    "print(data[['text', 'clean_text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  dear sir strictly private business proposal am...   \n",
      "1                                           will do    \n",
      "2  nora cheryl has emailed dozens of memos about ...   \n",
      "3  dear sir fmadam know that this proposal might ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                  final_cleaned_text  \n",
      "0  dear sir strictly private business proposal mi...  \n",
      "1                                                     \n",
      "2  nora cheryl emailed dozens memos haiti weekend...  \n",
      "3  dear sir fmadam know proposal might surprise e...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a single string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "# Apply the function to remove stopwords from the cleaned_text column\n",
    "data['final_cleaned_text'] = data['cleaned_text'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first few rows of the final cleaned text\n",
    "print(data[['text', 'cleaned_text', 'final_cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cindymartinezgrullon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cindymartinezgrullon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                  final_cleaned_text  \\\n",
      "0  dear sir strictly private business proposal mi...   \n",
      "1                                                      \n",
      "2  nora cheryl emailed dozens memos haiti weekend...   \n",
      "3  dear sir fmadam know proposal might surprise e...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  dear sir strictly private business proposal mi...  \n",
      "1                                                     \n",
      "2  nora cheryl emailed dozen memo haiti weekend p...  \n",
      "3  dear sir fmadam know proposal might surprise e...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure you have the required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the lemmatized words back into a single string\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply the lemmatization function to the final_cleaned_text column\n",
    "data['lemmatized_text'] = data['final_cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "# Display the first few rows of the lemmatized text\n",
    "print(data[['text', 'final_cleaned_text', 'lemmatized_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Remove empty documents after preprocessing\n",
    "data['final_cleaned_text'] = data['final_cleaned_text'].replace('', None)\n",
    "data = data.dropna(subset=['final_cleaned_text'])\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_messages = data[data['label'] == 0]['final_cleaned_text']\n",
    "spam_messages = data[data['label'] == 1]['final_cleaned_text']\n",
    "\n",
    "# Create a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform ham messages\n",
    "ham_bow = vectorizer.fit_transform(ham_messages)\n",
    "ham_words = vectorizer.get_feature_names_out()\n",
    "ham_counts = ham_bow.sum(axis=0).A1\n",
    "ham_word_counts = dict(zip(ham_words, ham_counts))\n",
    "\n",
    "# Fit and transform spam messages\n",
    "spam_bow = vectorizer.fit_transform(spam_messages)\n",
    "spam_words = vectorizer.get_feature_names_out()\n",
    "spam_counts = spam_bow.sum(axis=0).A1\n",
    "spam_word_counts = dict(zip(spam_words, spam_counts))\n",
    "\n",
    "# Get top 10 words in ham and spam messages\n",
    "top_ham_words = sorted(ham_word_counts.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "top_spam_words = sorted(spam_word_counts.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "\n",
    "# Create DataFrame to display results\n",
    "top_words_df = pd.DataFrame({\n",
    "    'Ham Words': [word[0] for word in top_ham_words],\n",
    "    'Ham Counts': [word[1] for word in top_ham_words],\n",
    "    'Spam Words': [word[0] for word in top_spam_words],\n",
    "    'Spam Counts': [word[1] for word in top_spam_words]\n",
    "})\n",
    "\n",
    "print(top_words_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                     lemmatized_text  money_mark  \\\n",
      "0  dear sir strictly private business proposal mi...           1   \n",
      "1                                                              1   \n",
      "2  nora cheryl emailed dozen memo haiti weekend p...           1   \n",
      "3  dear sir fmadam know proposal might surprise e...           1   \n",
      "4                                                fyi           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 1      1494  \n",
      "1                 0         0  \n",
      "2                 0       111  \n",
      "3                 1      1346  \n",
      "4                 0         3  \n"
     ]
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "\n",
    "money_simbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "# Add indicators and text length to the dataset\n",
    "data['money_mark'] = data['lemmatized_text'].str.contains(money_simbol_list, case=False)*1\n",
    "data['suspicious_words'] = data['lemmatized_text'].str.contains(suspicious_words, case=False)*1\n",
    "data['text_len'] = data['lemmatized_text'].apply(lambda x: len(x))\n",
    "\n",
    "# Display the first few rows of the dataset with new columns\n",
    "print(data[['text', 'lemmatized_text', 'money_mark', 'suspicious_words', 'text_len']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   also  data  enjoy  great  in  is  language  love  programming  python  \\\n",
      "0     0     0      0      0   1   0         0     1            1       1   \n",
      "1     0     0      0      1   0   1         1     0            1       1   \n",
      "2     1     1      1      0   0   0         0     0            0       0   \n",
      "\n",
      "   with  working  \n",
      "0     0        0  \n",
      "1     0        0  \n",
      "2     1        1  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python is a great programming language\",\n",
    "    \"I also enjoy working with data\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result to a DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 22016)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorize the entire dataset\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['lemmatized_text'])\n",
    "\n",
    "# Print the shape of the vectorized dataset\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "Precision: 1.00\n",
      "Recall: 0.85\n",
      "F1 Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
