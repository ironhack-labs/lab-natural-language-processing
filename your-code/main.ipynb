{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data_train = pd.read_csv(\"/Users/feepieper/Library/CloudStorage/OneDrive-Persönlich/Ironhack/Module4/labs/lab-natural-language-processing/data/kg_train.csv\",encoding='latin-1')\n",
    "data_test = pd.read_csv(\"/Users/feepieper/Library/CloudStorage/OneDrive-Persönlich/Ironhack/Module4/labs/lab-natural-language-processing/data/kg_test.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data_train = data_train.head(1000)\n",
    "print(data_train.shape)\n",
    "data_train.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2) (200, 2)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, val_set = train_test_split(data_train, test_size=0.2, random_state=42)\n",
    "print(train_set.shape, val_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def clean_text(data):\n",
    "    # 1️⃣ Remove inline JavaScript and CSS\n",
    "    data = re.sub(r'<script.*?>.*?</script>', '', data, flags=re.DOTALL | re.IGNORECASE)\n",
    "    data = re.sub(r'<style.*?>.*?</style>', '', data, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # 2️⃣ Remove HTML comments\n",
    "    data = re.sub(r'<!--.*?-->', '', data, flags=re.DOTALL)\n",
    "\n",
    "    # 3️⃣ Remove remaining HTML tags\n",
    "    data = re.sub(r'<[^>]+>', '', data)\n",
    "\n",
    "    # Optional: remove extra whitespace\n",
    "    data = re.sub(r'\\s+', ' ', data).strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "train_set['preprocessed_text'] = train_set['text'].apply(clean_text)\n",
    "val_set['preprocessed_text'] = val_set['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def further_clean_text(text):\n",
    "    # 1️⃣ Remove all special characters (keep letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # 2️⃣ Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 3️⃣ Remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "    # 4️⃣ Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # 5️⃣ Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 6️⃣ Remove prefixed 'b' (like b'example')\n",
    "    text = re.sub(r\"^b\\s+\", '', text)\n",
    "    \n",
    "    # 7️⃣ Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Optional: strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['preprocessed_text'] = train_set['preprocessed_text'].apply(further_clean_text)\n",
    "val_set['preprocessed_text'] = val_set['preprocessed_text'].apply(further_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/feepieper/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords_only(text):\n",
    "    # Text in Wörter aufteilen\n",
    "    words = text.split()\n",
    "    # Stopwords entfernen\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Wieder zu String zusammenfügen\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['preprocessed_text'] = train_set['preprocessed_text'].apply(remove_stopwords_only)\n",
    "val_set['preprocessed_text'] = val_set['preprocessed_text'].apply(remove_stopwords_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/feepieper/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/feepieper/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sicherstellen, dass WordNet-Daten vorhanden sind\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Text in Wörter aufteilen\n",
    "    words = text.split()\n",
    "    # Wörter lemmatisieren\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Wieder zu String zusammenfügen\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['preprocessed_text'] = train_set['preprocessed_text'].apply(lemmatize_text)\n",
    "val_set['preprocessed_text'] = val_set['preprocessed_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>1</td>\n",
       "      <td>regard mr nelson smith kindly reply private em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>0</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>huma abedin checking pat work jack jake rest a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>announced monday today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>1</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
       "535  I have not been able to reach oscar this am. W...      0   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
       "557  I can have it announced here on Monday - can't...      0   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
       "\n",
       "                                     preprocessed_text  \n",
       "29   regard mr nelson smith kindly reply private em...  \n",
       "535         able reach oscar supposed send pdb receive  \n",
       "695  huma abedin checking pat work jack jake rest a...  \n",
       "557                             announced monday today  \n",
       "836  bank africaagence san pedro bp san pedro cote ...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Ham Words: [('state', 117), ('pm', 97), ('would', 94), ('president', 89), ('mr', 89), ('time', 81), ('percent', 80), ('obama', 77), ('call', 74), ('secretary', 74)]\n",
      "Top 10 Spam Words: [('money', 842), ('account', 740), ('bank', 645), ('fund', 625), ('transaction', 466), ('business', 424), ('mr', 422), ('country', 419), ('million', 369), ('company', 365)]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def get_top_n_words(df, label, n=10):\n",
    "    # Filter für die Klasse (ham/spam)\n",
    "    texts = df[df['label'] == label]['preprocessed_text']\n",
    "    \n",
    "    # Bag-of-Words\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Summiere die Häufigkeiten\n",
    "    word_counts = X.sum(axis=0)\n",
    "    word_freq = [(word, word_counts[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    \n",
    "    # Sortieren nach Häufigkeit\n",
    "    word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Top N zurückgeben\n",
    "    return word_freq[:n]\n",
    "\n",
    "top_ham = get_top_n_words(train_set, 0, n=10)\n",
    "print(\"Top 10 Ham Words:\", top_ham)\n",
    "\n",
    "# Top 10 Wörter in Spam (label 1)\n",
    "top_spam = get_top_n_words(train_set, 1, n=10)\n",
    "print(\"Top 10 Spam Words:\", top_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>1</td>\n",
       "      <td>regard mr nelson smith kindly reply private em...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>0</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>huma abedin checking pat work jack jake rest a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>announced monday today</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>1</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
       "535  I have not been able to reach oscar this am. W...      0   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
       "557  I can have it announced here on Monday - can't...      0   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "29   regard mr nelson smith kindly reply private em...           0   \n",
       "535         able reach oscar supposed send pdb receive           0   \n",
       "695  huma abedin checking pat work jack jake rest a...           0   \n",
       "557                             announced monday today           0   \n",
       "836  bank africaagence san pedro bp san pedro cote ...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0        79  \n",
       "535                 0        42  \n",
       "695                 0        76  \n",
       "557                 0        22  \n",
       "836                 1      1052  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "train_set['money_mark'] = train_set['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "train_set['suspicious_words'] = train_set['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "train_set['text_len'] = train_set['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "val_set['money_mark'] = val_set['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "val_set['suspicious_words'] = val_set['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "val_set['text_len'] = val_set['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TF-IDF shape: (800, 5000)\n",
      "Validation TF-IDF shape: (200, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1️⃣ Load the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # optional: max_features, stop_words='english'\n",
    "\n",
    "# 2️⃣ Vectorize all dataset (hier z. B. clean_text_lemma Spalte)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_set['preprocessed_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_set['preprocessed_text'])\n",
    "\n",
    "# 3️⃣ Print the shape of the vectorized dataset\n",
    "print(\"Train TF-IDF shape:\", X_train_tfidf.shape)\n",
    "print(\"Validation TF-IDF shape:\", X_val_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       125\n",
      "           1       0.94      1.00      0.97        75\n",
      "\n",
      "    accuracy                           0.97       200\n",
      "   macro avg       0.97      0.98      0.97       200\n",
      "weighted avg       0.98      0.97      0.98       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1️⃣ TF-IDF Vektorisierung\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # optional: stop_words='english'\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_set['preprocessed_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_set['preprocessed_text'])\n",
    "\n",
    "# 2️⃣ Multinomial Naive Bayes Classifier\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_tfidf, train_set['label'])\n",
    "\n",
    "# 3️⃣ Vorhersage auf Validation Set\n",
    "y_pred = nb_clf.predict(X_val_tfidf)\n",
    "\n",
    "# 4️⃣ Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(val_set['label'], y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(val_set['label'], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
