{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Display a summary of the dataset\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'],  # Input feature\n",
    "    data['label'], # Target labels\n",
    "    test_size=0.2, # 20% of the data for testing\n",
    "    random_state=42 # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Display the sizes of each partition\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript/CSS: Any text within <script> or <style> tags\n",
    "    text = re.sub(r'<(script|style).*?>.*?(</\\1>)', '', text, flags=re.S)\n",
    "\n",
    "    # Remove HTML comments: <!-- ... -->\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.S)\n",
    "\n",
    "    # Remove remaining HTML tags: <...>\n",
    "    text = re.sub(r'<.*?>', '', text, flags=re.S)\n",
    "\n",
    "    # Return cleaned text\n",
    "    return text\n",
    "\n",
    "# Clean HTML from training and test sets\n",
    "X_train_cleaned = X_train.apply(clean_html)\n",
    "X_test_cleaned = X_test.apply(clean_html)\n",
    "\n",
    "# Display the first 5 rows of the cleaned training set to verify\n",
    "print(\"Cleaned Training Data:\")\n",
    "print(X_train_cleaned.head())\n",
    "\n",
    "print(\"\\nCleaned Test Data:\")\n",
    "print(X_test_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "def additional_cleaning(text):\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\s*\\w\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r'\\bb\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the additional cleaning to the training and test sets\n",
    "X_train_cleaned = X_train_cleaned.apply(additional_cleaning)\n",
    "X_test_cleaned = X_test_cleaned.apply(additional_cleaning)\n",
    "\n",
    "# Display the first 5 rows of the cleaned training set\n",
    "print(\"Further Cleaned Training Data:\")\n",
    "print(X_train_cleaned.head())\n",
    "\n",
    "print(\"\\nFurther Cleaned Test Data:\")\n",
    "print(X_test_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Set of English stopwords\n",
    "    words = word_tokenize(text)  # Tokenize the text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)  # Reconstruct the text without stopwords\n",
    "\n",
    "# Apply the function to the training and test sets\n",
    "X_train_cleaned = X_train_cleaned.apply(remove_stopwords)\n",
    "X_test_cleaned = X_test_cleaned.apply(remove_stopwords)\n",
    "\n",
    "# Display the first 5 rows of the cleaned training set\n",
    "print(\"Training Data after Stopwords Removal:\")\n",
    "print(X_train_cleaned.head())\n",
    "\n",
    "print(\"\\nTest Data after Stopwords Removal:\")\n",
    "print(X_test_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')  # Download WordNet for lemmatization\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to perform lemmatization\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)  # Tokenize the text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
    "    return ' '.join(lemmatized_words)  # Reconstruct the text with lemmatized words\n",
    "\n",
    "# Apply the function to the training and test sets\n",
    "X_train_cleaned = X_train_cleaned.apply(lemmatize_text)\n",
    "X_test_cleaned = X_test_cleaned.apply(lemmatize_text)\n",
    "\n",
    "# Display the first 5 rows of the lemmatized training set\n",
    "print(\"Training Data after Lemmatization:\")\n",
    "print(X_train_cleaned.head())\n",
    "\n",
    "print(\"\\nTest Data after Lemmatization:\")\n",
    "print(X_test_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Separate spam and ham messages\n",
    "spam_messages = X_train_cleaned[y_train == 1]\n",
    "ham_messages = X_train_cleaned[y_train == 0]\n",
    "\n",
    "# Define a function to extract the top N words\n",
    "def get_top_n_words(messages, n=10):\n",
    "    all_words = ' '.join(messages).split()  # Combine all messages and split into words\n",
    "    word_counts = Counter(all_words)  # Count the frequency of each word\n",
    "    return word_counts.most_common(n)  # Return the top N words and their counts\n",
    "\n",
    "# Get top 10 words for spam and ham messages\n",
    "top_spam_words = get_top_n_words(spam_messages, n=10)\n",
    "top_ham_words = get_top_n_words(ham_messages, n=10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 words in spam messages:\")\n",
    "print(top_spam_words)\n",
    "\n",
    "print(\"\\nTop 10 words in ham messages:\")\n",
    "print(top_ham_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "# Convert Series to lists explicitly before creating DataFrames\n",
    "data_train = pd.DataFrame({\"preprocessed_text\": X_train_cleaned.tolist()})\n",
    "data_val = pd.DataFrame({\"preprocessed_text\": X_test_cleaned.tolist()})\n",
    "\n",
    "# Add features to the training set\n",
    "data_train[\"money_mark\"] = data_train[\"preprocessed_text\"].str.contains(money_symbol_list, regex=True).astype(int)\n",
    "data_train[\"suspicious_words\"] = data_train[\"preprocessed_text\"].str.contains(suspicious_words, regex=True).astype(int)\n",
    "data_train[\"text_len\"] = data_train[\"preprocessed_text\"].apply(len)\n",
    "\n",
    "# Add features to the validation set\n",
    "data_val[\"money_mark\"] = data_val[\"preprocessed_text\"].str.contains(money_symbol_list, regex=True).astype(int)\n",
    "data_val[\"suspicious_words\"] = data_val[\"preprocessed_text\"].str.contains(suspicious_words, regex=True).astype(int)\n",
    "data_val[\"text_len\"] = data_val[\"preprocessed_text\"].apply(len)\n",
    "\n",
    "# Display the first rows of the training data with the new features\n",
    "print(\"Training set with additional features:\")\n",
    "print(data_train.head())\n",
    "\n",
    "print(\"\\nValidation set with additional features:\")\n",
    "print(data_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training set\n",
    "X_train_counts = vectorizer.fit_transform(data_train[\"preprocessed_text\"])\n",
    "\n",
    "# Transform the validation set using the same vectorizer\n",
    "X_val_counts = vectorizer.transform(data_val[\"preprocessed_text\"])\n",
    "\n",
    "# Get the feature names (words) and their frequencies for the training set\n",
    "word_counts_train = X_train_counts.toarray().sum(axis=0)  # Sum frequencies across all documents in train set\n",
    "word_frequencies_train = dict(zip(vectorizer.get_feature_names_out(), word_counts_train))\n",
    "\n",
    "# Sort and get the top 10 words for the training set\n",
    "top_words_train = sorted(word_frequencies_train.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 words in training data:\")\n",
    "print(top_words_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train[\"preprocessed_text\"])\n",
    "\n",
    "# Transform the validation set using the same vectorizer\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val[\"preprocessed_text\"])\n",
    "\n",
    "# Print the shapes of the vectorized datasets\n",
    "print(\"Shape of training data (TF-IDF):\", X_train_tfidf.shape)\n",
    "print(\"Shape of validation data (TF-IDF):\", X_val_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in data_train\n",
    "print(\"Columns in data_train:\")\n",
    "print(data_train.columns)\n",
    "\n",
    "# Display the first few rows to inspect the structure\n",
    "print(\"\\nFirst rows of data_train:\")\n",
    "print(data_train.head())\n",
    "\n",
    "# Check if the labels are present in the DataFrame\n",
    "if \"label\" not in data_train.columns:\n",
    "    print(\"\\nThe column 'label' is missing in data_train!\")\n",
    "else:\n",
    "    print(\"\\nThe column 'label' exists in data_train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the labels to the training and validation DataFrames\n",
    "data_train[\"label\"] = y_train.values\n",
    "data_val[\"label\"] = y_test.values\n",
    "\n",
    "# Verify the structure of data_train and data_val\n",
    "print(\"Updated columns in data_train:\")\n",
    "print(data_train.columns)\n",
    "\n",
    "print(\"\\nFirst rows of data_train after adding label:\")\n",
    "print(data_train.head())\n",
    "\n",
    "print(\"\\nUpdated columns in data_val:\")\n",
    "print(data_val.columns)\n",
    "\n",
    "print(\"\\nFirst rows of data_val after adding label:\")\n",
    "print(data_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the classifier\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "model.fit(X_train_tfidf, data_train[\"label\"])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_val_tfidf)\n",
    "\n",
    "# Print the predictions (optional)\n",
    "print(\"Predictions on the validation set:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(data_val[\"label\"], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(data_val[\"label\"], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define function to train and evaluate the classifier\n",
    "def train_and_evaluate(vectorizer):\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    # Train the classifier\n",
    "    pipeline.fit(data_train[\"preprocessed_text\"], data_train[\"label\"])\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = pipeline.predict(data_val[\"preprocessed_text\"])\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(data_val[\"label\"], y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(data_val[\"label\"], y_pred))\n",
    "    return accuracy\n",
    "\n",
    "# Try different vectorizers\n",
    "print(\"Using TfidfVectorizer:\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 2))\n",
    "train_and_evaluate(tfidf_vectorizer)\n",
    "\n",
    "print(\"\\nUsing CountVectorizer:\")\n",
    "count_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "train_and_evaluate(count_vectorizer)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
