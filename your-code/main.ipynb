{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kenneth\\AppData\\Local\\Temp\\ipykernel_22128\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(r\"C:\\Users\\kenneth\\Desktop\\IronHack AI Engineer\\lab-natural-language-processing\\data\\kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val, y_train, y_val = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', ' ', text)  \n",
    "    text = re.sub(r'www\\S+', ' ', text)   \n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Remove single characters appearing at the start\n",
    "    text = re.sub(r'^\\s*[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_cleaned = data_train.apply(clean_text)\n",
    "data_val_cleaned = data_val.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     regards mr nelson smith kindly reply private e...\n",
      "535           able reach oscar supposed send pdb receive\n",
      "695    huma abedin checking pat work jack jake rest a...\n",
      "557                               announced monday today\n",
      "836    bank africaagence san pedro bp san pedro cote ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join(word for word in text.split() if word.lower() not in stop_words)\n",
    "\n",
    "# Apply to cleaned training and validation\n",
    "data_train_cleaned = data_train_cleaned.apply(remove_stopwords)\n",
    "data_val_cleaned = data_val_cleaned.apply(remove_stopwords)\n",
    "\n",
    "print(data_train_cleaned.head())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     regard mr nelson smith kindly reply private em...\n",
      "535           able reach oscar supposed send pdb receive\n",
      "695    huma abedin checking pat work jack jake rest a...\n",
      "557                               announced monday today\n",
      "836    bank africaagence san pedro bp san pedro cote ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization\n",
    "data_train_cleaned = data_train_cleaned.apply(lemmatize_text)\n",
    "data_val_cleaned = data_val_cleaned.apply(lemmatize_text)\n",
    "\n",
    "print(data_train_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in HAM messages: [('â', 201), ('state', 142), ('pm', 97), ('would', 94), ('ã', 89), ('president', 89), ('mr', 89), ('time', 81), ('percent', 80), ('obama', 77)]\n",
      "Top 10 words in SPAM messages: [('money', 847), ('account', 743), ('u', 650), ('bank', 646), ('fund', 626), ('br', 600), ('e', 502), ('transaction', 471), ('business', 424), ('mr', 422)]\n"
     ]
    }
   ],
   "source": [
    "def get_top_words(series, n=10):\n",
    "    all_text = ' '.join(series)  \n",
    "    words = all_text.split()  \n",
    "    word_counts = {}  \n",
    "\n",
    "    for word in words:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1  \n",
    "\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)  # Sort by frequency\n",
    "    return sorted_words[:n]  # Return top n words\n",
    "\n",
    "train_data = pd.DataFrame({'text': data_train_cleaned, 'label': y_train})  # Combine text and labels into DataFrame\n",
    "\n",
    "# Filter based on integer labels (0 = ham, 1 = spam)\n",
    "ham_words = get_top_words(train_data[train_data['label'] == 0]['text'])  \n",
    "spam_words = get_top_words(train_data[train_data['label'] == 1]['text'])  \n",
    "\n",
    "\n",
    "print(\"Top 10 words in HAM messages:\", ham_words)\n",
    "print(\"Top 10 words in SPAM messages:\", spam_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['preprocessed_text'] = data_train_cleaned\n",
    "data_val['preprocessed_text'] = data_val_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
       "535    I have not been able to reach oscar this am. W...\n",
       "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
       "557    I can have it announced here on Monday - can't...\n",
       "836        BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words shape for training set: (800, 5000)\n",
      "Bag of Words shape for validation set: (200, 5000)\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_train_bow = count_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_bow = count_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "print(f\"Bag of Words shape for training set: {X_train_bow.shape}\")\n",
    "print(f\"Bag of Words shape for validation set: {X_val_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape for training set: (800, 5000)\n",
      "TF-IDF shape for validation set: (200, 5000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "print(f\"TF-IDF shape for training set: {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF shape for validation set: {X_val_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model Performance using Bag of Words:\n",
      "Accuracy: 0.9800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[122   3]\n",
      " [  1  74]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       125\n",
      "           1       0.96      0.99      0.97        75\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.98      0.98      0.98       200\n",
      "weighted avg       0.98      0.98      0.98       200\n",
      "\n",
      "==================================================\n",
      "==================================================\n",
      "Model Performance using TF-IDF:\n",
      "Accuracy: 0.9750\n",
      "\n",
      "Confusion Matrix:\n",
      "[[120   5]\n",
      " [  0  75]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       125\n",
      "           1       0.94      1.00      0.97        75\n",
      "\n",
      "    accuracy                           0.97       200\n",
      "   macro avg       0.97      0.98      0.97       200\n",
      "weighted avg       0.98      0.97      0.98       200\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def train_and_evaluate(X_train, X_val, y_train, y_val, feature_name):\n",
    "\n",
    "    # Classifier\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # Evaluate \n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "    class_report = classification_report(y_val, y_pred)\n",
    "\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Model Performance using {feature_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_report)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return clf, accuracy\n",
    "\n",
    "clf_bow, acc_bow = train_and_evaluate(X_train_bow, X_val_bow, y_train, y_val, \"Bag of Words\")\n",
    "\n",
    "\n",
    "clf_tfidf, acc_tfidf = train_and_evaluate(X_train_tfidf, X_val_tfidf, y_train, y_val, \"TF-IDF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance**\n",
    "\n",
    "Bag of Words Accuracy: 98.00%\n",
    "TF-IDF Accuracy: 97.50%\n",
    "\n",
    "Confusion matrix show that it often predicts correctly fake or not fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance using BoW: Accuracy = 0.9800\n",
      "Model Performance using TF-IDF: Accuracy = 0.9750\n",
      "Model Performance using Bi-TFIDF: Accuracy = 0.9850\n",
      "\n",
      "Best Feature Representation: Bi-TFIDF with Accuracy = 0.9850\n"
     ]
    }
   ],
   "source": [
    "# Trying three different vectorization methods\n",
    "def extract_features(method, train_texts, val_texts, max_features=5000):\n",
    "    if method == \"BoW\":\n",
    "        vectorizer = CountVectorizer(max_features=max_features)\n",
    "    elif method == \"TF-IDF\":\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    elif method == \"Bi-TFIDF\":\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))  # Bi-grams\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method\")\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    return X_train, X_val, vectorizer\n",
    "\n",
    "# Train and evaluate the classifier (Multinomial)\n",
    "def train_and_evaluate(X_train, X_val, y_train, y_val, method):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"Model Performance using {method}: Accuracy = {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for feature_method in [\"BoW\", \"TF-IDF\", \"Bi-TFIDF\"]:\n",
    "    X_train, X_val, vectorizer = extract_features(feature_method, data_train['preprocessed_text'], data_val['preprocessed_text'])\n",
    "    acc = train_and_evaluate(X_train, X_val, y_train, y_val, feature_method)\n",
    "    results[feature_method] = acc\n",
    "\n",
    "best_method = max(results, key=results.get)\n",
    "print(f\"\\nBest Feature Representation: {best_method} with Accuracy = {results[best_method]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
