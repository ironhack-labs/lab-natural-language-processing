{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/67/kwhhh9lj76v79c9zbkp29ms40000gn/T/ipykernel_97055/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"/Users/tiagovhp/Ironhack/Week_7/Day_1/lab-natural-language-processing/data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>General Auditor&lt;br/&gt;First National Bank,&lt;br/&gt;J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>Pls print.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Later--how about 10? That way I can leave the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Turns out the French got through to him today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>PIR &lt;preinesMonday April 26 2010 9:52 PMRe: Ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>Hello My Dear Beloved Friend, My name is Mrs. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Sorry - first I've heard of it. Philippe and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Thank you.MR.Cheung Pui. Managing Director, AD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Sullivan Jacob J &lt;Sullivann@state.gov&gt;Thursday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>FROM:Dr.RICHARD SHUBANE .#123CB/PBV,MAIN CITY,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "385  General Auditor<br/>First National Bank,<br/>J...\n",
       "489                                         Pls print.\n",
       "52   Later--how about 10? That way I can leave the ...\n",
       "114  Turns out the French got through to him today ...\n",
       "379  PIR <preinesMonday April 26 2010 9:52 PMRe: Ni...\n",
       "..                                                 ...\n",
       "553  Hello My Dear Beloved Friend, My name is Mrs. ...\n",
       "777  Sorry - first I've heard of it. Philippe and I...\n",
       "954  Thank you.MR.Cheung Pui. Managing Director, AD...\n",
       "243  Sullivan Jacob J <Sullivann@state.gov>Thursday...\n",
       "549  FROM:Dr.RICHARD SHUBANE .#123CB/PBV,MAIN CITY,...\n",
       "\n",
       "[800 rows x 1 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "# Split into features and labels\n",
    "features = pd.DataFrame(data['text'],columns=['text'])\n",
    "labels = pd.DataFrame(data['label'],columns=['label'])\n",
    "\n",
    "# Lets divide the training data into train and *validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,labels,test_size=.2)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tiagovhp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords #list of common words in a language that don't carry much meaning, such as \"the\", \"is\", \"in\", etc.\n",
    "print(stopwords.words(\"english\")[100:110]) #print a slice of 10 stopwords from the English list\n",
    "\n",
    "import string\n",
    "print(string.punctuation) # string of all punctuation characters\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer #stemming algorithm\n",
    "snowball = SnowballStemmer('english') # creating a SnowballStemmer object for the English language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear sir strictly a private business proposal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nora cheryl has emailed dozens of memos about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear sir fmadam c i know that this proposal mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sure bottom line you need a special security c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dear sir i am engr ugo nzego with the engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abedin huma saturday november pmhfw quint fmsi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>there is an oct th george marshall event at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>for you as the account owner for i and my col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>strong http www cnn com world africa kenya cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dear friend my name is edward moore qc princip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>compliment how are you doing today hope you ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         cleaned_text\n",
       "0   dear sir strictly a private business proposal ...\n",
       "1                                            will do \n",
       "2   nora cheryl has emailed dozens of memos about ...\n",
       "3   dear sir fmadam c i know that this proposal mi...\n",
       "4                                                 fyi\n",
       "5   sure bottom line you need a special security c...\n",
       "6   dear sir i am engr ugo nzego with the engineer...\n",
       "7   abedin huma saturday november pmhfw quint fmsi...\n",
       "8   there is an oct th george marshall event at th...\n",
       "9    for you as the account owner for i and my col...\n",
       "10  strong http www cnn com world africa kenya cra...\n",
       "11  dear friend my name is edward moore qc princip...\n",
       "12  compliment how are you doing today hope you ha..."
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "\n",
    "cleaned_texts = [] # list to hold cleaned texts\n",
    "for element in data['text']:\n",
    "    str_element = str(element)\n",
    "    # Remove inline CSS:\n",
    "    cleaned_element = re.sub(r'<style.*?>.*?</style>', '',str_element)\n",
    "    # Remove HTML comments\n",
    "    cleaned_element = re.sub(r'<!--.*?-->', '', cleaned_element)\n",
    "    # Remove remaining HTML tags\n",
    "    cleaned_element = re.sub(r'<.*?>', '', cleaned_element)\n",
    "    # Remove special characters (keeping only alphanumeric characters and spaces) -> substitue with space \" \n",
    "    cleaned_element = re.sub(r'[^a-zA-Z0-9\\s]', ' ', cleaned_element)\n",
    "    # Replace numbers with a space:\n",
    "    cleaned_element = re.sub(r'\\d+', ' ', cleaned_element)\n",
    "    # Remove single characters from the start\n",
    "    cleaned_element = re.sub(r'^\\s*[a-zA-Z]\\s+', '', cleaned_element)\n",
    "    # Substitute multiple spaces with a single space\n",
    "    cleaned_element = re.sub(r'\\s+', ' ', cleaned_element)\n",
    "     # Remove the prefixed 'b' if present\n",
    "    cleaned_element = re.sub(r\"^b'|'\", '', cleaned_element)\n",
    "    # Convert to lowercase\n",
    "    cleaned_element = cleaned_element.lower()\n",
    "\n",
    "    cleaned_texts.append(cleaned_element)\n",
    "cleaned_texts=pd.DataFrame(cleaned_texts,columns=['cleaned_text'])\n",
    "cleaned_texts.head(13)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear sir strictly  private business proposal  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nora cheryl  emailed dozens  memos  haiti    w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear sir fmadam c  know   proposal might   sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sure bottom line  need  special security code ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dear sir   engr ugo nzego   engineering stores...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abedin huma saturday november pmhfw quint fmsi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>oct th george marshall event   department  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>account owner     colleagues   set aside ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>strong http www cnn com world africa kenya cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dear friend  name  edward moore qc principal p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   dear sir strictly  private business proposal  ...\n",
       "1                                                    \n",
       "2   nora cheryl  emailed dozens  memos  haiti    w...\n",
       "3   dear sir fmadam c  know   proposal might   sur...\n",
       "4                                                 fyi\n",
       "5   sure bottom line  need  special security code ...\n",
       "6   dear sir   engr ugo nzego   engineering stores...\n",
       "7   abedin huma saturday november pmhfw quint fmsi...\n",
       "8      oct th george marshall event   department  ...\n",
       "9        account owner     colleagues   set aside ...\n",
       "10  strong http www cnn com world africa kenya cra...\n",
       "11  dear friend  name  edward moore qc principal p..."
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = stopwords.words(\"english\")\n",
    "cleaned_texts2=[]\n",
    "for element in cleaned_texts['cleaned_text']:\n",
    "    new_element = element\n",
    "    # Remove stop words from list \"element\"\n",
    "    for word in stopwords:\n",
    "        new_element = re.sub(r'\\b' + re.escape(word) + r'\\b', '', new_element)\n",
    "    cleaned_texts2.append(new_element)\n",
    "cleaned_texts2 = pd.DataFrame(cleaned_texts2,columns=['text'])\n",
    "cleaned_texts2.head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text\n",
      "0   dear sir strictly private business proposal mi...\n",
      "1                                                    \n",
      "2   nora cheryl emailed dozen memo haiti weekend p...\n",
      "3   dear sir fmadam c know proposal might surprise...\n",
      "4                                                 fyi\n",
      "5   sure bottom line need special security code ge...\n",
      "6   dear sir engr ugo nzego engineering store depa...\n",
      "7   abedin huma saturday november pmhfw quint fmsi...\n",
      "8   oct th george marshall event department tentat...\n",
      "9   account owner colleague set aside defray incid...\n",
      "10  strong http www cnn com world africa kenya cra...\n",
      "11  dear friend name edward moore qc principal par...\n",
      "12  compliment today hope forgotten dr mr michaelh...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tiagovhp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data (only needed once)\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "cleaned_texts3 = []\n",
    "for text in cleaned_texts2['text']:\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    #Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the lemmatized words back into a single string\n",
    "    new_text = ' '.join(lemmatized_words)\n",
    "    # Append the cleaned text to a new list\n",
    "    cleaned_texts3.append(new_text)\n",
    "\n",
    "cleaned_texts3=pd.DataFrame(cleaned_texts3,columns=['text'])\n",
    "\n",
    "# Display the result\n",
    "print(cleaned_texts3.head(13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words in Ham Messages:\n",
      "state        136\n",
      "pm           127\n",
      "would        107\n",
      "president     99\n",
      "time          95\n",
      "call          94\n",
      "mr            91\n",
      "obama         84\n",
      "percent       81\n",
      "secretary     79\n",
      "Name: 0, dtype: int64\n",
      "\n",
      "Top 10 Words in Spam Messages:\n",
      "money          981\n",
      "account        895\n",
      "bank           800\n",
      "fund           781\n",
      "transaction    549\n",
      "business       514\n",
      "country        508\n",
      "mr             489\n",
      "nbsp           475\n",
      "million        460\n",
      "Name: 1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate the count vector class\n",
    "v = CountVectorizer()\n",
    "\n",
    "# Generate the BoW model for our training set\n",
    "cleaned_texts3_cv = v.fit_transform(cleaned_texts3['text']) #X_train_cv is X_train count vectorizer\n",
    "\n",
    "# Convert the result to a DataFrame for easier manipulation\n",
    "cleaned_texts3_cv=pd.DataFrame(cleaned_texts3_cv.toarray(), columns=v.get_feature_names_out())\n",
    "\n",
    "# Add the labels (spam or ham) to the columns and sum the word counts\n",
    "cleaned_texts3_cv['label']=data['label']\n",
    "\n",
    "# Groupby label and sum the word counts\n",
    "top_words = cleaned_texts3_cv.groupby('label').sum().T\n",
    "top_words\n",
    "\n",
    "# Get the top 10 words for ham and spam\n",
    "top_10_ham = top_words[0].nlargest(10)\n",
    "top_10_spam = top_words[1].nlargest(10)\n",
    "\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"Top 10 Words in Ham Messages:\")\n",
    "print(top_10_ham)\n",
    "\n",
    "print(\"\\nTop 10 Words in Spam Messages:\")\n",
    "print(top_10_spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words in Ham Messages:\n",
      "the     3456\n",
      "to      2598\n",
      "of      2242\n",
      "and     1935\n",
      "in      1556\n",
      "you     1260\n",
      "this    1062\n",
      "for      950\n",
      "that     839\n",
      "your     820\n",
      "Name: 0.0, dtype: int64\n",
      "\n",
      "Top 10 Words in Spam Messages:\n",
      "the     2564\n",
      "to      1986\n",
      "of      1736\n",
      "and     1379\n",
      "you     1169\n",
      "in      1116\n",
      "this     963\n",
      "for      766\n",
      "my       692\n",
      "your     679\n",
      "Name: 1.0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate the count vector class\n",
    "v = CountVectorizer()\n",
    "\n",
    "# Generate the BoW model for our training set\n",
    "X_train_cv = v.fit_transform(X_train['text']) #X_train_cv is X_train count vectorizer\n",
    "\n",
    "# Convert the result to a DataFrame for easier manipulation\n",
    "X_train_cv=pd.DataFrame(X_train_cv.toarray(), columns=v.get_feature_names_out())\n",
    "\n",
    "# Add the labels (spam or ham) to the columns and sum the word counts\n",
    "X_train_cv['label']=y_train['label']\n",
    "\n",
    "# Groupby label and sum the word counts\n",
    "top_words = X_train_cv.groupby('label').sum().T\n",
    "top_words\n",
    "\n",
    "# Get the top 10 words for ham and spam\n",
    "top_10_ham = top_words[0].nlargest(10)\n",
    "top_10_spam = top_words[1].nlargest(10)\n",
    "\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"Top 10 Words in Ham Messages:\")\n",
    "print(top_10_ham)\n",
    "\n",
    "print(\"\\nTop 10 Words in Spam Messages:\")\n",
    "print(top_10_spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[256], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m money_simbol_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdollar\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpound\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m€\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m suspicious_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfree\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoney\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccount\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbank\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfund\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeposit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m data_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoney_mark\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_train\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(money_simbol_list)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m data_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuspicious_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(suspicious_words)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m data_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x)) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
