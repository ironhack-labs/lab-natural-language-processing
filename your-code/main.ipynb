{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (9.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython) (4.14.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack_data->ipython) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack_data->ipython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack_data->ipython) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Lovely\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(r\"C:\\Users\\Lovely\\Desktop\\AI_Worklab\\Lab_Week4_Aug11\\lab_natural_language_processing\\lab-natural-language-processing\\data\\kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split complete.\n",
      "Test data saved to: C:\\Users\\Lovely\\Desktop\\AI_Worklab\\Lab_Week4_Aug11\\lab_natural_language_processing\\lab-natural-language-processing\\data\\kg_test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the original data\n",
    "file_path = r\"C:\\Users\\Lovely\\Desktop\\AI_Worklab\\Lab_Week4_Aug11\\lab_natural_language_processing\\lab-natural-language-processing\\data\\kg_train.csv\"\n",
    "data = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save test set to a new CSV file\n",
    "test_path = r\"C:\\Users\\Lovely\\Desktop\\AI_Worklab\\Lab_Week4_Aug11\\lab_natural_language_processing\\lab-natural-language-processing\\data\\kg_test.csv\"\n",
    "test_data.to_csv(test_path, index=False, encoding='latin-1')\n",
    "\n",
    "print(\"Train/Test split complete.\")\n",
    "print(f\"Test data saved to: {test_path}\")\n",
    "\n",
    "train_data['preprocessed_text'] = train_data['text'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (3578, 2)\n",
      "Validation size: (1193, 2)\n",
      "Test size: (1193, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Split data into 80% train+val and 20% test\n",
    "train_val_data, data_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Split train_val_data into 75% train and 25% val (which is 60% train, 20% val overall)\n",
    "data_train, data_val = train_test_split(train_val_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# Optional: Check sizes\n",
    "print(f\"Train size: {data_train.shape}\")\n",
    "print(f\"Validation size: {data_val.shape}\")\n",
    "print(f\"Test size: {data_test.shape}\")\n",
    "\n",
    "# Step 3: Apply your text cleaning function to all splits if needed\n",
    "data_train['preprocessed_text'] = data_train['text'].apply(clean_text)\n",
    "data_val['preprocessed_text'] = data_val['text'].apply(clean_text)\n",
    "data_test['preprocessed_text'] = data_test['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    # 1. Remove inline JavaScript/CSS:\n",
    "    raw_html = re.sub(r'<(script|style).*?>.*?</\\1>', '', raw_html, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # 2. Remove HTML comments:\n",
    "    raw_html = re.sub(r'<!--.*?-->', '', raw_html, flags=re.DOTALL)\n",
    "\n",
    "    # 3. Remove all remaining HTML tags:\n",
    "    raw_html = re.sub(r'<[^>]+>', '', raw_html)\n",
    "\n",
    "    # Optional: Replace multiple spaces/newlines with one\n",
    "    raw_html = re.sub(r'\\s+', ' ', raw_html).strip()\n",
    "\n",
    "    return raw_html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Remove special characters (keep only letters and space)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "    \n",
    "    # 2. Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 3. Remove all single characters (e.g., \"a\", \"x\", etc.)\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "\n",
    "    # 4. Remove single characters from the start (e.g., \"aTest\" → \"Test\")\n",
    "    text = re.sub(r'\\b\\w{1}\\b\\s*', '', text)\n",
    "\n",
    "    # 5. Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # 6. Remove prefixed 'b' if present (from byte strings like b'Text')\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "\n",
    "    # 7. Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Strip leading/trailing whitespace\n",
    "    return text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lovely\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\Lovely\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lovely\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lovely\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lovely\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # For wordnet data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    # Convert Treebank POS tags to WordNet's format\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text_pos(text):\n",
    "    words = text.split()\n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    all_words = ' '.join(corpus).split()\n",
    "    word_freq = Counter(all_words)\n",
    "    return word_freq.most_common(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['text'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If 0 = ham, 1 = spam (adjust if reversed)\n",
    "ham_msgs = data[data['label'] == 0]['clean_text']  # or your actual text column name\n",
    "spam_msgs = data[data['label'] == 1]['clean_text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Ham words: [('the', 14282), ('to', 8439), ('and', 6836), ('of', 6542), ('in', 5114), ('that', 3247), ('is', 2914), ('for', 2796), ('on', 2700), ('you', 2033)]\n",
      "Top 10 Spam words: [('the', 39757), ('to', 31509), ('of', 27934), ('and', 21762), ('in', 17976), ('you', 17941), ('this', 14583), ('my', 12159), ('your', 12029), ('for', 11837)]\n"
     ]
    }
   ],
   "source": [
    "top_ham_words = get_top_n_words(ham_msgs, 10)\n",
    "top_spam_words = get_top_n_words(spam_msgs, 10)\n",
    "\n",
    "print(\"Top 10 Ham words:\", top_ham_words)\n",
    "print(\"Top 10 Spam words:\", top_spam_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "preprocessed_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "money_mark",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "suspicious_words",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text_len",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "dddf3163-529a-4577-801c-1f125efe1495",
       "rows": [
        [
         "3821",
         "back -- we need to talkCall only my berry.",
         "0",
         "back we need to talkcall only my berry",
         "0",
         "0",
         "38"
        ],
        [
         "2407",
         "What is mullen's first name?",
         "0",
         "what is mullen first name",
         "0",
         "0",
         "25"
        ],
        [
         "1173",
         "Greetings from Dubai=2CThis letter must come to you as a big surprise=2C but I believe it is only a day that people meet and become great friends and business partners=2E  I am Mr=2E Arif Shaikh=2C currently Chief Credit & Risk Officer with a reputable bank here in U=2E A=2E E=2E  I write you this proposal in good faith=2Cbelieving that I can trust you with the information I am about to reveal to you=2EI have an urgent and very confidential business proposition for you=2E On November 6=2C 2000=2C an Iraqi Foreign Oil consultant=2Fcontractor with the CHEVRON PETROLEUM CORPORATION=2C MR MOHAMMADAL NASSER made a =28Fixed deposit=29 for 36 calendar months=2Cvalued at US$17=2C500=2C000=2E00 =28Seventeen Million Five hundred Thousand Dollars only=29 in my bank and I happen to be his account officer before I was moved to my present position recently=2E Upon maturity in 2003=2C as his account officer and as well the bank manger=2C it is my duty to notify him on the maturity date so I sent a routine notification to his forwarding address but the letter was returned undelivered=2E After sometime=2C I tried sending back the letter=2C but it was again returned and finally I discovered from his contract employers=2C Chevron Petroleum Corporation that Mr=2E Mohammad Al Nasser died as a result of torture in the hand of Saddam Hussein =28former Iraqi President=29 during one of his trips to his country Iraq=2C as he was accused of leaking information to the Americans=2E On further investigation=2C I discovered that Mr=2E Al Nasser's family wife and two sons died during the Gulf War in Iraq and was the reason why he did not declare any next of kin or relation in all his official documents=2C including his Bank Deposit paperwork in my Bank and did not leave any WILL=2E  This sum of US$17=2C500=2C000=2E00 have been floating and placed under dormant=2Funserviceable account by my bank management since no one have heard from the owner since 2003=2E  I wish to let you know that all the investigation I have made so far=2C my bank management is not aware of it=2C I am the only one that have the information=2EWith the recent change of government in my country and with their efforts to support the United Nations in checkmating terrorism aid in the U=2E A=2E E=2E  By end of this year=2C the government will pass a new financial control law which will give the government authority to interrogate account owners of above $5=2C000=2C000=2E00 to explain the source of the funds=2Cmaking sure it is not for terrorism support=2E  If I do not move this money out of the country immediately=2C by end of the year the government will definitely confiscate the money=2C because my bank cannot provide the account owner to explain the source of the money=2E  I cannot directly transfer out this money without the help of a foreigner and that is why I am contacting you for an assistance=2E  As the Account Officer to late Al Nasser=2Ccoupled with my present position and status in the bank as Chief Credit & Risk Officer =2C I have the power to influence the release of the funds to any foreigner that comes up as the next of kin to the account=2C with the correct information concerning the account=2C which I shall give you=2E All documents to enable you claim this fund will be carefully worked out and there is practically no risk involved=2C the transaction will be executed under a legitimate arrangement that will protect you from any breach of law=2C beside U=2E A=2E E is porous and anything goes=2EIf you accept to work with me=2C I want you to state how you wish us to share the funds in percentage=2C so that both parties will be satisfied=2E  If you are interested=2C contact me as soon as you receive this message  so we can go over the details=2E  Thanking you in advance and may God bless you=2E Please=2C treat with utmost confidentiality=2E  I shall send you copy of thedeposit certificate issued to Al Nasser when the deposit was made for your perusal=2E   I wait your urgent response=2E   Regards=2C Mr=2E Arif Shaikh",
         "1",
         "greetings from dubai cthis letter must come to you as big surprise but believe it is only day that people meet and become great friends and business partners am mr arif shaikh currently chief credit risk officer with reputable bank here in write you this proposal in good faith cbelieving that can trust you with the information am about to reveal to you ei have an urgent and very confidential business proposition for you on november an iraqi foreign oil consultant fcontractor with the chevron petroleum corporation mr mohammadal nasser made fixed deposit for calendar months cvalued at us seventeen million five hundred thousand dollars only in my bank and happen to be his account officer before was moved to my present position recently upon maturity in as his account officer and as well the bank manger it is my duty to notify him on the maturity date so sent routine notification to his forwarding address but the letter was returned undelivered after sometime tried sending back the letter but it was again returned and finally discovered from his contract employers chevron petroleum corporation that mr mohammad al nasser died as result of torture in the hand of saddam hussein former iraqi president during one of his trips to his country iraq as he was accused of leaking information to the americans on further investigation discovered that mr al nasser family wife and two sons died during the gulf war in iraq and was the reason why he did not declare any next of kin or relation in all his official documents including his bank deposit paperwork in my bank and did not leave any will this sum of us have been floating and placed under dormant funserviceable account by my bank management since no one have heard from the owner since wish to let you know that all the investigation have made so far my bank management is not aware of it am the only one that have the information ewith the recent change of government in my country and with their efforts to support the united nations in checkmating terrorism aid in the by end of this year the government will pass new financial control law which will give the government authority to interrogate account owners of above to explain the source of the funds cmaking sure it is not for terrorism support if do not move this money out of the country immediately by end of the year the government will definitely confiscate the money because my bank cannot provide the account owner to explain the source of the money cannot directly transfer out this money without the help of foreigner and that is why am contacting you for an assistance as the account officer to late al nasser ccoupled with my present position and status in the bank as chief credit risk officer have the power to influence the release of the funds to any foreigner that comes up as the next of kin to the account with the correct information concerning the account which shall give you all documents to enable you claim this fund will be carefully worked out and there is practically no risk involved the transaction will be executed under legitimate arrangement that will protect you from any breach of law beside is porous and anything goes eif you accept to work with me want you to state how you wish us to share the funds in percentage so that both parties will be satisfied if you are interested contact me as soon as you receive this message so we can go over the details thanking you in advance and may god bless you please treat with utmost confidentiality shall send you copy of thedeposit certificate issued to al nasser when the deposit was made for your perusal wait your urgent response regards mr arif shaikh",
         "1",
         "1",
         "3657"
        ],
        [
         "4231",
         "Sullivan Jacob J <SullivanJJ@state.gov>Friday December 4 2009 3:31 AMIranThe EU meets in the coming days and we are hoping for a strong public - and private - position on Iran. Bill hasidentified 5 countries that need touching to help drive a good outcome:I know Huma has discussed with you but a 2-minute discussion with each that underscores the key points reflected onyour card would do the trick if you can swing it.Tx. Also the intervention with your modifications turned out well. The process in this case did not generate a good Ã¢ÂÂ¢enough product -- I tried to make it clearer and stronger this morning and your amendments helped a lot.",
         "0",
         "sullivan jacob sullivanjj state gov friday december amiranthe eu meets in the coming days and we are hoping for strong public and private position on iran bill hasidentified countries that need touching to help drive good outcome know huma has discussed with you but minute discussion with each that underscores the key points reflected onyour card would do the trick if you can swing it tx also the intervention with your modifications turned out well the process in this case did not generate good enough product tried to make it clearer and stronger this morning and your amendments helped lot",
         "0",
         "1",
         "596"
        ],
        [
         "1666",
         "Fyi",
         "0",
         "fyi",
         "0",
         "0",
         "3"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3821</th>\n",
       "      <td>back -- we need to talkCall only my berry.</td>\n",
       "      <td>0</td>\n",
       "      <td>back we need to talkcall only my berry</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>What is mullen's first name?</td>\n",
       "      <td>0</td>\n",
       "      <td>what is mullen first name</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>Greetings from Dubai=2CThis letter must come t...</td>\n",
       "      <td>1</td>\n",
       "      <td>greetings from dubai cthis letter must come to...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4231</th>\n",
       "      <td>Sullivan Jacob J &lt;SullivanJJ@state.gov&gt;Friday ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sullivan jacob sullivanjj state gov friday dec...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>Fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "3821         back -- we need to talkCall only my berry.      0   \n",
       "2407                       What is mullen's first name?      0   \n",
       "1173  Greetings from Dubai=2CThis letter must come t...      1   \n",
       "4231  Sullivan Jacob J <SullivanJJ@state.gov>Friday ...      0   \n",
       "1666                                                Fyi      0   \n",
       "\n",
       "                                      preprocessed_text  money_mark  \\\n",
       "3821             back we need to talkcall only my berry           0   \n",
       "2407                          what is mullen first name           0   \n",
       "1173  greetings from dubai cthis letter must come to...           1   \n",
       "4231  sullivan jacob sullivanjj state gov friday dec...           0   \n",
       "1666                                                fyi           0   \n",
       "\n",
       "      suspicious_words  text_len  \n",
       "3821                 0        38  \n",
       "2407                 0        25  \n",
       "1173                 1      3657  \n",
       "4231                 1       596  \n",
       "1666                 0         3  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "train_data['money_mark'] = train_data['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "train_data['suspicious_words'] = train_data['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "train_data['text_len'] = train_data['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a way to convert text into numbers so a machine learning model can understand it.\n",
    "\n",
    "Each unique word in your entire text corpus is treated as a feature (a column).\n",
    "\n",
    "For each document (like an email or sentence), you count how many times each word appears.\n",
    "\n",
    "The result is a matrix where:\n",
    "\n",
    "Rows = documents\n",
    "\n",
    "Columns = unique words (vocabulary)\n",
    "\n",
    "Values = counts of words in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 1 1 1 0]\n",
      " [0 0 1 0 1 1 0 1 0]\n",
      " [1 0 0 1 1 0 0 0 1]]\n",
      "['ai' 'and' 'fun' 'future' 'is' 'learning' 'love' 'machine' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample data - list of documents (strings)\n",
    "documents = [\n",
    "    \"I love AI and machine learning\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"AI is the future\"\n",
    "]\n",
    "\n",
    "# Create the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit on documents and transform text to BoW counts\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array for visualization\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "# Get feature names (words)\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (3578, 77348)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assume your dataset text column is called 'preprocessed_text'\n",
    "# For example, on your training data:\n",
    "texts = data_train['preprocessed_text'].tolist()\n",
    "\n",
    "# Step 1: Load the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform your dataset\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "# Step 3: Print the shape of the vectorized dataset\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_transform() learns the vocabulary and IDF from texts, then transforms texts into a TF-IDF-weighted sparse matrix.\n",
    "\n",
    "The .shape prints (number_of_documents, number_of_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9815590947191953\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       670\n",
      "           1       1.00      0.96      0.98       523\n",
      "\n",
      "    accuracy                           0.98      1193\n",
      "   macro avg       0.98      0.98      0.98      1193\n",
      "weighted avg       0.98      0.98      0.98      1193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assume you already have:\n",
    "# data_train and data_val splits\n",
    "# TF-IDF vectorizer fitted on train data (tfidf_vectorizer)\n",
    "\n",
    "# 1. Vectorize train and validation text\n",
    "X_train = tfidf_vectorizer.transform(data_train['preprocessed_text'])\n",
    "X_val = tfidf_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# 2. Get labels\n",
    "y_train = data_train['label']\n",
    "y_val = data_val['label']\n",
    "\n",
    "# 3. Initialize the classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# 4. Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict on validation set\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# 6. Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\Lovely\\\\Desktop\\\\AI_Worklab\\\\Lab_Week4_Aug11\\\\lab_extra_kaggle_ham_or_spam\\\\kg_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Download once\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# nltk.download('punkt')\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# nltk.download('stopwords')\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# nltk.download('wordnet')\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m train_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mLovely\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDesktop\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mAI_Worklab\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mLab_Week4_Aug11\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mlab_extra_kaggle_ham_or_spam\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mkg_train.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin-1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Preprocessing\u001b[39;00m\n\u001b[32m     28\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'C:\\\\Users\\\\Lovely\\\\Desktop\\\\AI_Worklab\\\\Lab_Week4_Aug11\\\\lab_extra_kaggle_ham_or_spam\\\\kg_train.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Download once\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load data\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    r\"C:\\Users\\Lovely\\Desktop\\AI_Worklab\\Lab_Week4_Aug11\\lab_extra_kaggle_ham_or_spam\\kg_train.csv\",\n",
    "    encoding='latin-1'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_data['clean_text'] = train_data['text'].astype(str).apply(preprocess_text)\n",
    "\n",
    "# Optional extra features\n",
    "def add_extra_features(df):\n",
    "    df['text_len'] = df['text'].apply(len)\n",
    "    df['money_mark'] = df['text'].apply(lambda x: 1 if '$' in x or '£' in x else 0)\n",
    "    suspicious = ['free', 'win', 'winner', 'cash', 'urgent']\n",
    "    df['suspicious_words'] = df['text'].apply(lambda x: sum([1 for word in suspicious if word in x.lower()]))\n",
    "    return df\n",
    "\n",
    "train_data = add_extra_features(train_data)\n",
    "\n",
    "# Split for evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, train_data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizers\n",
    "count_vec = CountVectorizer()\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# --- A. Bag of Words Only ---\n",
    "X_bow = count_vec.fit_transform(X_train['clean_text'])\n",
    "X_val_bow = count_vec.transform(X_val['clean_text'])\n",
    "\n",
    "model_bow = MultinomialNB()\n",
    "model_bow.fit(X_bow, y_train)\n",
    "pred_bow = model_bow.predict(X_val_bow)\n",
    "acc_bow = accuracy_score(y_val, pred_bow)\n",
    "\n",
    "# --- B. TF-IDF Only ---\n",
    "X_tfidf = tfidf_vec.fit_transform(X_train['clean_text'])\n",
    "X_val_tfidf = tfidf_vec.transform(X_val['clean_text'])\n",
    "\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_tfidf, y_train)\n",
    "pred_tfidf = model_tfidf.predict(X_val_tfidf)\n",
    "acc_tfidf = accuracy_score(y_val, pred_tfidf)\n",
    "\n",
    "# --- C. Bag of Words + Extra Features ---\n",
    "X_bow_extra = pd.concat([\n",
    "    pd.DataFrame(X_bow.toarray()),\n",
    "    X_train[['text_len', 'money_mark', 'suspicious_words']].reset_index(drop=True)\n",
    "], axis=1)\n",
    "X_val_bow_extra = pd.concat([\n",
    "    pd.DataFrame(X_val_bow.toarray()),\n",
    "    X_val[['text_len', 'money_mark', 'suspicious_words']].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "model_bow_extra = MultinomialNB()\n",
    "model_bow_extra.fit(X_bow_extra, y_train)\n",
    "pred_bow_extra = model_bow_extra.predict(X_val_bow_extra)\n",
    "acc_bow_extra = accuracy_score(y_val, pred_bow_extra)\n",
    "\n",
    "# --- D. TF-IDF + Extra Features ---\n",
    "X_tfidf_extra = pd.concat([\n",
    "    pd.DataFrame(X_tfidf.toarray()),\n",
    "    X_train[['text_len', 'money_mark', 'suspicious_words']].reset_index(drop=True)\n",
    "], axis=1)\n",
    "X_val_tfidf_extra = pd.concat([\n",
    "    pd.DataFrame(X_val_tfidf.toarray()),\n",
    "    X_val[['text_len', 'money_mark', 'suspicious_words']].reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "model_tfidf_extra = MultinomialNB()\n",
    "model_tfidf_extra.fit(X_tfidf_extra, y_train)\n",
    "pred_tfidf_extra = model_tfidf_extra.predict(X_val_tfidf_extra)\n",
    "acc_tfidf_extra = accuracy_score(y_val, pred_tfidf_extra)\n",
    "\n",
    "# --- RESULTS ---\n",
    "print(\"📊 Accuracy Comparison:\")\n",
    "print(f\"Bag of Words only: {acc_bow:.4f}\")\n",
    "print(f\"TF-IDF only: {acc_tfidf:.4f}\")\n",
    "print(f\"BoW + extra features: {acc_bow_extra:.4f}\")\n",
    "print(f\"TF-IDF + extra features: {acc_tfidf_extra:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
