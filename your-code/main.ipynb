{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup, Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def clean_html_text(text):\n",
    "\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1.\n",
    "    text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL|re.IGNORECASE)\n",
    "    text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL|re.IGNORECASE)\n",
    "    \n",
    "    # 2.\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # 3.\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text(separator=' ')\n",
    "    \n",
    "    # 4.\n",
    "    text = re.sub(r'[^\\x00-\\x7F]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "data_train['cleaned_text'] = data_train['text'].apply(clean_html_text)\n",
    "data_val['cleaned_text'] = data_val['text'].apply(clean_html_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove all special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove single characters from anywhere in the text\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start of the string\n",
    "    text = re.sub(r'^\\w\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b' (common when converting bytes to string)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "data_train['preprocessed_text'] = data_train['cleaned_text'].apply(preprocess_text)\n",
    "data_val['preprocessed_text'] = data_val['cleaned_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "data_train['no_stopwords'] = data_train['preprocessed_text'].apply(remove_stopwords)\n",
    "data_val['no_stopwords'] = data_val['preprocessed_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['lemmatized_text'] = data_train['no_stopwords'].apply(lemmatize_text)\n",
    "data_val['lemmatized_text'] = data_val['no_stopwords'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham messages:\n",
      "[('u', 99), ('would', 93), ('state', 92), ('pm', 89), ('president', 84), ('percent', 76), ('call', 73), ('secretary', 71), ('time', 70), ('mr', 70)]\n",
      "[('district', 38), ('pm', 26), ('call', 18), ('u', 16), ('time', 14), ('policy', 14), ('new', 13), ('would', 13), ('see', 12), ('middle', 12)]\n",
      "\n",
      "Top 10 words in spam messages:\n",
      "[('money', 800), ('account', 666), ('bank', 608), ('fund', 569), ('u', 446), ('business', 395), ('transaction', 354), ('country', 342), ('transfer', 332), ('company', 321)]\n",
      "[('bank', 140), ('fund', 139), ('account', 135), ('money', 126), ('u', 107), ('business', 81), ('million', 73), ('country', 71), ('foreign', 70), ('transaction', 70)]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Separate ham (label 0) and spam (label 1) messages\n",
    "ham_texts_train = data_train[data_train['label'] == 0]['lemmatized_text']\n",
    "spam_texts_train = data_train[data_train['label'] == 1]['lemmatized_text']\n",
    "\n",
    "ham_texts_val = data_val[data_val['label'] == 0]['lemmatized_text']\n",
    "spam_texts_val = data_val[data_val['label'] == 1]['lemmatized_text']\n",
    "\n",
    "# Function to get all words from a series of texts\n",
    "def get_all_words(texts):\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        all_words.extend(text.split())\n",
    "    return all_words\n",
    "\n",
    "# Get all words from each category\n",
    "ham_words_train = get_all_words(ham_texts_train)\n",
    "spam_words_train = get_all_words(spam_texts_train)\n",
    "\n",
    "ham_words_val = get_all_words(ham_texts_val)\n",
    "spam_words_val = get_all_words(spam_texts_val)\n",
    "\n",
    "# Count the most common words\n",
    "top_ham_train = Counter(ham_words_train).most_common(10)\n",
    "top_spam_train = Counter(spam_words_train).most_common(10)\n",
    "\n",
    "top_ham_val = Counter(ham_words_val).most_common(10)\n",
    "top_spam_val = Counter(spam_words_val).most_common(10)\n",
    "\n",
    "print(\"Top 10 words in ham messages:\")\n",
    "print(top_ham_train)\n",
    "print(top_ham_val)\n",
    "\n",
    "print(\"\\nTop 10 words in spam messages:\")\n",
    "print(top_spam_train)\n",
    "print(top_spam_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>1</td>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>regards mr nelson smithkindly reply me on my p...</td>\n",
       "      <td>regards mr nelson smithkindly reply private em...</td>\n",
       "      <td>regard mr nelson smithkindly reply private ema...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>0</td>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>have not been able to reach oscar this am we a...</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>huma abedin bim checking with pat on the will ...</td>\n",
       "      <td>huma abedin bim checking pat work jack jake re...</td>\n",
       "      <td>huma abedin bim checking pat work jack jake re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>can have it announced here on monday cant today</td>\n",
       "      <td>announced monday cant today</td>\n",
       "      <td>announced monday cant today</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>1</td>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...</td>\n",
       "      <td>bank of africaagence san pedro bp san pedro co...</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
       "535  I have not been able to reach oscar this am. W...      0   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
       "557  I can have it announced here on Monday - can't...      0   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
       "\n",
       "                                          cleaned_text  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...   \n",
       "535  I have not been able to reach oscar this am. W...   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...   \n",
       "557  I can have it announced here on Monday - can't...   \n",
       "836  BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...   \n",
       "\n",
       "                                     preprocessed_text  \\\n",
       "29   regards mr nelson smithkindly reply me on my p...   \n",
       "535  have not been able to reach oscar this am we a...   \n",
       "695  huma abedin bim checking with pat on the will ...   \n",
       "557    can have it announced here on monday cant today   \n",
       "836  bank of africaagence san pedro bp san pedro co...   \n",
       "\n",
       "                                          no_stopwords  \\\n",
       "29   regards mr nelson smithkindly reply private em...   \n",
       "535         able reach oscar supposed send pdb receive   \n",
       "695  huma abedin bim checking pat work jack jake re...   \n",
       "557                        announced monday cant today   \n",
       "836  bank africaagence san pedro bp san pedro cote ...   \n",
       "\n",
       "                                       lemmatized_text  money_mark  \\\n",
       "29   regard mr nelson smithkindly reply private ema...           0   \n",
       "535         able reach oscar supposed send pdb receive           0   \n",
       "695  huma abedin bim checking pat work jack jake re...           0   \n",
       "557                        announced monday cant today           0   \n",
       "836  bank africaagence san pedro bp san pedro cote ...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0        75  \n",
       "535                 0        42  \n",
       "695                 0        79  \n",
       "557                 0        27  \n",
       "836                 1      1067  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['lemmatized_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['lemmatized_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['lemmatized_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['lemmatized_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['lemmatized_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['lemmatized_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abacha' 'abachac' 'abachae' 'abad' 'abandoned' 'abbas' 'abdul'\n",
      " 'abdullah' 'abedin' 'abidjan']\n",
      "Train shape: (800, 4889)\n",
      "Validation shape: (200, 4889)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=2, token_pattern=r'\\b\\w{4,}\\b', stop_words='english')\n",
    "\n",
    "# Fit and transform on training data\n",
    "X_train_bow = vectorizer.fit_transform(data_train['lemmatized_text'])\n",
    "\n",
    "# Transform validation data\n",
    "X_val_bow = vectorizer.transform(data_val['lemmatized_text'])\n",
    "\n",
    "# To inspect the vocabulary\n",
    "print(vectorizer.get_feature_names_out()[:10])  # Show first 10 words\n",
    "\n",
    "# To see the shape of the transformed matrices\n",
    "print(\"Train shape:\", X_train_bow.shape)\n",
    "print(\"Validation shape:\", X_val_bow.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TF-IDF shape: (800, 16689)\n",
      "Validation TF-IDF shape: (200, 16689)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['lemmatized_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val['lemmatized_text'])\n",
    "\n",
    "print(\"Train TF-IDF shape:\", X_train_tfidf.shape)\n",
    "print(\"Validation TF-IDF shape:\", X_val_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_tfidf, data_train['label'])\n",
    "\n",
    "y_pred = clf.predict(X_val_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95       125\n",
      "           1       0.86      1.00      0.93        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.93      0.95      0.94       200\n",
      "weighted avg       0.95      0.94      0.94       200\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[113  12]\n",
      " [  0  75]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(data_val['label'], y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(data_val['label'], y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(data_val['label'], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
