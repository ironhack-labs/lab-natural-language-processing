{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:19:32.690643Z",
     "start_time": "2024-10-01T15:19:32.589738Z"
    }
   },
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hibi9\\AppData\\Local\\Temp\\ipykernel_28356\\1877911641.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:19:37.586254Z",
     "start_time": "2024-10-01T15:19:33.345734Z"
    }
   },
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.corpus.reader.wordnet as wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hibi9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:19:39.918703Z",
     "start_time": "2024-10-01T15:19:39.517786Z"
    }
   },
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\", inplace=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:19:40.175468Z",
     "start_time": "2024-10-01T15:19:40.159040Z"
    }
   },
   "source": "# I will split them later, after all the cleanup below, to prevent doing this on 2 datas",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:19:40.362023Z",
     "start_time": "2024-10-01T15:19:40.318542Z"
    }
   },
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer('english')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:19:43.464926Z",
     "start_time": "2024-10-01T15:19:40.644570Z"
    }
   },
   "source": [
    "def remove_html_elements(text):\n",
    "    # Remove inline CSS\n",
    "    text = re.sub(r'<style.*?</style>', '', text)\n",
    "    text = re.sub(r'(<[^>]+)\\sstyle=\".*?\"', r'\\1', text)\n",
    "\n",
    "    # Remove Javascript\n",
    "    text = re.sub(r'<script.*?</script>', '', text)\n",
    "\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'', '', text)\n",
    "\n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove leading or ending spaces\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "data['preprocessed_text'] = data['text'].apply(remove_html_elements)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:19:45.109579Z",
     "start_time": "2024-10-01T15:19:43.546676Z"
    }
   },
   "source": [
    "def clean_text(text):\n",
    "    # Remove special Characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove all single caracters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "\n",
    "    # Remove single caracters at the beginning\n",
    "    text = re.sub(r'^\\w\\s', '', text)\n",
    "\n",
    "    # replace multiple spaces with just 1\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove prefix \"b\"\n",
    "    text = re.sub(r'^b\\s', '', text)\n",
    "\n",
    "    # to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(clean_text)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:19:47.657697Z",
     "start_time": "2024-10-01T15:19:45.189946Z"
    }
   },
   "source": [
    "words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in words])\n",
    "    return text\n",
    "\n",
    "\n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(remove_stopwords)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:20:41.844786Z",
     "start_time": "2024-10-01T15:19:47.723845Z"
    }
   },
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0]\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize(line):\n",
    "    final_doc = []\n",
    "    for word in line.split():\n",
    "        final_doc.append(WordNetLemmatizer().lemmatize(word, pos=get_wordnet_pos(word)))\n",
    "    return \" \".join(final_doc)\n",
    "\n",
    "\n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(lemmatize)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:20:42.005557Z",
     "start_time": "2024-10-01T15:20:41.946184Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  label  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
       "1                                           Will do.      0   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
       "4                                                fyi      0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  dear sir strictly private business proposal mi...  \n",
       "1                                                     \n",
       "2  noracheryl email dozen memo haiti weekend plea...  \n",
       "3  dear sirfmadamc know proposal might surprise e...  \n",
       "4                                                fyi  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>noracheryl email dozen memo haiti weekend plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sirfmadamc know proposal might surprise e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:20:42.679299Z",
     "start_time": "2024-10-01T15:20:42.528035Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "ham_texts = data[data['label'] == 0]\n",
    "spam_texts = data[data['label'] == 1]\n",
    "\n",
    "# Count the words in both textas\n",
    "ham_word_counts = Counter(\" \".join(ham_texts['preprocessed_text']).split())\n",
    "spam_word_counts = Counter(\" \".join(spam_texts['preprocessed_text']).split())\n",
    "\n",
    "# Top 10 words\n",
    "top_10_ham_words = ham_word_counts.most_common(10)\n",
    "top_10_spam_words = spam_word_counts.most_common(10)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:20:43.227150Z",
     "start_time": "2024-10-01T15:20:43.199097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now I split the data\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.3, random_state=42, stratify=data['label'])"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:20:44.179194Z",
     "start_time": "2024-10-01T15:20:43.785108Z"
    }
   },
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"â‚¬\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list) * 1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words) * 1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list) * 1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words) * 1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x))\n",
    "\n",
    "data_train.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  text  label  \\\n",
       "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0   \n",
       "428  H <hrod17@clintonemail.com >Saturday January 2...      0   \n",
       "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1   \n",
       "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1   \n",
       "380  Hello,This is Dr.Clive Whittaker. I work for F...      1   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "381  mill cheryl sunday january pmtravel scheduleca...           1   \n",
       "428  saturday january pmsbwhoeopre fyi foreign nati...           1   \n",
       "849  dear name mr mr ken edwarda former government ...           1   \n",
       "252  dear sir engr victor chigoziem engineering sto...           1   \n",
       "380  hellothis drclive whittaker work fidelity inve...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "381                 0       104  \n",
       "428                 0       115  \n",
       "849                 1       875  \n",
       "252                 1      2224  \n",
       "380                 1       852  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Mills Cheryl D &lt;MillsCD@state.gov&gt;Sunday Janua...</td>\n",
       "      <td>0</td>\n",
       "      <td>mill cheryl sunday january pmtravel scheduleca...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>H &lt;hrod17@clintonemail.com &gt;Saturday January 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>saturday january pmsbwhoeopre fyi foreign nati...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>DEAR,     MY NAME IS MR MR Ken Edward,A former...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear name mr mr ken edwarda former government ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Dear Sir, I am Engr. Victor Chigoziem with the...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sir engr victor chigoziem engineering sto...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Hello,This is Dr.Clive Whittaker. I work for F...</td>\n",
       "      <td>1</td>\n",
       "      <td>hellothis drclive whittaker work fidelity inve...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:20:46.825044Z",
     "start_time": "2024-10-01T15:20:45.651206Z"
    }
   },
   "source": [
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "\n",
    "cv_ham = count_vectorizer.fit_transform(ham_texts['preprocessed_text'])\n",
    "cv_spam = count_vectorizer.fit_transform(spam_texts['preprocessed_text'])"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:20:49.228514Z",
     "start_time": "2024-10-01T15:20:47.202039Z"
    }
   },
   "source": [
    "tfidfvector = TfidfVectorizer(ngram_range=(2, 2))\n",
    "tfidf_ham = tfidfvector.fit_transform(ham_texts['preprocessed_text'])\n",
    "tfidf_spam = tfidfvector.fit_transform(spam_texts['preprocessed_text'])\n",
    "\n",
    "print(tfidf_ham.shape)\n",
    "print(tfidf_spam.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 17087)\n",
      "(442, 46010)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:20:54.908867Z",
     "start_time": "2024-10-01T15:20:49.729737Z"
    }
   },
   "source": [
    "all_texts = pd.concat([ham_texts['preprocessed_text'], spam_texts['preprocessed_text']], ignore_index=True)\n",
    "all_classes = pd.concat([ham_texts['label'], spam_texts['label']], ignore_index=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorized_data = vectorizer.fit_transform(all_texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, all_classes, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "  \n",
    "y_pred = classifier.predict(X_test)\n",
    "  \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy\", accuracy)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.915\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:20:55.368573Z",
     "start_time": "2024-10-01T15:20:55.342695Z"
    }
   },
   "source": [
    "# Your code"
   ],
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
