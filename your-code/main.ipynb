{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reads data for the fraudulent email Kaggle Challenge\n",
    "train_data = pd.read_csv(\"/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduces the training set to speed up development. \n",
    "train_data = train_data.head(1000)\n",
    "print(train_data.shape)\n",
    "train_data.fillna(\"\",inplace=True)\n",
    "\n",
    "#train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loads the CSV file into a Pandas DataFrame\n",
    "# train_data = pd.read_csv(\"/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train.csv\",encoding='latin-1')\n",
    "test_data = pd.read_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_test.csv')\n",
    "\n",
    "# Specifies the size of the training validation set\n",
    "train_validation_size = 0.2  # 20% of the training data for validation\n",
    "test_validation_size = 0.2 # 20% of the test data for validation\n",
    "\n",
    "# Splits the datasets into training and validation sets\n",
    "train_set, validation_set = train_test_split(train_data, test_size=train_validation_size, random_state=42)\n",
    "test_set, test_validation_set = train_test_split(test_data, test_size=test_validation_size, random_state=42)\n",
    "\n",
    "# Prints the sizes of each dataset (training and training validation)\n",
    "print(f\"Training Data Splits\")\n",
    "print(f\"Total samples: {len(train_data)}\")\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Validation set size: {len(validation_set)}\")\n",
    "\n",
    "# Prints the sizes of each dataset (test & test validation)\n",
    "print(f\"Test Data Splits\")\n",
    "print(f\"Total samples: {len(test_data)}\")\n",
    "print(f\"Training set size: {len(test_set)}\")\n",
    "print(f\"Validation set size: {len(test_validation_set)}\")\n",
    "\n",
    "# Saves the splits to new CSV files if needed\n",
    "train_set.to_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split.csv', index=False)\n",
    "validation_set.to_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_validation_split.csv', index=False)\n",
    "test_set.to_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_test_split.csv', index=False)\n",
    "test_validation_set.to_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_test_validation_split.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')  # Downloads the stopwords resource\n",
    "# from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Loads the training data\n",
    "data = pd.read_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split.csv')\n",
    "\n",
    "# Specifies the column containing the text data\n",
    "text_column = 'text'\n",
    "\n",
    "# Defines a function to clean the text content\n",
    "def clean_text(text):\n",
    "    # Handle missing values\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Removes inline JavaScript/CSS (script and style tags)\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()  # Remove these tags completely\n",
    "\n",
    "    # Removes HTML comments\n",
    "    no_comments = re.sub(r'<!--.*?-->', '', str(soup), flags=re.DOTALL)\n",
    "\n",
    "    # Removes remaining HTML tags\n",
    "    no_tags = re.sub(r'<[^>]+>', '', no_comments)\n",
    "\n",
    "    # Removes extra whitespace\n",
    "    clean_text = re.sub(r'\\s+', ' ', no_tags).strip()\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "# Applies the cleaning function to the 'text' column\n",
    "data['cleaned_text'] = data[text_column].apply(clean_text)\n",
    "\n",
    "# Saves the cleaned data to a new CSV file\n",
    "output_file = '/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split_cleaned.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Loads the training data\n",
    "data = pd.read_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split.csv')\n",
    "\n",
    "# Specifies the column containing the text data\n",
    "text_column = 'text'\n",
    "\n",
    "# Defines a function to clean text content\n",
    "def clean_text(text):\n",
    "    # Handle missing values\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Removes inline JavaScript/CSS (script and style tags)\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    # Removes HTML comments\n",
    "    no_comments = re.sub(r'<!--.*?-->', '', str(soup), flags=re.DOTALL)\n",
    "\n",
    "    # Removes remaining HTML tags\n",
    "    no_tags = re.sub(r'<[^>]+>', '', no_comments)\n",
    "\n",
    "    # Removes special characters and numbers\n",
    "    no_special_chars = re.sub(r'[^A-Za-z\\s]', '', no_tags)\n",
    "\n",
    "    # Removes single characters\n",
    "    no_single_chars = re.sub(r'\\b\\w\\b', '', no_special_chars)\n",
    "\n",
    "    # Substitutes multiple spaces with a single space\n",
    "    single_spaced = re.sub(r'\\s+', ' ', no_single_chars).strip()\n",
    "\n",
    "    # Removes prefixed 'b' (if any)\n",
    "    no_prefixed_b = re.sub(r'^b\\s+', '', single_spaced)\n",
    "\n",
    "    # Converts text to lowercase\n",
    "    cleaned_text = no_prefixed_b.lower()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Applies the cleaning function to the 'text' column\n",
    "data['cleaned_text'] = data[text_column].apply(clean_text)\n",
    "\n",
    "# Saves the cleaned data to a new CSV file\n",
    "output_file = '/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split_cleaned.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"Cleaned data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes stopwords\n",
    "# Added to previous cleaning steps\n",
    "\n",
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Loads the training data\n",
    "data = pd.read_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split.csv')\n",
    "\n",
    "# Specifies the column containing the text data\n",
    "text_column = 'text'\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean text content and remove stopwords\n",
    "def clean_text(text):\n",
    "    # Handle missing values\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Removes inline JavaScript/CSS (script and style tags)\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "\n",
    "    # Removes HTML comments\n",
    "    no_comments = re.sub(r'<!--.*?-->', '', str(soup), flags=re.DOTALL)\n",
    "\n",
    "    # Removes remaining HTML tags\n",
    "    no_tags = re.sub(r'<[^>]+>', '', no_comments)\n",
    "\n",
    "    # Removes special characters and numbers\n",
    "    no_special_chars = re.sub(r'[^A-Za-z\\s]', '', no_tags)\n",
    "\n",
    "    # Removes single characters\n",
    "    no_single_chars = re.sub(r'\\b\\w\\b', '', no_special_chars)\n",
    "\n",
    "    # Substitutes multiple spaces with a single space\n",
    "    single_spaced = re.sub(r'\\s+', ' ', no_single_chars).strip()\n",
    "\n",
    "    # Removes prefixed 'b' (if any)\n",
    "    no_prefixed_b = re.sub(r'^b\\s+', '', single_spaced)\n",
    "\n",
    "    # Converts text to lowercase\n",
    "    cleaned_text = no_prefixed_b.lower()\n",
    "\n",
    "    # Removes stopwords\n",
    "    words = cleaned_text.split()\n",
    "    filtered_text = ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "# Applies the cleaning function to the 'text' column\n",
    "data['cleaned_text'] = data[text_column].apply(clean_text)\n",
    "\n",
    "# Saves the cleaned data to a new CSV file\n",
    "output_file = '/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split_cleaned_no_stopwords.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"Cleaned data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaks sentences into words, then uses lemmatization to reduce them to their base form.\n",
    "\n",
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Downloads the necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')  # WordNet for lemmatization\n",
    "nltk.download('omw-1.4')  # WordNet's optional multilingual data\n",
    "\n",
    "# Loads the dataset\n",
    "data = pd.read_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split.csv')\n",
    "\n",
    "# Specifies the column containing the text data\n",
    "text_column = 'text'\n",
    "\n",
    "# Initializes the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Loads stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Defines a function to clean and lemmatize text\n",
    "def clean_and_lemmatize(text):\n",
    "    # Handle missing values\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Removes inline JavaScript/CSS and HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "    text = re.sub(r'<[^>]+>', '', str(soup))\n",
    "    \n",
    "    # Removes special characters, numbers, and extra spaces\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Keep only alphabets and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove multiple spaces\n",
    "\n",
    "    # Converts to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removes stopwords and lemmatize each word\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "    # Joins the lemmatized words back into a sentence\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Applies the cleaning and lemmatization function\n",
    "data['cleaned_lemmatized_text'] = data[text_column].apply(clean_and_lemmatize)\n",
    "\n",
    "# Saves the cleaned data to a new CSV file\n",
    "output_file = '/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split_cleaned_lemmatized.csv'\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Lemmatized data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies the top 10 words in ham and spam messages.\n",
    "\n",
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Loads the dataset\n",
    "data = pd.read_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split_cleaned_lemmatized.csv')\n",
    "\n",
    "# Replaces NaN or non-string values with empty strings in the 'cleaned_lemmatized_text' column\n",
    "data['cleaned_lemmatized_text'] = data['cleaned_lemmatized_text'].fillna(\"\").astype(str)\n",
    "\n",
    "# Separates ham (label=0) and spam (label=1) messages\n",
    "ham_messages = data[data['label'] == 0]['cleaned_lemmatized_text']\n",
    "spam_messages = data[data['label'] == 1]['cleaned_lemmatized_text']\n",
    "\n",
    "# Defines a function to tokenize and count word frequencies\n",
    "def get_top_words(messages, top_n=10):\n",
    "    all_words = ' '.join(messages).split()  # Tokenize\n",
    "    word_counts = Counter(all_words)  # Count word frequencies\n",
    "    return word_counts.most_common(top_n)\n",
    "\n",
    "# Gets the top 10 words for ham and spam\n",
    "top_ham_words = get_top_words(ham_messages, top_n=10)\n",
    "top_spam_words = get_top_words(spam_messages, top_n=10)\n",
    "\n",
    "# Prints the results\n",
    "print(\"Top 10 Words in Ham Messages:\")\n",
    "print(top_ham_words)\n",
    "\n",
    "print(\"\\nTop 10 Words in Spam Messages:\")\n",
    "print(top_spam_words)\n",
    "\n",
    "# Visualization: Bar plots for ham and spam\n",
    "def plot_top_words(top_words, title):\n",
    "    words, counts = zip(*top_words)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, counts, color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Plots the top words\n",
    "plot_top_words(top_ham_words, \"Top 10 Words in Ham Messages\")\n",
    "plot_top_words(top_spam_words, \"Top 10 Words in Spam Messages\")\n",
    "\n",
    "# Generates word clouds for ham and spam messages\n",
    "def plot_wordcloud(messages, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(messages))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_wordcloud(ham_messages, \"Word Cloud for Ham Messages\")\n",
    "plot_wordcloud(spam_messages, \"Word Cloud for Spam Messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays data file headers for verification\n",
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the dataset\n",
    "data_train = pd.read_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split_cleaned_lemmatized.csv')\n",
    "\n",
    "# Defines lists of money symbols and suspicious words\n",
    "money_symbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words_list = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \n",
    "                                   \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "# Adds indicators to the training dataset\n",
    "data_train['money_mark'] = data_train['cleaned_lemmatized_text'].str.contains(money_symbol_list, case=False, na=False).astype(int)\n",
    "data_train['suspicious_words'] = data_train['cleaned_lemmatized_text'].str.contains(suspicious_words_list, case=False, na=False).astype(int)\n",
    "data_train['text_len'] = data_train['cleaned_lemmatized_text'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "\n",
    "# Adds indicators to the validation dataset\n",
    "#data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_symbol_list, case=False, na=False).astype(int)\n",
    "#data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words_list, case=False, na=False).astype(int)\n",
    "#data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "\n",
    "# Displays the first rows of the training dataset with the new indicators\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Makes sure there are no missing values in the 'cleaned_lemmatized_text' column\n",
    "data_train['cleaned_lemmatized_text'] = data_train['cleaned_lemmatized_text'].fillna(\"\").astype(str)\n",
    "\n",
    "# Initializes the CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000,  # Limit to top 1000 features (optional)\n",
    "                             stop_words='english',  # Remove common stopwords\n",
    "                             ngram_range=(1, 1))  # Use unigrams (single words)\n",
    "\n",
    "# Fits and transforms the 'cleaned_lemmatized_text' column\n",
    "X_train_bow = vectorizer.fit_transform(data_train['cleaned_lemmatized_text'])\n",
    "\n",
    "# Converts the sparse matrix to a DataFrame for easy visualization\n",
    "bow_df = pd.DataFrame(X_train_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Displays the top rows of the bag-of-words DataFrame\n",
    "print(\"Bag of Words Representation (Top 5 Rows):\")\n",
    "print(bow_df.head())\n",
    "\n",
    "# Displays vocabulary (words mapped to indices)\n",
    "print(\"\\nVocabulary (Top 20 Words):\")\n",
    "print(list(vectorizer.get_feature_names_out())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ensures no missing values in the 'cleaned_lemmatized_text' column\n",
    "data_train['cleaned_lemmatized_text'] = data_train['cleaned_lemmatized_text'].fillna(\"\").astype(str)\n",
    "\n",
    "# Initializes the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # Limit to top 1000 features (optional)\n",
    "    stop_words='english',  # Remove common stopwords\n",
    "    ngram_range=(1, 1)  # Use unigrams (single words)\n",
    ")\n",
    "\n",
    "# Vectorizes the dataset (fit and transform on training set)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['cleaned_lemmatized_text'])\n",
    "\n",
    "# Prints the shape of the vectorized dataset\n",
    "print(\"Shape of the TF-IDF Vectorized Dataset:\")\n",
    "print(X_train_tfidf.shape)  # (num_samples, num_features)\n",
    "\n",
    "# Displays the feature names (vocabulary)\n",
    "print(\"\\nVocabulary (Top 20 Words):\")\n",
    "print(list(tfidf_vectorizer.get_feature_names_out())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Classifier\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Makes sure there are no missing values in the 'label' column\n",
    "data_train['label'] = data_train['label'].fillna(0).astype(int)\n",
    "\n",
    "# Splits the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_tfidf, data_train['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initializes the Logistic Regression model\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Trains the classifier on the training set\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Makes predictions on the validation set\n",
    "y_pred = classifier.predict(X_val)\n",
    "\n",
    "# Evaluates the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=[\"Ham\", \"Spam\"]))\n",
    "\n",
    "# Displays the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"Ham\", \"Spam\"])\n",
    "\n",
    "# Plots the confusion matrix\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=\"vertical\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Loads the dataset\n",
    "data = pd.read_csv('/Users/sylviaperez-montero/Desktop/lab-natural-language-processing-main/data/kg_train_split_cleaned_lemmatized.csv')\n",
    "\n",
    "# Makes sure there are no missing values\n",
    "data['cleaned_lemmatized_text'] = data['cleaned_lemmatized_text'].fillna(\"\").astype(str)\n",
    "\n",
    "# Splits the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['cleaned_lemmatized_text'], data['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Bag of Words (CountVectorizer)\n",
    "print(\"\\n--- Using Bag of Words Representation ---\")\n",
    "bow_vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# Trains the MultinomialNB classifier\n",
    "bow_classifier = MultinomialNB()\n",
    "bow_classifier.fit(X_train_bow, y_train)\n",
    "\n",
    "# Evaluates the Bag of Words model\n",
    "y_pred_bow = bow_classifier.predict(X_test_bow)\n",
    "print(\"Classification Report (Bag of Words):\")\n",
    "print(classification_report(y_test, y_pred_bow))\n",
    "\n",
    "# Experiment 2: TF-IDF\n",
    "print(\"\\n--- Using TF-IDF Representation ---\")\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Trains the MultinomialNB classifier\n",
    "tfidf_classifier = MultinomialNB()\n",
    "tfidf_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluates the TF-IDF model\n",
    "y_pred_tfidf = tfidf_classifier.predict(X_test_tfidf)\n",
    "print(\"Classification Report (TF-IDF):\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "# Experiment 3: N-Grams with TF-IDF\n",
    "print(\"\\n--- Using TF-IDF with N-Grams Representation ---\")\n",
    "ngram_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2))  # Unigrams + Bigrams\n",
    "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
    "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
    "\n",
    "# Trains the MultinomialNB classifier\n",
    "ngram_classifier = MultinomialNB()\n",
    "ngram_classifier.fit(X_train_ngram, y_train)\n",
    "\n",
    "# Evaluates the N-Gram model\n",
    "y_pred_ngram = ngram_classifier.predict(X_test_ngram)\n",
    "print(\"Classification Report (N-Grams):\")\n",
    "print(classification_report(y_test, y_pred_ngram))\n",
    "\n",
    "# Displays the confusion matrices for all experiments\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"Ham\", \"Spam\"])\n",
    "    disp.plot(cmap=\"Blues\", xticks_rotation=\"vertical\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- Confusion Matrices ---\")\n",
    "plot_confusion_matrix(y_test, y_pred_bow, \"Bag of Words Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, y_pred_tfidf, \"TF-IDF Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, y_pred_ngram, \"N-Grams Confusion Matrix\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
