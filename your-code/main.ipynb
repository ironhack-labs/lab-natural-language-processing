{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pbele\\Ironhack\\lab-natural-language-processing\\your-code\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\pbele\\\\Ironhack\\\\lab-natural-language-processing\\\\data\\\\kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (800, 2)\n",
      "Testing data shape: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' is the dataframe read in the previous steps.\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Testing data shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pbele\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(text):\n",
    "    # Remove JavaScript and CSS\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.S)\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.S)\n",
    "    # Remove remaining HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    return text\n",
    "\n",
    "# Print column names to check\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbele\\AppData\\Local\\Temp\\ipykernel_25952\\2184479602.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "C:\\Users\\pbele\\AppData\\Local\\Temp\\ipykernel_25952\\2184479602.py:10: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(text):\n",
    "    # Remove JavaScript and CSS\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.S)\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.S)\n",
    "    # Remove remaining HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    return text\n",
    "\n",
    "# Apply the clean_html function to the 'text' column\n",
    "data['cleaned_text'] = data['text'].apply(clean_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define stopwords set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "# Apply stopword removal to the 'cleaned_text' column and store the result in a new column 'processed_text'\n",
    "data['processed_text'] = data['cleaned_text'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# Apply lemmatization\n",
    "data['processed_text'] = data['processed_text'].apply(lemmatize_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No spam messages available for analysis.\n",
      "No ham messages available for analysis.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Filter out empty rows to ensure non-empty text data\n",
    "spam_messages = data[(data['label'] == 'spam') & (data['text'].str.strip() != '')]\n",
    "ham_messages = data[(data['label'] == 'ham') & (data['text'].str.strip() != '')]\n",
    "\n",
    "# Verify that there are non-empty messages in each category\n",
    "if spam_messages.empty:\n",
    "    print(\"No spam messages available for analysis.\")\n",
    "else:\n",
    "    # Initialize vectorizer with a maximum of 10 features (top 10 words)\n",
    "    vectorizer = CountVectorizer(max_features=10)\n",
    "    spam_bow = vectorizer.fit_transform(spam_messages['text'])\n",
    "    print(\"Top 10 words in spam messages:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "if ham_messages.empty:\n",
    "    print(\"No ham messages available for analysis.\")\n",
    "else:\n",
    "    # Reinitialize the vectorizer for ham messages\n",
    "    vectorizer = CountVectorizer(max_features=10)\n",
    "    ham_bow = vectorizer.fit_transform(ham_messages['text'])\n",
    "    print(\"Top 10 words in ham messages:\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>DEAR SIR, STRICTLY PRIVATE BUSINESS PROPOSAL M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>Will do.</td>\n",
       "      <td>do.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>Nora--Cheryl emailed dozen memo Haiti weekend....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>Dear Sir=2FMadam=2C know proposal might surpri...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
       "1                                           Will do.      0   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
       "4                                                fyi      0   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
       "1                                           Will do.   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                      processed_text  money_mark  \\\n",
       "0  DEAR SIR, STRICTLY PRIVATE BUSINESS PROPOSAL M...           1   \n",
       "1                                                do.           0   \n",
       "2  Nora--Cheryl emailed dozen memo Haiti weekend....           0   \n",
       "3  Dear Sir=2FMadam=2C know proposal might surpri...           1   \n",
       "4                                                fyi           0   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 0      1608  \n",
       "1                 0         3  \n",
       "2                 0       118  \n",
       "3                 1      1524  \n",
       "4                 0         3  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define indicators\n",
    "money_symbols = r\"euro|dollar|pound|€|\\$\"\n",
    "suspicious_words = r\"free|cheap|sex|money|account|bank|win|fund\"\n",
    "\n",
    "# Adding features for money symbols and suspicious words\n",
    "data['money_mark'] = data['processed_text'].str.contains(money_symbols).astype(int)\n",
    "data['suspicious_words'] = data['processed_text'].str.contains(suspicious_words).astype(int)\n",
    "data['text_len'] = data['processed_text'].apply(len)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Bag of Words dataset: (1000, 25260)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "bow_data = count_vectorizer.fit_transform(data['processed_text'])\n",
    "\n",
    "# Print the shape of the vectorized dataset\n",
    "print(\"Shape of the Bag of Words dataset:\", bow_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF dataset: (1000, 25260)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_data = tfidf_vectorizer.fit_transform(data['processed_text'])\n",
    "\n",
    "# Print the shape of the vectorized dataset\n",
    "print(\"Shape of the TF-IDF dataset:\", tfidf_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split into features and labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_data, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the classifier:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorizer Accuracy: 0.945\n",
      "Classification Report (TF-IDF):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95       125\n",
      "           1       0.87      1.00      0.93        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.96      0.94       200\n",
      "weighted avg       0.95      0.94      0.95       200\n",
      "\n",
      "Count Vectorizer Accuracy: 0.94\n",
      "Classification Report (Count Vectorizer):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95       125\n",
      "           1       0.87      0.99      0.93        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.93      0.95      0.94       200\n",
      "weighted avg       0.95      0.94      0.94       200\n",
      "\n",
      "Best feature representation: TF-IDF Vectorizer with accuracy 0.945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming 'data' contains the cleaned and preprocessed dataset with 'processed_text' and 'label' columns\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['processed_text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Trying both TF-IDF and Count Vectorizer to see which yields the best accuracy\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the MultinomialNB classifier\n",
    "tfidf_classifier = MultinomialNB()\n",
    "tfidf_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate with TF-IDF\n",
    "y_pred_tfidf = tfidf_classifier.predict(X_test_tfidf)\n",
    "tfidf_accuracy = accuracy_score(y_test, y_pred_tfidf)\n",
    "print(\"TF-IDF Vectorizer Accuracy:\", tfidf_accuracy)\n",
    "print(\"Classification Report (TF-IDF):\\n\", classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "# Count Vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Train the classifier with Count Vectorizer\n",
    "count_classifier = MultinomialNB()\n",
    "count_classifier.fit(X_train_count, y_train)\n",
    "\n",
    "# Predict and evaluate with Count Vectorizer\n",
    "y_pred_count = count_classifier.predict(X_test_count)\n",
    "count_accuracy = accuracy_score(y_test, y_pred_count)\n",
    "print(\"Count Vectorizer Accuracy:\", count_accuracy)\n",
    "print(\"Classification Report (Count Vectorizer):\\n\", classification_report(y_test, y_pred_count))\n",
    "\n",
    "# Determine the best feature representation\n",
    "if tfidf_accuracy > count_accuracy:\n",
    "    print(\"Best feature representation: TF-IDF Vectorizer with accuracy\", tfidf_accuracy)\n",
    "else:\n",
    "    print(\"Best feature representation: Count Vectorizer with accuracy\", count_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
