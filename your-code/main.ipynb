{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "#data = pd.read_csv(\"c:/Users/Mercy/OneDrive/Documents/ServiceNow classes/OneDrive/AI Training Docs/Labs/week 4/lab-natural-language-processing/data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# divide the training and test set into 2 partitions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...\n",
      "1                                               Will do.\n",
      "2      Nora--Cheryl has emailed dozens of memos about...\n",
      "3      Dear Sir=2FMadam=2C I know that this proposal ...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    So what's the latest? It sounds contradictory ...\n",
      "996    TRANSFER OF 36,759,000.00 MILLION POUNDS TO YO...\n",
      "997    Barb I will call to explain. Are you back in t...\n",
      "998      Yang on travelNot free tonite.May work tomorrow\n",
      "999    sbwhoeopSunday February 21 2010 7:42 PMHShaunH...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# remove inline javascript/css\n",
    "import re\n",
    "\n",
    "def remove_inline_js_css(text):\n",
    "    # Remove <script>...</script> and <style>...</style> blocks\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Apply to your DataFrame column (replace 'text' with your actual column name)\n",
    "data['clean_text'] = data['text'].apply(remove_inline_js_css)\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...\n",
      "1                                               Will do.\n",
      "2      Nora--Cheryl has emailed dozens of memos about...\n",
      "3      Dear Sir=2FMadam=2C I know that this proposal ...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    So what's the latest? It sounds contradictory ...\n",
      "996    TRANSFER OF 36,759,000.00 MILLION POUNDS TO YO...\n",
      "997    Barb I will call to explain. Are you back in t...\n",
      "998      Yang on travelNot free tonite.May work tomorrow\n",
      "999    sbwhoeopSunday February 21 2010 7:42 PMHShaunH...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove html comments\n",
    "import re\n",
    "\n",
    "def remove_html_comments(text):\n",
    "    return re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "\n",
    "# Apply to your DataFrame column (replace 'clean_text' with your actual column name)\n",
    "data['clean_text'] = data['clean_text'].apply(remove_html_comments)\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...\n",
      "1                                               Will do.\n",
      "2      Nora--Cheryl has emailed dozens of memos about...\n",
      "3      Dear Sir=2FMadam=2C I know that this proposal ...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    So what's the latest? It sounds contradictory ...\n",
      "996    TRANSFER OF 36,759,000.00 MILLION POUNDS TO YO...\n",
      "997    Barb I will call to explain. Are you back in t...\n",
      "998      Yang on travelNot free tonite.May work tomorrow\n",
      "999    sbwhoeopSunday February 21 2010 7:42 PMHShaunH...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(remove_html_tags)\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DEAR SIR STRICTLY A PRIVATE BUSINESS PROPOSAL ...\n",
      "1                                                Will do\n",
      "2      NoraCheryl has emailed dozens of memos about H...\n",
      "3      Dear Sir2FMadam2C I know that this proposal mi...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    So whats the latest It sounds contradictory an...\n",
      "996    TRANSFER OF 3675900000 MILLION POUNDS TO YOURA...\n",
      "997    Barb I will call to explain Are you back in th...\n",
      "998       Yang on travelNot free toniteMay work tomorrow\n",
      "999    sbwhoeopSunday February 21 2010 742 PMHShaunH ...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# remove all the special characters\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(remove_special_characters)\n",
    "print(data['clean_text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DEAR SIR STRICTLY A PRIVATE BUSINESS PROPOSAL ...\n",
      "1                                                Will do\n",
      "2      NoraCheryl has emailed dozens of memos about H...\n",
      "3      Dear SirFMadamC I know that this proposal migh...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    So whats the latest It sounds contradictory an...\n",
      "996    TRANSFER OF  MILLION POUNDS TO YOURACCOUNTMy n...\n",
      "997    Barb I will call to explain Are you back in th...\n",
      "998       Yang on travelNot free toniteMay work tomorrow\n",
      "999    sbwhoeopSunday February    PMHShaunH Just talk...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(remove_numbers)\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all single characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DEAR SIR STRICTLY  PRIVATE BUSINESS PROPOSAL  ...\n",
      "1                                                Will do\n",
      "2      NoraCheryl has emailed dozens of memos about H...\n",
      "3      Dear SirFMadamC  know that this proposal might...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    So whats the latest It sounds contradictory an...\n",
      "996    TRANSFER OF  MILLION POUNDS TO YOURACCOUNTMy n...\n",
      "997    Barb  will call to explain Are you back in the...\n",
      "998       Yang on travelNot free toniteMay work tomorrow\n",
      "999    sbwhoeopSunday February    PMHShaunH Just talk...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_single_characters(text):\n",
    "    # Remove single characters surrounded by spaces\n",
    "    return re.sub(r'\\b\\w\\b', '', text)\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(remove_single_characters)\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove single characters from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DEAR SIR STRICTLY  PRIVATE BUSINESS PROPOSAL  ...\n",
      "1                                                Will do\n",
      "2      NoraCheryl has emailed dozens of memos about H...\n",
      "3      Dear SirFMadamC  know that this proposal might...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    So whats the latest It sounds contradictory an...\n",
      "996    TRANSFER OF  MILLION POUNDS TO YOURACCOUNTMy n...\n",
      "997    Barb  will call to explain Are you back in the...\n",
      "998       Yang on travelNot free toniteMay work tomorrow\n",
      "999    sbwhoeopSunday February    PMHShaunH Just talk...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_single_char_start(text):\n",
    "    return re.sub(r'^\\w\\s+', '', text)\n",
    "\n",
    "# Apply to your DataFrame column\n",
    "data['clean_text'] = data['clean_text'].apply(remove_single_char_start)\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Substitute multiple spaces with single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DEAR SIR STRICTLY PRIVATE BUSINESS PROPOSAL AM...\n",
      "1                                                Will do\n",
      "2      NoraCheryl has emailed dozens of memos about H...\n",
      "3      Dear SirFMadamC know that this proposal might ...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    So whats the latest It sounds contradictory an...\n",
      "996    TRANSFER OF MILLION POUNDS TO YOURACCOUNTMy na...\n",
      "997    Barb will call to explain Are you back in the ...\n",
      "998       Yang on travelNot free toniteMay work tomorrow\n",
      "999    sbwhoeopSunday February PMHShaunH Just talked ...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def substitute_multiple_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Apply to your DataFrame column\n",
    "data['clean_text'] = data['clean_text'].apply(substitute_multiple_spaces)\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove prefixed 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      DEAR SIR STRICTLY PRIVATE BUSINESS PROPOSAL AM...\n",
      "1                                                Will do\n",
      "2      NoraCheryl has emailed dozens of memos about H...\n",
      "3      Dear SirFMadamC know that this proposal might ...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    So whats the latest It sounds contradictory an...\n",
      "996    TRANSFER OF MILLION POUNDS TO YOURACCOUNTMy na...\n",
      "997    Barb will call to explain Are you back in the ...\n",
      "998       Yang on travelNot free toniteMay work tomorrow\n",
      "999    sbwhoeopSunday February PMHShaunH Just talked ...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_prefixed_b(text):\n",
    "    return re.sub(r\"^b['\\\"]?\", '', text)\n",
    "\n",
    "# Apply to your DataFrame column\n",
    "data['clean_text'] = data['clean_text'].apply(remove_prefixed_b)\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert to Lowercase\n",
    "- Observation: Line 996 changed to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      dear sir strictly private business proposal am...\n",
      "1                                                will do\n",
      "2      noracheryl has emailed dozens of memos about h...\n",
      "3      dear sirfmadamc know that this proposal might ...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    so whats the latest it sounds contradictory an...\n",
      "996    transfer of million pounds to youraccountmy na...\n",
      "997    barb will call to explain are you back in the ...\n",
      "998       yang on travelnot free tonitemay work tomorrow\n",
      "999    sbwhoeopsunday february pmhshaunh just talked ...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['clean_text'] = data['clean_text'].str.lower()\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      dear sir strictly private business proposal mi...\n",
      "1                                                       \n",
      "2      noracheryl emailed dozens memos haiti weekend ...\n",
      "3      dear sirfmadamc know proposal might surprise e...\n",
      "4                                                    fyi\n",
      "                             ...                        \n",
      "995    whats latest sounds contradictory af decide sh...\n",
      "996    transfer million pounds youraccountmy name mre...\n",
      "997                       barb call explain back country\n",
      "998          yang travelnot free tonitemay work tomorrow\n",
      "999    sbwhoeopsunday february pmhshaunh talked shaun...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(remove_stopwords)\n",
    "print(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: 0      [dear, sir, strictly, private, business, propo...\n",
      "1                                                     []\n",
      "2      [noracheryl, emailed, dozen, memo, haiti, week...\n",
      "3      [dear, sirfmadamc, know, proposal, might, surp...\n",
      "4                                                  [fyi]\n",
      "                             ...                        \n",
      "995    [whats, latest, sound, contradictory, af, deci...\n",
      "996    [transfer, million, pound, youraccountmy, name...\n",
      "997                 [barb, call, explain, back, country]\n",
      "998    [yang, travelnot, free, tonitemay, work, tomor...\n",
      "999    [sbwhoeopsunday, february, pmhshaunh, talked, ...\n",
      "Name: lemmatized_tokens, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# Create a new column with lemmatized tokens (list of words)\n",
    "data['lemmatized_tokens'] = data['clean_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x.split()])\n",
    "print(\"Lemmatized Tokens:\", data['lemmatized_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham messages: [('u', 116), ('pm', 115), ('would', 106), ('state', 103), ('president', 94), ('call', 91), ('time', 84), ('percent', 77), ('secretary', 76), ('work', 73)]\n",
      "Top 10 words in spam messages: [('money', 920), ('account', 794), ('bank', 745), ('fund', 703), ('u', 550), ('business', 473), ('transaction', 416), ('country', 406), ('transfer', 392), ('million', 385)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# For ham messages (label == 0)\n",
    "ham_words = [word for tokens in data[data['label'] == 0]['lemmatized_tokens'] for word in tokens]\n",
    "ham_top10 = Counter(ham_words).most_common(10)\n",
    "print(\"Top 10 words in ham messages:\", ham_top10)\n",
    "\n",
    "# For spam messages (label == 1)\n",
    "spam_words = [word for tokens in data[data['label'] == 1]['lemmatized_tokens'] for word in tokens]\n",
    "spam_top10 = Counter(spam_words).most_common(10)\n",
    "print(\"Top 10 words in spam messages:\", spam_top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  money_mark  \\\n",
       "442  Dear=2C Good day hope fine=2Cdear am writting ...      1           1   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...      1           1   \n",
       "971                                           Will do.      0           1   \n",
       "190  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...      1           1   \n",
       "551  Dear Friend, My name is LOI C.ESTRADA,The wife...      1           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "442                 1      1609  \n",
       "962                 1      3123  \n",
       "971                 0         8  \n",
       "190                 1       530  \n",
       "551                 1      2126  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "# Make sure to use the correct column for preprocessed text. If you want to use 'clean_text', change below accordingly.\n",
    "train['money_mark'] = train['text'].str.contains(money_simbol_list, case=False, na=False)*1\n",
    "train['suspicious_words'] = train['text'].str.contains(suspicious_words, case=False, na=False)*1\n",
    "train['text_len'] = train['text'].apply(lambda x: len(x)) \n",
    "\n",
    "test['money_mark'] = test['text'].str.contains(money_simbol_list, case=False, na=False)*1\n",
    "test['suspicious_words'] = test['text'].str.contains(suspicious_words, case=False, na=False)*1\n",
    "test['text_len'] = test['text'].apply(lambda x: len(x)) \n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words matrix shape: (1000, 19354)\n",
      "Sample feature names: ['aac' 'aaclocated' 'aae' 'aag' 'aaronovitchon' 'abacha' 'abachabefore'\n",
      " 'abachac' 'abachace' 'abachaco']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the 'clean_text' column to create the bag-of-words representation\n",
    "X_bow = vectorizer.fit_transform(data['clean_text'])\n",
    "\n",
    "# Show the shape of the resulting matrix (documents x vocabulary size)\n",
    "print(\"Bag of Words matrix shape:\", X_bow.shape)\n",
    "\n",
    "# Show feature names (words)\n",
    "print(\"Sample feature names:\", vectorizer.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (1000, 19354)\n"
     ]
    }
   ],
   "source": [
    "# Load the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorize the entire 'clean_text' column\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['clean_text'])\n",
    "\n",
    "# Print the shape of the vectorized dataset\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       112\n",
      "           1       0.91      1.00      0.95        88\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.95      0.96      0.95       200\n",
      "weighted avg       0.96      0.95      0.96       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Prepare features and labels\n",
    "X = X_tfidf\n",
    "y = data['label']\n",
    "\n",
    "# Split into train and test sets (using the same indices as before)\n",
    "X_train = X[train.index]\n",
    "X_test = X[test.index]\n",
    "y_train = train['label']\n",
    "y_test = test['label']\n",
    "\n",
    "# Train the classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in spam messages: [('money', 920), ('account', 794), ('bank', 745), ('fund', 703), ('u', 550), ('business', 473), ('transaction', 416), ('country', 406), ('transfer', 392), ('million', 385)]\n",
      "Accuracy: 0.575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.24      0.39       112\n",
      "           1       0.51      1.00      0.67        88\n",
      "\n",
      "    accuracy                           0.57       200\n",
      "   macro avg       0.75      0.62      0.53       200\n",
      "weighted avg       0.78      0.57      0.51       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Example: Try TF-IDF with unigrams and bigrams + extra features (already shown in cell 45)\n",
    "# You can experiment with different feature combinations below:\n",
    "\n",
    "# For spam messages (label == 1)\n",
    "spam_words = [word for tokens in data[data['label'] == 1]['lemmatized_tokens'] for word in tokens]\n",
    "spam_top10 = Counter(spam_words).most_common(10)\n",
    "print(\"Top 10 words in spam messages:\", spam_top10)\n",
    "\n",
    "# Try character n-grams + word n-grams + extra features\n",
    "vectorizer_word = TfidfVectorizer(ngram_range=(1,2), analyzer='word', min_df=2)\n",
    "vectorizer_char = TfidfVectorizer(ngram_range=(3,5), analyzer='char', min_df=2)\n",
    "\n",
    "X_word = vectorizer_word.fit_transform(data['clean_text'])\n",
    "X_char = vectorizer_char.fit_transform(data['clean_text'])\n",
    "\n",
    "# Combine all features using train and test DataFrames for extra features\n",
    "extra_features_train = train[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "extra_features_test = test[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "\n",
    "X_train_all = hstack([X_word[train.index], X_char[train.index], csr_matrix(extra_features_train)])\n",
    "X_test_all = hstack([X_word[test.index], X_char[test.index], csr_matrix(extra_features_test)])\n",
    "y_train = train['label']\n",
    "y_test = test['label']\n",
    "\n",
    "# Train and evaluate\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_all, y_train)\n",
    "y_pred = clf.predict(X_test_all)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
