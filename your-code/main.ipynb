{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(r\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# english_stopwords = set(stopwords.words(\"english\")[100:110])\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# this function does all the cleaning, including removing stopwords as instructed \n",
    "def clean_text(text, stop_words=None):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<(script|style).*?>.*?(</\\1>)', '', text, flags=re.DOTALL)\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove all special characters\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords if provided\n",
    "    if stop_words:\n",
    "        text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "columns_to_clean = ['text']  \n",
    "for col in columns_to_clean:\n",
    "    data[col] = data[col].apply(lambda x: clean_text(x, stop_words=english_stopwords))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's work on removing stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# english_stopwords = set(stopwords.words(\"english\")[100:110])\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# this function does all the cleaning, including removing stopwords as instructed \n",
    "def clean_text(text, stop_words=None):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<(script|style).*?>.*?(</\\1>)', '', text, flags=re.DOTALL)\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove all special characters\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords if provided\n",
    "    if stop_words:\n",
    "        text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "columns_to_clean = ['text']  \n",
    "for col in columns_to_clean:\n",
    "    data[col] = data[col].apply(lambda x: clean_text(x, stop_words=english_stopwords))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_and_lemmatize(text, stop_words=None):\n",
    "    # Tokenize text into words\n",
    "    words = text.split()\n",
    "    # Remove stopwords and apply lemmatization\n",
    "    if stop_words:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    else:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "columns_to_clean = ['text']  \n",
    "for col in columns_to_clean:\n",
    "    data[col] = data[col].apply(lambda x: tokenize_and_lemmatize(x, stop_words=english_stopwords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target\n",
    "X = data[['text']] \n",
    "y = data['label']  \n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Combine all rows in the 'text' column into a single string\n",
    "all_text = ' '.join(data['text'])\n",
    "\n",
    "# Split the combined text into individual words\n",
    "words = all_text.split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Get the 10 most common words\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 Words:\", top_10_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "# Add indicators to the training set\n",
    "X_train['money_mark'] = X_train['text'].str.contains(money_simbol_list) * 1\n",
    "X_train['suspicious_words'] = X_train['text'].str.contains(suspicious_words) * 1\n",
    "X_train['text_len'] = X_train['text'].apply(lambda x: len(x))\n",
    "\n",
    "# Add indicators to the testing set\n",
    "X_test['money_mark'] = X_test['text'].str.contains(money_simbol_list) * 1\n",
    "X_test['suspicious_words'] = X_test['text'].str.contains(suspicious_words) * 1\n",
    "X_test['text_len'] = X_test['text'].apply(lambda x: len(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization:\n",
    "\n",
    "# Each text is transformed into a vector where element corresponds to a word in the vocabulary.\n",
    "# The value represents the count of how many times the word appears in the text.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer and transform the text\n",
    "X = vectorizer.fit_transform(words)\n",
    "\n",
    "# Convert to array for better visualization\n",
    "print(X.toarray())\n",
    "\n",
    "# Get the vocabulary (mapping of words to indices)\n",
    "print(vectorizer.vocabulary_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency) assigns weights to words based on their importance. \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer and transform the text\n",
    "X = vectorizer.fit_transform(words)\n",
    "\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# Fit and transform  training data with TF-IDF\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['text'])\n",
    "\n",
    "# Transform  testing data using same vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test['text'])\n",
    "\n",
    "# I will use Logistic regression as classifier\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Training  classifier on TF-IDF features\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on  test set\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# saving trained model and TF-IDF vectorizer \n",
    "joblib.dump(classifier, \"classifier_model.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying GridSearchCV to optimize logistic regressoin parameters\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']}\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "columns_to_clean = ['text']  \n",
    "for col in columns_to_clean:\n",
    "    data[col] = data[col].apply(lambda x: clean_text(x, stop_words=english_stopwords))\n",
    "\n",
    "# Train the model with  best parameters\n",
    "best_classifier = LogisticRegression(C=10, solver='liblinear', random_state=42)\n",
    "best_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_best = best_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Print results\n",
    "print(\"Optimized Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"Optimized Model Classification Report:\\n\", classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what words are more triggering?\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = classifier.coef_[0]\n",
    "sorted_features = sorted(zip(coefficients, feature_names), reverse=True)\n",
    "print(\"Top features:\", sorted_features[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation metric for this competition is Mean F1-Score. \n",
    "# MultinimialNB = Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(r\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "data.fillna(\"\",inplace=True)\n",
    "\n",
    "\n",
    "columns_to_clean = ['text']  \n",
    "for col in columns_to_clean:\n",
    "    data[col] = data[col].apply(lambda x: clean_text(x, stop_words=english_stopwords))\n",
    "\n",
    "columns_to_clean = ['text']  \n",
    "for col in columns_to_clean:\n",
    "    data[col] = data[col].apply(lambda x: tokenize_and_lemmatize(x, stop_words=english_stopwords))\n",
    "\n",
    "\n",
    "# Split into features and target\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CountVectorizer with bigrams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform training data, transform test data\n",
    "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
    "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
    "\n",
    "# Train and evaluate MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_ngram, y_train)\n",
    "y_pred_ngram = classifier.predict(X_test_ngram)\n",
    "\n",
    "# get feature nnames\n",
    "bigram_features = ngram_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Extract the log probabilities of the features\n",
    "feature_log_prob = classifier.feature_log_prob_\n",
    "\n",
    "# Convert log probabilities to regular probabilities\n",
    "feature_prob = np.exp(feature_log_prob)\n",
    "\n",
    "# Get the top 10 bigrams for each class\n",
    "top_n = 10\n",
    "\n",
    "# For class 0\n",
    "top_bigrams_class_0 = sorted(zip(feature_prob[0], bigram_features), reverse=True)[:top_n]\n",
    "\n",
    "# For class 1\n",
    "top_bigrams_class_1 = sorted(zip(feature_prob[1], bigram_features), reverse=True)[:top_n]\n",
    "\n",
    "print(\"Top 10 Bigrams for Class 0 (HAM):\")\n",
    "for prob, bigram in top_bigrams_class_0:\n",
    "    print(f\"{bigram}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 Bigrams for Class 1 (SPAM):\")\n",
    "for prob, bigram in top_bigrams_class_1:\n",
    "    print(f\"{bigram}: {prob:.4f}\")\n",
    "\n",
    "# Get the bigram indices in a specific sample\n",
    "sample_index = 0  # Example: First test sample\n",
    "bigram_indices = X_test_ngram[sample_index].nonzero()[1]\n",
    "sample_bigrams = [bigram_features[i] for i in bigram_indices]\n",
    "\n",
    "print(f\"Bigrams in Sample {sample_index}: {sample_bigrams}\")\n",
    "# Print results\n",
    "print(\"N-grams Representation (Bigrams)\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_ngram))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_ngram))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model seems to be the optimized model fro before. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
