{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 800\n",
      "Test size: 200\n"
     ]
    }
   ],
   "source": [
    "X = data[\"text\"] \n",
    "y = data[\"label\"]  \n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from html import unescape\n",
    "\n",
    "def clean_html(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")  # or \"html.parser\"\n",
    "\n",
    "    # 1) Remove inline JS/CSS:\n",
    "    #    a) <script> and <style> blocks\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    #    b) inline attributes like onclick, onload, style=...\n",
    "    for tag in soup.find_all(True):\n",
    "        # drop event handlers and inline styles\n",
    "        for attr in list(tag.attrs):\n",
    "            if attr.lower().startswith(\"on\") or attr.lower() == \"style\":\n",
    "                del tag.attrs[attr]\n",
    "\n",
    "    # 2) Remove HTML comments (do this before stripping tags)\n",
    "    for c in soup.find_all(string=lambda t: isinstance(t, Comment)):\n",
    "        c.extract()\n",
    "\n",
    "    # 3) Remove remaining tags -> keep only visible text\n",
    "    text = soup.get_text(separator=\" \")\n",
    "\n",
    "    # Unescape entities and normalize whitespace\n",
    "    text = unescape(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # 1) Remove all special characters (keep only letters and spaces)\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
    "\n",
    "    # 2) Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "    # 3) Remove all single characters (isolated letters)\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n",
    "\n",
    "    # 4) Remove single characters from the start of words\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # 5) Substitute multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # 6) Remove prefixed 'b' (like b'word' when decoding bytes)\n",
    "    text = re.sub(r\"^b\\s+\", \"\", text)\n",
    "\n",
    "    # 7) Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying this thingy guess\n"
     ]
    }
   ],
   "source": [
    "sample = \"123 trying! this.. thingy? I gUESS?!\"\n",
    "print(normalize_text(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pktto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # 1) Remove all special characters (keep only letters and spaces)\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
    "\n",
    "    # 2) Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "    # 3) Remove all single characters (isolated letters)\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n",
    "\n",
    "    # 4) Remove single characters from the start of words\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # 5) Substitute multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # 6) Remove prefixed 'b'\n",
    "    text = re.sub(r\"^b\\s+\", \"\", text)\n",
    "\n",
    "    # 7) Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # 8) Remove stopwords\n",
    "    tokens = [word for word in text.split() if word not in STOPWORDS]\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying remove stopwords see works example\n"
     ]
    }
   ],
   "source": [
    "sample = \"I'm trying to remove some stopwords to see if this works or not, so this is an example.\"\n",
    "print(normalize_text(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pktto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pktto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_text(text: str, remove_stopwords: bool = True, do_lemmatize: bool = True) -> str:\n",
    "    # 1) Remove all special characters\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
    "\n",
    "    # 2) Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "    # 3) Remove single characters\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n",
    "\n",
    "    # 4) Remove single characters from start\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\s+\", \" \", text)\n",
    "\n",
    "    # 5) Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # 6) Remove prefixed 'b'\n",
    "    text = re.sub(r\"^b\\s+\", \"\", text)\n",
    "\n",
    "    # 7) Lowercase\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "\n",
    "    # 8) Remove stopwords (optional)\n",
    "    if remove_stopwords:\n",
    "        tokens = [w for w in tokens if w not in STOPWORDS]\n",
    "\n",
    "    # 9) Lemmatize (optional)\n",
    "    if do_lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat running faster dog past day\n"
     ]
    }
   ],
   "source": [
    "sample = \"Cats are running faster than the dogs were these past days.\"\n",
    "print(normalize_text(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_clean(data, label, n=10):\n",
    "    texts = data[data[\"label\"] == label][\"text\"]\n",
    "    tokens = []\n",
    "    for msg in texts:\n",
    "        clean_msg = normalize_text(msg)  # your cleaning fn\n",
    "        tokens.extend(clean_msg.split())\n",
    "    counter = Counter(tokens)\n",
    "    return counter.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label label_name\n",
      "0      1       spam\n",
      "1      0        ham\n",
      "2      0        ham\n",
      "3      1       spam\n",
      "4      0        ham\n"
     ]
    }
   ],
   "source": [
    "# Map 0 -> ham, 1 -> spam\n",
    "label_map = {0: \"ham\", 1: \"spam\"}\n",
    "data[\"label_name\"] = data[\"label\"].map(label_map)\n",
    "\n",
    "# Check it worked\n",
    "print(data[[\"label\", \"label_name\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top HAM words: [('the', 1710), ('to', 1056), ('and', 808), ('of', 789), ('a', 610), ('in', 582), ('that', 385), ('is', 379), ('for', 357), ('on', 304)]\n",
      "Top SPAM words: [('the', 6785), ('to', 5494), ('of', 4858), ('and', 3867), ('in', 3156), ('i', 2859), ('you', 2722), ('this', 2523), ('a', 2245), ('my', 2026)]\n"
     ]
    }
   ],
   "source": [
    "# Using the new column\n",
    "ham_texts = data[data[\"label_name\"] == \"ham\"][\"text\"]\n",
    "spam_texts = data[data[\"label_name\"] == \"spam\"][\"text\"]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def top_words(texts, n=10):\n",
    "    all_words = \" \".join(texts).lower().split()\n",
    "    counter = Counter(all_words)\n",
    "    return counter.most_common(n)\n",
    "\n",
    "print(\"Top HAM words:\", top_words(ham_texts))\n",
    "print(\"Top SPAM words:\", top_words(spam_texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m money_simbol_list = \u001b[33m\"\u001b[39m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[33m\"\u001b[39m\u001b[33meuro\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdollar\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mpound\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m€\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m$\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m suspicious_words = \u001b[33m\"\u001b[39m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[33m\"\u001b[39m\u001b[33mfree\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcheap\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msex\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmoney\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33maccount\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mbank\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mfund\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtransfer\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mtransaction\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mwin\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdeposit\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mpassword\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m data_train[\u001b[33m'\u001b[39m\u001b[33mmoney_mark\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata_train\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mpreprocessed_text\u001b[39m\u001b[33m'\u001b[39m].str.contains(money_simbol_list)*\u001b[32m1\u001b[39m\n\u001b[32m      6\u001b[39m data_train[\u001b[33m'\u001b[39m\u001b[33msuspicious_words\u001b[39m\u001b[33m'\u001b[39m] = data_train[\u001b[33m'\u001b[39m\u001b[33mpreprocessed_text\u001b[39m\u001b[33m'\u001b[39m].str.contains(suspicious_words)*\u001b[32m1\u001b[39m\n\u001b[32m      7\u001b[39m data_train[\u001b[33m'\u001b[39m\u001b[33mtext_len\u001b[39m\u001b[33m'\u001b[39m] = data_train[\u001b[33m'\u001b[39m\u001b[33mpreprocessed_text\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x)) \n",
      "\u001b[31mNameError\u001b[39m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
