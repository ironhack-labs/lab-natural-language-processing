{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katha\\AppData\\Local\\Temp\\ipykernel_56652\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv('../data/kg_train.csv',encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataFrame:\n",
      "                                                  text  label\n",
      "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0\n",
      "428  H <hrod17@clintonemail.com >Saturday January 2...      0\n",
      "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1\n",
      "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1\n",
      "380  Hello,This is Dr.Clive Whittaker. I work for F...      1\n",
      "\n",
      "Validation DataFrame:\n",
      "                                                  text  label\n",
      "289  No=2E 5667 Allen Avenue=2CVictoria Garden City...      1\n",
      "984  He's in cairo. Saying after 2:30 is ideal. Sec...      0\n",
      "785  Robert Pape is a top strategic thinker from U ...      0\n",
      "123  Inside CGI.. More later.This is written from m...      0\n",
      "615  DEAR RESPECTFUL ONE, I AM MISS ALICIA MADOUH  ...      1\n"
     ]
    }
   ],
   "source": [
    "#x = data[\"text\"]\n",
    "#y = data[\"label\"]\n",
    "data_train, data_val = train_test_split(data, test_size=0.3, random_state=42, stratify=data['label'])\n",
    "\n",
    "# Print the results to verify the split\n",
    "print(\"Training DataFrame:\")\n",
    "print(data_train.head())\n",
    "\n",
    "print(\"\\nValidation DataFrame:\")\n",
    "print(data_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katha\\AppData\\Local\\Temp\\ipykernel_56652\\1852950438.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0   \n",
      "428  H <hrod17@clintonemail.com >Saturday January 2...      0   \n",
      "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1   \n",
      "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1   \n",
      "380  Hello,This is Dr.Clive Whittaker. I work for F...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "381  Mills Cheryl D Sunday January 31 2010 12:17 PM...  \n",
      "428  H Saturday January 23 2010 4:09 PM'sbwhoeopRe:...  \n",
      "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...  \n",
      "252  Dear Sir, I am Engr. Victor Chigoziem with the...  \n",
      "380  Hello,This is Dr.Clive Whittaker. I work for F...  \n",
      "                                                  text  label  \\\n",
      "289  No=2E 5667 Allen Avenue=2CVictoria Garden City...      1   \n",
      "984  He's in cairo. Saying after 2:30 is ideal. Sec...      0   \n",
      "785  Robert Pape is a top strategic thinker from U ...      0   \n",
      "123  Inside CGI.. More later.This is written from m...      0   \n",
      "615  DEAR RESPECTFUL ONE, I AM MISS ALICIA MADOUH  ...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "289  No=2E 5667 Allen Avenue=2CVictoria Garden City...  \n",
      "984  He's in cairo. Saying after 2:30 is ideal. Sec...  \n",
      "785  Robert Pape is a top strategic thinker from U ...  \n",
      "123  Inside CGI.. More later.This is written from m...  \n",
      "615  DEAR RESPECTFUL ONE, I AM MISS ALICIA MADOUH  ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katha\\AppData\\Local\\Temp\\ipykernel_56652\\1852950438.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n",
      "C:\\Users\\katha\\AppData\\Local\\Temp\\ipykernel_56652\\1852950438.py:4: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "def clean_html(html):\n",
    "\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for data in soup(['style', 'script', 'code', 'a']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "\n",
    "    # Optionally handle comments separately\n",
    "    for element in soup(text=lambda text: isinstance(text, Comment)):\n",
    "        element.extract()\n",
    "\n",
    "    # Return cleaned text\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "# Sample application to x_train and x_test\n",
    "data_train[\"preprocessed_text\"] = data_train[\"text\"].apply(clean_html)\n",
    "data_val[\"preprocessed_text\"] = data_val[\"text\"].apply(clean_html)\n",
    "\n",
    "print(data_train.head())\n",
    "print(data_val.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0   \n",
      "428  H <hrod17@clintonemail.com >Saturday January 2...      0   \n",
      "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1   \n",
      "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1   \n",
      "380  Hello,This is Dr.Clive Whittaker. I work for F...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "381  mill cheryl sunday january pmtravel scheduleca...  \n",
      "428  saturday january pmsbwhoeopre fyi foreign nati...  \n",
      "849  dear name mr mr ken edwarda former government ...  \n",
      "252  dear sir engr victor chigoziem engineering sto...  \n",
      "380  hellothis drclive whittaker work fidelity inve...  \n",
      "                                                  text  label  \\\n",
      "289  No=2E 5667 Allen Avenue=2CVictoria Garden City...      1   \n",
      "984  He's in cairo. Saying after 2:30 is ideal. Sec...      0   \n",
      "785  Robert Pape is a top strategic thinker from U ...      0   \n",
      "123  Inside CGI.. More later.This is written from m...      0   \n",
      "615  DEAR RESPECTFUL ONE, I AM MISS ALICIA MADOUH  ...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "289  noe allen avenuecvictoria garden citymarine be...  \n",
      "984  he cairo saying ideal secure better call grey ...  \n",
      "785  robert pape top strategic thinker chicago best...  \n",
      "123            inside cgi laterthis written blackberry  \n",
      "615  dear respectful one miss alicia madouh republi...  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove all single characters except 'I' when it stands alone\n",
    "    text = re.sub(r'\\b(?!\\bI\\b)\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from start\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove prefixed 'b' \n",
    "    text = text.replace(\"b'\", \"\").replace(\"b\\\"\", \"\")\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Clean text\n",
    "data_train[\"preprocessed_text\"] = data_train[\"preprocessed_text\"].apply(clean_text)\n",
    "data_val[\"preprocessed_text\"] = data_val[\"preprocessed_text\"].apply(clean_text)\n",
    "\n",
    "print(data_train.head())\n",
    "print(data_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0   \n",
      "428  H <hrod17@clintonemail.com >Saturday January 2...      0   \n",
      "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1   \n",
      "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1   \n",
      "380  Hello,This is Dr.Clive Whittaker. I work for F...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "381  mill cheryl sunday january pmtravel scheduleca...  \n",
      "428  saturday january pmsbwhoeopre fyi foreign nati...  \n",
      "849  dear name mr mr ken edwarda former government ...  \n",
      "252  dear sir engr victor chigoziem engineering sto...  \n",
      "380  hellothis drclive whittaker work fidelity inve...  \n",
      "                                                  text  label  \\\n",
      "289  No=2E 5667 Allen Avenue=2CVictoria Garden City...      1   \n",
      "984  He's in cairo. Saying after 2:30 is ideal. Sec...      0   \n",
      "785  Robert Pape is a top strategic thinker from U ...      0   \n",
      "123  Inside CGI.. More later.This is written from m...      0   \n",
      "615  DEAR RESPECTFUL ONE, I AM MISS ALICIA MADOUH  ...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "289  noe allen avenuecvictoria garden citymarine be...  \n",
      "984   cairo saying ideal secure better call grey phone  \n",
      "785  robert pape top strategic thinker chicago best...  \n",
      "123            inside cgi laterthis written blackberry  \n",
      "615  dear respectful one miss alicia madouh republi...  \n"
     ]
    }
   ],
   "source": [
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create a pattern that matches any of the stopwords\n",
    "stopwords_pattern = r'\\b(?:' + '|'.join(stop_words) + r')\\b'\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Use the regex pattern to remove stopwords\n",
    "    cleaned_text = re.sub(stopwords_pattern, '', text)\n",
    "    # Remove any extra spaces created due to stopword removal\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the stopword removal function to the cleaned datasets\n",
    "data_train[\"preprocessed_text\"] = data_train[\"preprocessed_text\"].apply(remove_stopwords)\n",
    "data_val[\"preprocessed_text\"] = data_val[\"preprocessed_text\"].apply(remove_stopwords)\n",
    "\n",
    "print(data_train.head())\n",
    "print(data_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0   \n",
      "428  H <hrod17@clintonemail.com >Saturday January 2...      0   \n",
      "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1   \n",
      "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1   \n",
      "380  Hello,This is Dr.Clive Whittaker. I work for F...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "381  mill cheryl sunday january pmtravel scheduleca...  \n",
      "428  saturday january pmsbwhoeopre fyi foreign nati...  \n",
      "849  dear name mr mr ken edwarda former government ...  \n",
      "252  dear sir engr victor chigoziem engineering sto...  \n",
      "380  hellothis drclive whittaker work fidelity inve...  \n",
      "                                                  text  label  \\\n",
      "289  No=2E 5667 Allen Avenue=2CVictoria Garden City...      1   \n",
      "984  He's in cairo. Saying after 2:30 is ideal. Sec...      0   \n",
      "785  Robert Pape is a top strategic thinker from U ...      0   \n",
      "123  Inside CGI.. More later.This is written from m...      0   \n",
      "615  DEAR RESPECTFUL ONE, I AM MISS ALICIA MADOUH  ...      1   \n",
      "\n",
      "                                     preprocessed_text  \n",
      "289  noe allen avenuecvictoria garden citymarine be...  \n",
      "984   cairo saying ideal secure better call grey phone  \n",
      "785  robert pape top strategic thinker chicago best...  \n",
      "123            inside cgi laterthis written blackberry  \n",
      "615  dear respectful one miss alicia madouh republi...  \n"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization to the text\n",
    "data_train[\"preprocessed_text\"] = data_train[\"preprocessed_text\"].apply(lemmatize_text)\n",
    "data_val[\"preprocessed_text\"] = data_val[\"preprocessed_text\"].apply(lemmatize_text)\n",
    "\n",
    "print(data_train.head())\n",
    "print(data_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham messages:\n",
      "president: 83\n",
      "would: 80\n",
      "percent: 76\n",
      "call: 66\n",
      "state: 65\n",
      "american: 60\n",
      "work: 58\n",
      "obama: 57\n",
      "pm: 57\n",
      "mr: 56\n",
      "\n",
      "Top 10 words in spam messages:\n",
      "money: 594\n",
      "account: 536\n",
      "bank: 515\n",
      "fund: 484\n",
      "business: 331\n",
      "transaction: 291\n",
      "million: 277\n",
      "country: 274\n",
      "company: 267\n",
      "transfer: 266\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Separate ham (y=0) and spam (y=1) messages based on labels\n",
    "ham_messages = data_train[data_train[\"label\"] == 0][\"preprocessed_text\"]\n",
    "spam_messages = data_train[data_train[\"label\"] == 1][\"preprocessed_text\"]\n",
    "\n",
    "# Count the words in the ham and spam lists\n",
    "ham_word_counts = Counter(\" \".join(ham_messages).split())\n",
    "spam_word_counts = Counter(\" \".join(spam_messages).split())\n",
    "\n",
    "# Display top 10 words \n",
    "top_10_ham_words = ham_word_counts.most_common(10)\n",
    "top_10_spam_words = spam_word_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 words in ham messages:\")\n",
    "for word, freq in top_10_ham_words:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 words in spam messages:\")\n",
    "for word, freq in top_10_spam_words:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Mills Cheryl D &lt;MillsCD@state.gov&gt;Sunday Janua...</td>\n",
       "      <td>0</td>\n",
       "      <td>mill cheryl sunday january pmtravel scheduleca...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>H &lt;hrod17@clintonemail.com &gt;Saturday January 2...</td>\n",
       "      <td>0</td>\n",
       "      <td>saturday january pmsbwhoeopre fyi foreign nati...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>DEAR,     MY NAME IS MR MR Ken Edward,A former...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear name mr mr ken edwarda former government ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Dear Sir, I am Engr. Victor Chigoziem with the...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sir engr victor chigoziem engineering sto...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Hello,This is Dr.Clive Whittaker. I work for F...</td>\n",
       "      <td>1</td>\n",
       "      <td>hellothis drclive whittaker work fidelity inve...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "381  Mills Cheryl D <MillsCD@state.gov>Sunday Janua...      0   \n",
       "428  H <hrod17@clintonemail.com >Saturday January 2...      0   \n",
       "849  DEAR,     MY NAME IS MR MR Ken Edward,A former...      1   \n",
       "252  Dear Sir, I am Engr. Victor Chigoziem with the...      1   \n",
       "380  Hello,This is Dr.Clive Whittaker. I work for F...      1   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "381  mill cheryl sunday january pmtravel scheduleca...           1   \n",
       "428  saturday january pmsbwhoeopre fyi foreign nati...           1   \n",
       "849  dear name mr mr ken edwarda former government ...           1   \n",
       "252  dear sir engr victor chigoziem engineering sto...           1   \n",
       "380  hellothis drclive whittaker work fidelity inve...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "381                 0       157  \n",
       "428                 0       207  \n",
       "849                 1      1310  \n",
       "252                 1      3491  \n",
       "380                 1      1327  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham messages:\n",
      "president: 83\n",
      "would: 80\n",
      "percent: 76\n",
      "call: 66\n",
      "state: 65\n",
      "american: 60\n",
      "work: 58\n",
      "obama: 57\n",
      "pm: 57\n",
      "mr: 56\n",
      "\n",
      "Top 10 words in spam messages:\n",
      "money: 594\n",
      "account: 536\n",
      "bank: 515\n",
      "fund: 484\n",
      "business: 331\n",
      "transaction: 291\n",
      "million: 277\n",
      "country: 274\n",
      "company: 267\n",
      "transfer: 266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Separate ham (y=0) and spam (y=1) messages based on labels\n",
    "ham_messages = data_train[data_train[\"label\"] == 0][\"preprocessed_text\"]\n",
    "spam_messages = data_train[data_train[\"label\"] == 1][\"preprocessed_text\"]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform ham messages\n",
    "X_ham = vectorizer.fit_transform(ham_messages)\n",
    "ham_word_counts = X_ham.toarray().sum(axis=0)\n",
    "ham_word_freq = dict(zip(vectorizer.get_feature_names_out(), ham_word_counts))\n",
    "\n",
    "# Fit and transform spam messages\n",
    "X_spam = vectorizer.fit_transform(spam_messages)\n",
    "spam_word_counts = X_spam.toarray().sum(axis=0)\n",
    "spam_word_freq = dict(zip(vectorizer.get_feature_names_out(), spam_word_counts))\n",
    "\n",
    "# Get top 10 words\n",
    "top_ham_words = sorted(ham_word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "top_spam_words = sorted(spam_word_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 words in ham messages:\")\n",
    "for word, freq in top_ham_words:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 words in spam messages:\")\n",
    "for word, freq in top_spam_words:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit CountVectorizer on the entire dataset so it includes all words\n",
    "X_train_counts = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_counts = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Display feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Initialize TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Fit and transform the count matrix to get the TF-IDF matrix for training data\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Transform the count matrix to get the TF-IDF matrix for validation data\n",
    "X_val_tfidf = tfidf_transformer.transform(X_val_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "nb_classifier.fit(X_train_tfidf, data_train['label'])\n",
    "\n",
    "# Predict on the validation data\n",
    "y_pred = nb_classifier.predict(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.00%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.92       167\n",
      "           1       0.87      0.96      0.91       133\n",
      "\n",
      "    accuracy                           0.92       300\n",
      "   macro avg       0.92      0.92      0.92       300\n",
      "weighted avg       0.92      0.92      0.92       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy\n",
    "accuracy = accuracy_score(data_val['label'], y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(data_val['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer and TfidfTransformer\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Fit and transform preprocessed text to get the TF-IDF matrix for training data\n",
    "X_train_counts = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Transform preprocessed text to get the TF-IDF matrix for validation data\n",
    "X_val_counts = vectorizer.transform(data_val['preprocessed_text'])\n",
    "X_val_tfidf = tfidf_transformer.transform(X_val_counts)\n",
    "\n",
    "# Convert TF-IDF sparse matrices to dense arrays\n",
    "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
    "X_val_tfidf_dense = X_val_tfidf.toarray()\n",
    "\n",
    "# Combine TF-IDF with additional features\n",
    "additional_train_features = data_train[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "additional_val_features = data_val[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "\n",
    "X_train_combined = np.hstack((X_train_tfidf_dense, additional_train_features))\n",
    "X_val_combined = np.hstack((X_val_tfidf_dense, additional_val_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.67%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.84       167\n",
      "           1       0.75      0.97      0.85       133\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.86      0.86      0.85       300\n",
      "weighted avg       0.87      0.85      0.85       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "nb_classifier.fit(X_train_combined, data_train['label'])\n",
    "\n",
    "# Predict on the validation data\n",
    "y_pred = nb_classifier.predict(X_val_combined)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(data_val['label'], y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(data_val['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0 feature log probabilities:\n",
      "text_len: -0.09084081231364394\n",
      "money_mark: -6.206659727025651\n",
      "suspicious_words: -8.743934362330968\n",
      "fyi: -8.776707033549528\n",
      "ok: -9.720671901090999\n",
      "call: -9.731880829148091\n",
      "yes: -9.765638864979\n",
      "talk: -10.244434735572518\n",
      "pls: -10.283244496374596\n",
      "today: -10.391143771465426\n",
      "\n",
      "Class 1 feature log probabilities:\n",
      "text_len: -0.022636075374594\n",
      "money_mark: -7.905651905080719\n",
      "suspicious_words: -8.058727893778212\n",
      "money: -10.767953992086236\n",
      "account: -10.790136653681838\n",
      "bank: -10.79232362567065\n",
      "fund: -10.89003179122844\n",
      "business: -11.22252308271903\n",
      "transaction: -11.265598039317016\n",
      "transfer: -11.297527225871708\n"
     ]
    }
   ],
   "source": [
    "# Examine feature log probabilities\n",
    "feature_names = np.array(vectorizer.get_feature_names_out().tolist() + ['money_mark', 'suspicious_words', 'text_len'])\n",
    "log_probs = nb_classifier.feature_log_prob_\n",
    "\n",
    "# Summarize feature importance\n",
    "for i in range(log_probs.shape[0]):\n",
    "    print(f\"\\nClass {i} feature log probabilities:\")\n",
    "    feature_importance = sorted(zip(log_probs[i], feature_names), reverse=True)\n",
    "    for log_prob, feature in feature_importance[:10]:  # Top 10 features\n",
    "        print(f\"{feature}: {log_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.00%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94       167\n",
      "           1       0.90      0.97      0.93       133\n",
      "\n",
      "    accuracy                           0.94       300\n",
      "   macro avg       0.94      0.94      0.94       300\n",
      "weighted avg       0.94      0.94      0.94       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Building new classifier without text_length\n",
    "\n",
    "# Initialize CountVectorizer and TfidfTransformer\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Combine TF-IDF with additional features\n",
    "additional_train_features = data_train[['money_mark', 'suspicious_words']].values\n",
    "additional_val_features = data_val[['money_mark', 'suspicious_words']].values\n",
    "\n",
    "X_train_combined = np.hstack((X_train_tfidf_dense, additional_train_features))\n",
    "X_val_combined = np.hstack((X_val_tfidf_dense, additional_val_features))\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "nb_classifier.fit(X_train_combined, data_train['label'])\n",
    "\n",
    "# Predict on the validation data\n",
    "y_pred = nb_classifier.predict(X_val_combined)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(data_val['label'], y_pred)\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(data_val['label'], y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
