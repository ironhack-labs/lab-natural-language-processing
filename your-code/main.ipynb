{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m2/12hw6p5106x98ycbwzpq3hsh0000gn/T/ipykernel_17636/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "# Adjust the file path to the correct location\n",
    "data = pd.read_csv(\"../data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (800, 2)\n",
      "Validation set shape: (200, 2)\n",
      "Spam distribution in training: 0.44\n",
      "Spam distribution in validation: 0.44\n"
     ]
    }
   ],
   "source": [
    "# divide the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Rename variables to be more descriptive \n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42, stratify=data['label'])\n",
    "\n",
    "print(f\"Training set shape: {data_train.shape}\")\n",
    "print(f\"Validation set shape: {data_val.shape}\")\n",
    "print(f\"Spam distribution in training: {data_train['label'].mean():.2f}\")\n",
    "print(f\"Spam distribution in validation: {data_val['label'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove html comments\n",
    "def remove_html_comments(text):\n",
    "    return ' '.join(word for word in text.split() if not (word.startswith('<!--') and word.endswith('-->')))\n",
    "\n",
    "# remove inline javascript/css\n",
    "def remove_inline_js_css(text):\n",
    "    return ' '.join(word for word in text.split() if not (word.startswith('<') and word.endswith('>')))\n",
    "\n",
    "# clean html code removing words that are not useful\n",
    "def clean_html(text):\n",
    "    text = remove_html_comments(text)\n",
    "    text = remove_inline_js_css(text)\n",
    "    return text\n",
    "\n",
    "# clean data_train and data_val\n",
    "data_train['text'] = data_train['text'].apply(clean_html)\n",
    "data_val['text'] = data_val['text'].apply(clean_html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all special characters and digits\n",
    "def remove_special_characters(text):\n",
    "    return ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "\n",
    "#remove numbers\n",
    "def remove_numbers(text):\n",
    "    return ''.join(char for char in text if not char.isdigit())\n",
    "\n",
    "#remove all single characters\n",
    "def remove_single_characters(text):\n",
    "    return ' '.join(word for word in text.split() if len(word) > 1)\n",
    "\n",
    "#remove single characters from the start of the word\n",
    "def remove_single_characters_start(text):\n",
    "    return ' '.join(word[1:] if len(word) > 1 and word[0].isalpha() else word for word in text.split())\n",
    "\n",
    "# remove multiple spaces\n",
    "def remove_multiple_spaces(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# remove prefixed 'b'\n",
    "def remove_prefixed_b(text):\n",
    "    return text.replace(\"b'\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "# convert to lowercase\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# apply all cleaning functions\n",
    "def clean_text(text):\n",
    "    text = remove_html_comments(text)\n",
    "    text = remove_inline_js_css(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_single_characters(text)\n",
    "    text = remove_single_characters_start(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    text = remove_prefixed_b(text)\n",
    "    text = convert_to_lowercase(text)\n",
    "    return text\n",
    "\n",
    "# clean the text in data_train and data_val\n",
    "data_train['text'] = data_train['text'].apply(clean_text)\n",
    "data_val['text'] = data_val['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "# apply stopword removal\n",
    "data_train['text'] = data_train['text'].apply(remove_stopwords)\n",
    "data_val['text'] = data_val['text'].apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break sentences into words\n",
    "def break_into_words(text):\n",
    "    return text.split()\n",
    "\n",
    "# use lemmatization\n",
    "def lemmatize_text(text):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "\n",
    "# apply lemmatization\n",
    "data_train['text'] = data_train['text'].apply(lemmatize_text)\n",
    "data_val['text'] = data_val['text'].apply(lemmatize_text)\n",
    "\n",
    "# see how this creates cleaner data\n",
    "print(\"Sample cleaned text from training data:\")\n",
    "print(data_train['text'].iloc[0])\n",
    "# visualize the distribution of labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "data_train['label'].value_counts().plot(kind='bar', color=['blue', 'orange'])\n",
    "plt.title('Distribution of Labels in Training Data')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=[0, 1], labels=['Ham', 'Spam'], rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 top words in ham and spam messages\n",
    "def get_top_words(data, label, n=10):\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(data[data['label'] == label]['text'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    sums = X.sum(axis=0)\n",
    "    data = pd.DataFrame(sums, columns=feature_names).T\n",
    "    data.columns = ['tfidf']\n",
    "    return data.nlargest(n, 'tfidf')\n",
    "\n",
    "data_train_top_ham = get_top_words(data_train, 0, 10)\n",
    "data_train_top_spam = get_top_words(data_train, 1, 10)\n",
    "print(\"Top words in Ham messages:\")\n",
    "print(data_train_top_ham)\n",
    "print(\"Top words in Spam messages:\")\n",
    "print(data_train_top_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bags of words with count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X_train_counts = count_vectorizer.fit_transform(data_train['text'])\n",
    "X_val_counts = count_vectorizer.transform(data_val['text'])\n",
    "\n",
    "# see the resulting shape\n",
    "print(f\"Shape of training data after CountVectorizer: {X_train_counts.shape}\")\n",
    "print(f\"Shape of validation data after CountVectorizer: {X_val_counts.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "# Create and fit the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val['text'])\n",
    "\n",
    "\n",
    "# Save the CountVectorizer for later use\n",
    "import joblib\n",
    "joblib.dump(count_vectorizer, '../data/count_vectorizer.pkl')\n",
    "\n",
    "\n",
    "# Load the TF-IDF vectorizer\n",
    "loaded_tfidf_vectorizer = joblib.load('../data/tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Initialize the classifier\n",
    "classifier = MultinomialNB()\n",
    "# Fit the classifier on the training data\n",
    "classifier.fit(X_train_tfidf, data_train['label'])\n",
    "# Predict on the validation set\n",
    "predictions = classifier.predict(X_val_tfidf)\n",
    "# Evaluate the classifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(data_val['label'], predictions)\n",
    "print(f\"Accuracy of the Naive Bayes classifier: {accuracy:.2f}\")\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(data_val['label'], predictions, target_names=['Ham', 'Spam']))\n",
    "# Print confusion matrix\n",
    "conf_matrix = confusion_matrix(data_val['label'], predictions)\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "# Save the trained classifier\n",
    "joblib.dump(classifier, '../data/naive_bayes_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
