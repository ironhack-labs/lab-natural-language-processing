{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BxGnYy2c4yRj",
        "outputId": "b21d6b14-2509-4c1d-cd61-6b5771920fe5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI9WkEXr4yRk"
      },
      "source": [
        "# Lab | Natural Language Processing\n",
        "### SMS: SPAM or HAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5asjEIE4yRk"
      },
      "source": [
        "### Let's prepare the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn pandas matplotlib\n"
      ],
      "metadata": {
        "id": "gTE6wXM-GhCM",
        "outputId": "e155b927-5262-4677-c973-312ce0f5c01c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rgyGM4fK4yRl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Nm1YXh4yRl"
      },
      "source": [
        "- Read Data for the Fraudulent Email Kaggle Challenge\n",
        "- Reduce the training set to speead up development."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "url_test = \"https://raw.githubusercontent.com/KJanzon/lab-natural-language-processing/refs/heads/main/data/kg_test.csv\"\n",
        "url_train = \"https://raw.githubusercontent.com/KJanzon/lab-natural-language-processing/refs/heads/main/data/kg_train.csv\"\n"
      ],
      "metadata": {
        "id": "kdSt3V_778fz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3iXz34r4yRl",
        "outputId": "a86835bd-6790-4510-eb34-4f179df7e032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2)\n",
            "<bound method NDFrame.head of                                                   text  label\n",
            "0    DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
            "1                                             Will do.      0\n",
            "2    Nora--Cheryl has emailed dozens of memos about...      0\n",
            "3    Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
            "4                                                  fyi      0\n",
            "..                                                 ...    ...\n",
            "995  So what's the latest? It sounds contradictory ...      0\n",
            "996  TRANSFER OF 36,759,000.00 MILLION POUNDS TO YO...      1\n",
            "997  Barb I will call to explain. Are you back in t...      0\n",
            "998    Yang on travelNot free tonite.May work tomorrow      0\n",
            "999  sbwhoeopSunday February 21 2010 7:42 PMHShaunH...      0\n",
            "\n",
            "[1000 rows x 2 columns]>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-15ba5bf19852>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data.fillna(\"\",inplace=True)\n"
          ]
        }
      ],
      "source": [
        "## Read Data for the Fraudulent Email Kaggle Challenge\n",
        "train_data = pd.read_csv(url_train)\n",
        "test_data = pd.read_csv(url_test)\n",
        "\n",
        "# Reduce the training set to speed up development.\n",
        "# Modify for final system\n",
        "train_data = train_data.head(1000)\n",
        "print(train_data.shape)\n",
        "train_data.fillna(\"\",inplace=True)\n",
        "print(train_data.head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69JXvloT4yRl"
      },
      "source": [
        "### Let's divide the training and test set into two partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "td1oO-Tf4yRl"
      },
      "outputs": [],
      "source": [
        "# Ensuring the dataset has the expected structure\n",
        "if \"text\" in train_data.columns and \"label\" in train_data.columns:\n",
        "    X = train_data[\"text\"]  # Features (SMS messages)\n",
        "    y = train_data[\"label\"]  # Labels (Spam or Ham)\n",
        "else:\n",
        "    X = train_data.iloc[:, 0]  # Assuming first column is text\n",
        "    y = train_data.iloc[:, 1]  # Assuming second column is label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmtVg89e4yRl"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "tEeETKVNGVsl",
        "outputId": "28cbc9a1-76cb-4c08-8494-bb897be5b541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEN0Rsmw4yRl",
        "outputId": "147ff337-53e6-4091-ac7d-729c89c2760c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "print(string.punctuation)\n",
        "print(stopwords.words(\"english\")[100:110])\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snowball = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvOsnHmS4yRm"
      },
      "source": [
        "## Now, we have to clean the html code removing words\n",
        "\n",
        "- First we remove inline JavaScript/CSS\n",
        "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
        "- Next we can remove the remaining tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LNzmgIrj4yRm"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_html(text):\n",
        "    # Remove inline JavaScript & CSS\n",
        "    text = re.sub(r'<script.*?</script>', '', text, flags=re.DOTALL)  # Remove JavaScript\n",
        "    text = re.sub(r'<style.*?</style>', '', text, flags=re.DOTALL)  # Remove CSS\n",
        "\n",
        "    # Remove HTML comments\n",
        "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
        "\n",
        "    # Remove remaining HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_vLhNIi4yRm"
      },
      "source": [
        "- Remove all the special characters\n",
        "    \n",
        "- Remove numbers\n",
        "    \n",
        "- Remove all single characters\n",
        "\n",
        "- Remove single characters from the start\n",
        "\n",
        "- Substitute multiple spaces with single space\n",
        "\n",
        "- Remove prefixed 'b'\n",
        "\n",
        "- Convert to Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMq1ey5Q4yRm"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNmslj7c4yRm"
      },
      "source": [
        "## Now let's work on removing stopwords\n",
        "Remove the stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ISk3RGb4yRm"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljOq9jLD4yRm"
      },
      "source": [
        "## Tame Your Text with Lemmatization\n",
        "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnlKkdYF4yRm"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbpjyH1c4yRm"
      },
      "source": [
        "## Bag Of Words\n",
        "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBSvnSid4yRm"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joDnk4Kz4yRm"
      },
      "source": [
        "## Extra features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxFg4w1N4yRm"
      },
      "outputs": [],
      "source": [
        "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
        "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",\"$\"])\n",
        "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
        "\n",
        "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
        "\n",
        "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x))\n",
        "\n",
        "data_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92hAG0HK4yRm"
      },
      "source": [
        "## How would work the Bag of Words with Count Vectorizer concept?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5XbEXe54yRm"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGgpf7984yRm"
      },
      "source": [
        "## TD-IDF\n",
        "\n",
        "- Load the vectorizer\n",
        "\n",
        "- Vectorize all dataset\n",
        "\n",
        "- print the shape of the vetorized dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3hIebeh4yRm"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw7uUGio4yRm"
      },
      "source": [
        "## And the Train a Classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-ggefvR4yRm"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7dLFM6-4yRn"
      },
      "source": [
        "### Extra Task - Implement a SPAM/HAM classifier\n",
        "\n",
        "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
        "\n",
        "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
        "\n",
        "Your task is to find the **best feature representation**.\n",
        "\n",
        "You can work with teams of two persons (recommended)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WJGESO94yRn"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}