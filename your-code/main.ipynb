{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/gs_200v517s5clp2y1jt0mgc0000gn/T/ipykernel_14511/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 800\n",
      "Test size: 200\n"
     ]
    }
   ],
   "source": [
    "X = data['text']            # Features (SMS content)\n",
    "y = data['label']           # Labels (ham/spam)\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# checking split\n",
    "print(\"Training size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript and CSS\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)          # remove special characters\n",
    "    text = re.sub(r'\\d+', '', text)               # remove numbers\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)            # remove single characters\n",
    "    text = re.sub(r'^\\s*\\w\\s+', '', text)         # remove single characters from start\n",
    "    text = re.sub(r'\\s+', ' ', text)              # multiple spaces to single\n",
    "    text = re.sub(r'^b\\s+', '', text)             # remove 'b' if prefixed\n",
    "    text = text.lower().strip()                   # convert to lowercase\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Tokenize by whitespace\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/affy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/affy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_encoded_chars(text):\n",
    "    return re.sub(r'=2C', ',', text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Clean HTML\n",
    "X_train_clean = X_train.apply(clean_html)\n",
    "X_test_clean = X_test.apply(clean_html)\n",
    "\n",
    "# Clean text\n",
    "X_train_clean = X_train_clean.apply(clean_text)\n",
    "X_test_clean = X_test_clean.apply(clean_text)\n",
    "\n",
    "# Remove stopwords\n",
    "X_train_cleaned = X_train_clean.apply(remove_stopwords)\n",
    "X_test_cleaned = X_test_clean.apply(remove_stopwords)\n",
    "\n",
    "# Decode weird HTML artifacts like '=2C'\n",
    "X_train_cleaned = X_train_cleaned.apply(clean_encoded_chars)\n",
    "X_test_cleaned = X_test_cleaned.apply(clean_encoded_chars)\n",
    "\n",
    "# Lemmatize\n",
    "X_train_lemmatized = X_train_cleaned.apply(lemmatize_text)\n",
    "X_test_lemmatized = X_test_cleaned.apply(lemmatize_text)\n",
    "\n",
    "# Convert lemmatized version to lowercase\n",
    "X_train_lemmatized = X_train_lemmatized.str.lower()\n",
    "X_test_lemmatized = X_test_lemmatized.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words in HAM Messages:\n",
      "president: 94\n",
      "would: 92\n",
      "mr: 85\n",
      "obama: 80\n",
      "percent: 80\n",
      "state: 77\n",
      "pm: 68\n",
      "work: 67\n",
      "one: 67\n",
      "call: 61\n",
      "\n",
      "Top 10 Words in SPAM Messages:\n",
      "money: 756\n",
      "account: 630\n",
      "bank: 598\n",
      "us: 590\n",
      "transaction: 422\n",
      "business: 412\n",
      "nbsp: 387\n",
      "fund: 385\n",
      "country: 370\n",
      "million: 338\n"
     ]
    }
   ],
   "source": [
    "# Separate cleaned messages based on label\n",
    "ham_cleaned = X_train_cleaned[y_train == 0]   # ham is labeled as 0\n",
    "spam_cleaned = X_train_cleaned[y_train == 1]  # spam is labeled as 1\n",
    "\n",
    "# Tokenize into words\n",
    "ham_words = ' '.join(ham_cleaned).lower().split()\n",
    "spam_words = ' '.join(spam_cleaned).lower().split()\n",
    "\n",
    "# Count frequencies\n",
    "ham_freq = {}\n",
    "for word in ham_words:\n",
    "    ham_freq[word] = ham_freq.get(word, 0) + 1\n",
    "\n",
    "spam_freq = {}\n",
    "for word in spam_words:\n",
    "    spam_freq[word] = spam_freq.get(word, 0) + 1\n",
    "\n",
    "# Sort by frequency and get top 10 words\n",
    "top_ham = sorted(ham_freq.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "top_spam = sorted(spam_freq.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "\n",
    "# Display results\n",
    "print(\"Top 10 Words in HAM Messages:\")\n",
    "for word, count in top_ham:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Words in SPAM Messages:\")\n",
    "for word, count in top_spam:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap up the preprocessed data into a DataFrame\n",
    "data_train = pd.DataFrame({\n",
    "    'preprocessed_text': X_train_lemmatized,\n",
    "    'label': y_train\n",
    "})\n",
    "\n",
    "data_val = pd.DataFrame({\n",
    "    'preprocessed_text': X_test_lemmatized,\n",
    "    'label': y_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>label</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>dear good day hope fine cdear writting mail du...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>mr henry kaborethe chief auditor inchargeforei...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>desk dr adamu ismalerauditing account manager ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>dear friend name loi estrada wife mr josephest...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     preprocessed_text  label  money_mark  \\\n",
       "442  dear good day hope fine cdear writting mail du...      1           1   \n",
       "962  mr henry kaborethe chief auditor inchargeforei...      1           1   \n",
       "971                                                         0           1   \n",
       "190  desk dr adamu ismalerauditing account manager ...      1           1   \n",
       "551  dear friend name loi estrada wife mr josephest...      1           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "442                 1       980  \n",
       "962                 1      1981  \n",
       "971                 0         0  \n",
       "190                 1       372  \n",
       "551                 1      1458  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words - Train shape: (800, 19686)\n",
      "Bag of Words - Validation shape: (200, 19686)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the vectorizer\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit training data then transform both train and validation sets\n",
    "X_train_bow = bow_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_bow = bow_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Print the shape\n",
    "print(\"Bag of Words - Train shape:\", X_train_bow.shape)\n",
    "print(\"Bag of Words - Validation shape:\", X_val_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF - Train shape: (800, 19686)\n",
      "TF-IDF - Validation shape: (200, 19686)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Print shape\n",
    "print(\"TF-IDF - Train shape:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF - Validation shape:\", X_val_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95       112\n",
      "           1       0.89      1.00      0.94        88\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.95      0.94       200\n",
      "weighted avg       0.95      0.94      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize and train\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = nb_classifier.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/affy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/affy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# init tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\n|\\r', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def clean_encoded_chars(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def full_clean_pipeline(text):\n",
    "    text = clean_html(text)\n",
    "    text = clean_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = clean_encoded_chars(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "kg_train = pd.read_csv('../data/kg_train.csv')\n",
    "kg_test = pd.read_csv('../data/kg_test.csv')\n",
    "\n",
    "X_train = kg_train['text']\n",
    "y_train = kg_train['label']\n",
    "X_test = kg_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/gs_200v517s5clp2y1jt0mgc0000gn/T/ipykernel_14511/606309534.py:2: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n",
      "/var/folders/2n/gs_200v517s5clp2y1jt0mgc0000gn/T/ipykernel_14511/606309534.py:2: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "# Apply the cleaning pipeline\n",
    "X_train_cleaned = X_train.apply(full_clean_pipeline)\n",
    "X_test_cleaned = X_test.apply(full_clean_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9681475272422464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       674\n",
      "           1       0.99      0.94      0.96       519\n",
      "\n",
      "    accuracy                           0.97      1193\n",
      "   macro avg       0.97      0.96      0.97      1193\n",
      "weighted avg       0.97      0.97      0.97      1193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train_cleaned, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "X_train_sub_tfidf = vectorizer.fit_transform(X_train_sub)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_sub_tfidf, y_train_sub)\n",
    "\n",
    "val_predictions = model.predict(X_val_tfidf)\n",
    "\n",
    "print(\"accuracy:\", accuracy_score(y_val, val_predictions))\n",
    "print(classification_report(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain on the full training set and predict on the test set\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_cleaned)\n",
    "X_test_tfidf = vectorizer.transform(X_test_cleaned)\n",
    "\n",
    "final_model = MultinomialNB()\n",
    "final_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "test_predictions = final_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_submission = pd.DataFrame({\n",
    "    'text': kg_test['text'],\n",
    "    'predicted_label': test_predictions\n",
    "})\n",
    "\n",
    "final_model_submission.head()\n",
    "\n",
    "# save to file\n",
    "final_model_submission.to_csv('spam_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
