{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hq/l56ghxv518j9wg6pgqkbbvd80000gn/T/ipykernel_76198/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "0     DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                              Will do.      0\n",
      "2     Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3     Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                   fyi      0\n",
      "...                                                 ...    ...\n",
      "5959  I talked to CDM about doing some strategy sess...      0\n",
      "5960  Dear Sir/CEO,=20I am Mr.D S Ammer, the purchas...      1\n",
      "5961                                 Faxing to you now.      0\n",
      "5962  Goodday,I am Joseph vaye,the son of late Issac...      1\n",
      "5963  Hello my dearMy name is Prince George Sanoussi...      1\n",
      "\n",
      "[5964 rows x 2 columns]\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"/Users/test/Desktop/ironhack_labs/lab-natural-language-processing/data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "print(data)\n",
    "\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "0     DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                              Will do.      0\n",
      "2     Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3     Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                   fyi      0\n",
      "...                                                 ...    ...\n",
      "5959  I talked to CDM about doing some strategy sess...      0\n",
      "5960  Dear Sir/CEO,=20I am Mr.D S Ammer, the purchas...      1\n",
      "5961                                 Faxing to you now.      0\n",
      "5962  Goodday,I am Joseph vaye,the son of late Issac...      1\n",
      "5963  Hello my dearMy name is Prince George Sanoussi...      1\n",
      "\n",
      "[5964 rows x 2 columns]\n",
      "(1000, 2)\n",
      "Training set size: (800,)\n",
      "Test set size: (200,)\n",
      "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
      "535    I have not been able to reach oscar this am. W...\n",
      "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
      "557    I can have it announced here on Monday - can't...\n",
      "836        BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n",
      "                             ...                        \n",
      "106    7653 2612ADAMA IBRAHIM________________________...\n",
      "270               What does that mean for our schedules?\n",
      "860    Dear Friend,My Compliment to you,I guess this ...\n",
      "435    Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...\n",
      "102    Let me know if today or tomorrow works for you...\n",
      "Name: text, Length: 800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define your feature column (e.g., \"text\") and label column (e.g., \"label\")\n",
    "# Replace 'text_column_name' and 'label_column_name' with your actual column names\n",
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"/Users/test/Desktop/ironhack_labs/lab-natural-language-processing/data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "print(data)\n",
    "\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)\n",
    "X = data['text']   # Features (text)\n",
    "y = data['label']  # Labels (target)\n",
    "\n",
    "# Split into train and test sets (e.g., 80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Confirm the split\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "\n",
    "data_train = X_train\n",
    "data_val = X_test\n",
    "print(data_train) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
      "535    I have not been able to reach oscar this am. W...\n",
      "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
      "557    I can have it announced here on Monday - can't...\n",
      "836        BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n",
      "                             ...                        \n",
      "106    7653 2612ADAMA IBRAHIM________________________...\n",
      "270               What does that mean for our schedules?\n",
      "860    Dear Friend,My Compliment to you,I guess this ...\n",
      "435    Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...\n",
      "102    Let me know if today or tomorrow works for you...\n",
      "Name: text, Length: 800, dtype: object\n",
      "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
      "535    I have not been able to reach oscar this am. W...\n",
      "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
      "557    I can have it announced here on Monday - can't...\n",
      "836        BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n",
      "                             ...                        \n",
      "106    7653 2612ADAMA IBRAHIM________________________...\n",
      "270               What does that mean for our schedules?\n",
      "860    Dear Friend,My Compliment to you,I guess this ...\n",
      "435    Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...\n",
      "102    Let me know if today or tomorrow works for you...\n",
      "Name: text, Length: 800, dtype: object\n",
      "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
      "535    I have not been able to reach oscar this am. W...\n",
      "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
      "557    I can have it announced here on Monday - can't...\n",
      "836        BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: Remove inline JS/CSS\n",
    "def remove_inline_js_css(df):\n",
    "    def clean_html(text):\n",
    "        # Remove <script>...</script>\n",
    "        text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "        # Remove <style>...</style>\n",
    "        text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "        # Remove inline JavaScript event handlers like onclick=\"...\"\n",
    "        text = re.sub(r'on\\w+=\"[^\"]*\"', '', text, flags=re.IGNORECASE)\n",
    "        # Remove inline styles\n",
    "        text = re.sub(r'style=\"[^\"]*\"', '', text, flags=re.IGNORECASE)\n",
    "        return text\n",
    "\n",
    "    return df.apply(clean_html)\n",
    "\n",
    "data_train = remove_inline_js_css(data_train)\n",
    "print(data_train)\n",
    "\n",
    "\n",
    "# Step 2: Remove HTML comments\n",
    "def remove_html_comments(df):\n",
    "    def remove_comments(text):\n",
    "        return re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    return df.apply(remove_comments)\n",
    "\n",
    "data_train = remove_html_comments(data_train)\n",
    "print(data_train)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "# Step 3: Remove remaining HTML tags\n",
    "def remove_html_tags(df):\n",
    "    def strip_tags(text):\n",
    "        return re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    return data_train.apply(strip_tags)\n",
    "\n",
    "\n",
    "data_train = remove_html_tags(data_train)\n",
    "\n",
    "\n",
    "print(data_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     regards mr nelson smith kindly reply me on my ...\n",
      "535    have not been able to reach oscar this am we a...\n",
      "695    huma abedin i checking with pat on the will wo...\n",
      "557       can have it announced here on monday can today\n",
      "836    bank of africaagence san pedro bp san pedro co...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def clean_text_regex(df):\n",
    "\n",
    "    def clean(text):\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)               # Remove special characters and punctuation\n",
    "        text = re.sub(r\"\\b\\d+\\b\", \"\", text)                    # Remove standalone numbers\n",
    "        text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)            # Remove single characters\n",
    "        text = re.sub(r\"^[a-zA-Z]\\s+\", \"\", text)               # Remove single characters from start\n",
    "        text = re.sub(r\"\\s+\", \" \", text)                       # Replace multiple spaces with single space\n",
    "        text = re.sub(r\"^b\\s+\", \"\", text)                      # Remove prefixed 'b' if exists\n",
    "        return text.lower().strip()                            # Convert to lowercase and strip edges\n",
    "\n",
    "    return df.apply(clean)\n",
    "\n",
    "data_train = clean_text_regex(data_train)\n",
    "print(data_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29     regards mr nelson smith kindly reply private e...\n",
       "535           able reach oscar supposed send pdb receive\n",
       "695    huma abedin checking pat work jack jake rest a...\n",
       "557                               announced monday today\n",
       "836    bank africaagence san pedro bp san pedro cote ...\n",
       "                             ...                        \n",
       "468    mrs farah al hashemiaddress chiang rai hospita...\n",
       "935    dear sir madam mr john coleman sister miss ros...\n",
       "428    saturday january pm sbwhoeopre fyi foreign nat...\n",
       "7      abedin huma saturday november pmhfw quint fmsi...\n",
       "155    xiamen oritent wanlistone company joint ventur...\n",
       "Name: text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(df):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def filter_stopwords(text):\n",
    "        tokens = text.split()\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "        return \" \".join(filtered_tokens)\n",
    "\n",
    "    return df.apply(filter_stopwords)\n",
    "\n",
    "data_train = remove_stopwords(data_train)\n",
    "\n",
    "data_train.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/test/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/test/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/test/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29     regard mr nelson smith kindly reply private em...\n",
       "535            able reach oscar suppose send pdb receive\n",
       "695    huma abedin check pat work jack jake rest also...\n",
       "557                                announce monday today\n",
       "836    bank africaagence san pedro bp san pedro cote ...\n",
       "                             ...                        \n",
       "468    mr farah al hashemiaddress chiang rai hospital...\n",
       "935    dear sir madam mr john coleman sister miss ris...\n",
       "428    saturday january pm sbwhoeopre fyi foreign nat...\n",
       "7      abedin huma saturday november pmhfw quint fmsi...\n",
       "155    xiamen oritent wanlistone company joint ventur...\n",
       "Name: text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Download required resources (only needs to be run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# POS mapping helper function\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Lemmatization function for a single string\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "# Apply to the DataFrame\n",
    "data_train = data_train.apply(lemmatize_text)\n",
    "\n",
    "\n",
    "data_train.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "Vocabulary: ['102' '106' '270' ... 'zzz' 'zzzahbxntxe' 'zzzj']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "data_train = data_train.astype(str)\n",
    "\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a BoW representation\n",
    "X = vectorizer.fit_transform(data_train)\n",
    "                             \n",
    "print(\"Bag of Words:\\n\", X.toarray())\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
       "535    I have not been able to reach oscar this am. W...\n",
       "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
       "557    I can have it announced here on Monday - can't...\n",
       "836        BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "#define patterns\n",
    "#match any of these words or symbols to the text\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"]) \n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"]) \n",
    "\n",
    "data_train['money_mark'] = data_train.str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train.str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train.apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val.str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val.str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val.apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (np.int32(0), np.int32(1))\t1.0\n",
      "  (np.int32(1), np.int32(0))\t1.0\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "# # Your code\n",
    "# # Step 1: Initialize the TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # Step 2: Fit and transform the corpus into a TF-IDF representation\n",
    "# X_tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "# print(X_tfidf)\n",
    "# print(X_tfidf.shape)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "data_train = data_train.astype(str)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(X_tfidf)\n",
    "print(X_tfidf.shape)\n",
    "\n",
    "                             \n",
    "# print(\"Bag of Words:\\n\", X.toarray())\n",
    "# print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (np.int32(0), np.int32(17265))\t0.19794644495682132\n",
      "  (np.int32(0), np.int32(14206))\t0.17577517319695612\n",
      "  (np.int32(0), np.int32(14556))\t0.4348479070390364\n",
      "  (np.int32(0), np.int32(18600))\t0.36506030424677033\n",
      "  (np.int32(0), np.int32(12579))\t0.2606047392491126\n",
      "  (np.int32(0), np.int32(17426))\t0.2031445982365822\n",
      "  (np.int32(0), np.int32(13705))\t0.128564650907443\n",
      "  (np.int32(0), np.int32(15253))\t0.12327793252226088\n",
      "  (np.int32(0), np.int32(14311))\t0.13105094836221168\n",
      "  (np.int32(0), np.int32(16514))\t0.19521931962375325\n",
      "  (np.int32(0), np.int32(8601))\t0.19311267387964398\n",
      "  (np.int32(0), np.int32(3495))\t0.19575618838274103\n",
      "  (np.int32(0), np.int32(14557))\t0.4826662507702955\n",
      "  (np.int32(0), np.int32(22844))\t0.24802686961780476\n",
      "  (np.int32(0), np.int32(6642))\t0.2031445982365822\n",
      "  (np.int32(1), np.int32(10634))\t0.12327723007352402\n",
      "  (np.int32(1), np.int32(14753))\t0.13923989042630888\n",
      "  (np.int32(1), np.int32(5410))\t0.16214545369483496\n",
      "  (np.int32(1), np.int32(3230))\t0.24369728481835157\n",
      "  (np.int32(1), np.int32(20092))\t0.19250548265523754\n",
      "  (np.int32(1), np.int32(17101))\t0.2708853282982126\n",
      "  (np.int32(1), np.int32(15447))\t0.4130065478864666\n",
      "  (np.int32(1), np.int32(19883))\t0.11549426877615115\n",
      "  (np.int32(1), np.int32(4114))\t0.12588642288358315\n",
      "  (np.int32(1), np.int32(22011))\t0.12629722186365763\n",
      "  :\t:\n",
      "  (np.int32(801), np.int32(8221))\t0.20530897050826608\n",
      "  (np.int32(802), np.int32(14413))\t0.07628152562207005\n",
      "  (np.int32(802), np.int32(1038))\t0.0977388622458712\n",
      "  (np.int32(802), np.int32(2084))\t0.15600180906373642\n",
      "  (np.int32(802), np.int32(19540))\t0.13193571626191974\n",
      "  (np.int32(802), np.int32(211))\t0.1872060720843235\n",
      "  (np.int32(802), np.int32(12993))\t0.1726969160462329\n",
      "  (np.int32(802), np.int32(205))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(2091))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(2334))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(2105))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(2570))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(1857))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(207))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(14085))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(19236))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(19541))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(2553))\t0.5371048174469841\n",
      "  (np.int32(802), np.int32(8221))\t0.17903493914899468\n",
      "  (np.int32(802), np.int32(294))\t0.19872263712695265\n",
      "  (np.int32(802), np.int32(344))\t0.19872263712695265\n",
      "  (np.int32(802), np.int32(1615))\t0.19872263712695265\n",
      "  (np.int32(802), np.int32(574))\t0.19872263712695265\n",
      "  (np.int32(802), np.int32(2552))\t0.19872263712695265\n",
      "  (np.int32(802), np.int32(11714))\t0.19872263712695265\n",
      "(803, 23392)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "data_train = data_train.astype(str)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data_train)\n",
    "\n",
    "print(X_tfidf)\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
