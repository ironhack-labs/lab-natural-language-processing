{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_14144\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\mktmi\\\\Documents\\\\ironhack\\\\AI_Engineering\\\\Work\\\\Week4\\\\lab-natural-language-processing-1\\\\data\\\\kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development.\n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "\n",
    "# Print the shape of the data\n",
    "print(data.shape)\n",
    "\n",
    "# Fill missing values with an empty string\n",
    "data.fillna(\"\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (800, 1) (800,)\n",
      "Testing set shape: (200, 1) (200,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"C:/Users/mktmi/Documents/ironhack/AI_Engineering/Work/Week4/lab-natural-language-processing-1/data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development.\n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "\n",
    "# Fill missing values with an empty string\n",
    "data.fillna(\"\", inplace=True)\n",
    "\n",
    "# Assuming 'label' is the target column and all others are features\n",
    "X = data.drop(columns=['label'])  # Features\n",
    "y = data['label']  # Target variable\n",
    "\n",
    "# Split the data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes to verify the split\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: This is a header This is a paragraph with a link.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html_content):\n",
    "    # Step 1: Remove inline JavaScript/CSS\n",
    "    # Using regex to remove content within <script> and <style> tags\n",
    "    html_content = re.sub(r'<(script|style).*?>.*?</\\1>', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 2: Remove HTML comments\n",
    "    # Removing comments using regex\n",
    "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    # Using BeautifulSoup to strip the remaining HTML tags\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "    \n",
    "    # Optional: You can further clean the text, e.g., removing extra whitespace or special characters\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replace multiple spaces with a single space\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Example usage on some HTML content\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><style>body {font-size: 12px;}</style><script>alert('This is JS');</script></head>\n",
    "<body>\n",
    "    <!-- This is a comment -->\n",
    "    <h1>This is a header</h1>\n",
    "    <p>This is a <strong>paragraph</strong> with <a href=\"#\">a link</a>.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_html(html_content)\n",
    "print(\"Cleaned text:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: this is header this is paragraph with link special characters\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html_content):\n",
    "    # Step 1: Remove inline JavaScript/CSS\n",
    "    html_content = re.sub(r'<(script|style).*?>.*?</\\1>', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 2: Remove HTML comments\n",
    "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "\n",
    "    # Step 4: Remove special characters (keeping only letters and spaces)\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', cleaned_text)\n",
    "    \n",
    "    # Step 5: Remove numbers\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "    \n",
    "    # Step 6: Remove all single characters\n",
    "    cleaned_text = re.sub(r'\\b\\w\\b', '', cleaned_text)\n",
    "    \n",
    "    # Step 7: Remove single characters from the start\n",
    "    cleaned_text = re.sub(r'\\b\\w{1}\\s', '', cleaned_text)\n",
    "    \n",
    "    # Step 8: Substitute multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    # Step 9: Remove prefixed 'b' (if present in text encoding issues)\n",
    "    cleaned_text = re.sub(r'^b\\s+', '', cleaned_text)\n",
    "    \n",
    "    # Step 10: Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    # Strip leading/trailing spaces\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Example usage on some HTML content\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><style>body {font-size: 12px;}</style><script>alert('This is JS');</script></head>\n",
    "<body>\n",
    "    <!-- This is a comment -->\n",
    "    <h1>This is a header 123</h1>\n",
    "    <p>This is a b <strong>paragraph</strong> with 56 <a href=\"#\">a link</a> & special characters!!!.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_html(html_content)\n",
    "print(\"Cleaned text:\", cleaned_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: header paragraph link special characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mktmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Ensure you have downloaded the stopwords dataset from NLTK\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_html(html_content):\n",
    "    # Step 1: Remove inline JavaScript/CSS\n",
    "    html_content = re.sub(r'<(script|style).*?>.*?</\\1>', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 2: Remove HTML comments\n",
    "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "\n",
    "    # Step 4: Remove special characters (keeping only letters and spaces)\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', cleaned_text)\n",
    "    \n",
    "    # Step 5: Remove numbers\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "    \n",
    "    # Step 6: Remove all single characters\n",
    "    cleaned_text = re.sub(r'\\b\\w\\b', '', cleaned_text)\n",
    "    \n",
    "    # Step 7: Remove single characters from the start\n",
    "    cleaned_text = re.sub(r'\\b\\w{1}\\s', '', cleaned_text)\n",
    "    \n",
    "    # Step 8: Substitute multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    # Step 9: Remove prefixed 'b'\n",
    "    cleaned_text = re.sub(r'^b\\s+', '', cleaned_text)\n",
    "    \n",
    "    # Step 10: Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    # Step 11: Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = cleaned_text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the remaining words back into a single string\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "\n",
    "    # Strip leading/trailing spaces and return cleaned text\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Example usage on some HTML content\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><style>body {font-size: 12px;}</style><script>alert('This is JS');</script></head>\n",
    "<body>\n",
    "    <!-- This is a comment -->\n",
    "    <h1>This is a header 123</h1>\n",
    "    <p>This is a b <strong>paragraph</strong> with 56 <a href=\"#\">a link</a> & special characters!!!.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_html(html_content)\n",
    "print(\"Cleaned text:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mktmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\mktmi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\mktmi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: dog running faster cat paragraph link special character\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_html(html_content):\n",
    "    # Step 1: Remove inline JavaScript/CSS\n",
    "    html_content = re.sub(r'<(script|style).*?>.*?</\\1>', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 2: Remove HTML comments\n",
    "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "\n",
    "    # Step 4: Remove special characters (keeping only letters and spaces)\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', cleaned_text)\n",
    "    \n",
    "    # Step 5: Remove numbers\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "    \n",
    "    # Step 6: Remove all single characters\n",
    "    cleaned_text = re.sub(r'\\b\\w\\b', '', cleaned_text)\n",
    "    \n",
    "    # Step 7: Remove single characters from the start\n",
    "    cleaned_text = re.sub(r'\\b\\w{1}\\s', '', cleaned_text)\n",
    "    \n",
    "    # Step 8: Substitute multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    # Step 9: Remove prefixed 'b'\n",
    "    cleaned_text = re.sub(r'^b\\s+', '', cleaned_text)\n",
    "    \n",
    "    # Step 10: Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    # Step 11: Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = cleaned_text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Step 12: Apply Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    \n",
    "    # Join the lemmatized words back into a single string\n",
    "    cleaned_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    # Strip leading/trailing spaces and return cleaned text\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Example usage on some HTML content\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><style>body {font-size: 12px;}</style><script>alert('This is JS');</script></head>\n",
    "<body>\n",
    "    <!-- This is a comment -->\n",
    "    <h1>Dogs are running faster than cats!</h1>\n",
    "    <p>This is a b <strong>paragraph</strong> with 56 <a href=\"#\">a link</a> & special characters!!!.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_html(html_content)\n",
    "print(\"Cleaned text:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_14144\\2523896845.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, 'html.parser')\n",
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_14144\\2523896845.py:16: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label values: [1 0]\n",
      "Top 10 words in ham messages:\n",
      "       word  count\n",
      "28927   the  13765\n",
      "30086    to   8369\n",
      "1433    and   6694\n",
      "19932    of   6507\n",
      "14083    in   4892\n",
      "28799  that   3065\n",
      "15068    is   2874\n",
      "10870   for   2747\n",
      "20348    on   2578\n",
      "33108  with   1899\n",
      "\n",
      "Top 10 words in spam messages:\n",
      "       word  count\n",
      "37061   the  38709\n",
      "38258    to  31169\n",
      "27174    of  27601\n",
      "2387    and  21392\n",
      "19198    in  17473\n",
      "42238   you  16285\n",
      "37826  this  14051\n",
      "42457  your  11666\n",
      "25555    my  11635\n",
      "15390   for  11618\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Define the clean_html function\n",
    "def clean_html(html_content):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    html_content = re.sub(r'<(script|style).*?>.*?</\\1>', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "\n",
    "    # Remove special characters (keeping only letters and spaces)\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', cleaned_text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "    \n",
    "    # Remove single characters and substitute multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\b\\w\\b', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    return cleaned_text.lower().strip()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C:/Users/mktmi/Documents/ironhack/AI_Engineering/Work/Week4/lab-natural-language-processing-1/data/kg_train.csv\", encoding='latin-1')\n",
    "data.fillna(\"\", inplace=True)  # Handle missing values\n",
    "\n",
    "# Apply the clean_html function to clean the 'text' column\n",
    "data['cleaned_text'] = data['text'].apply(clean_html)\n",
    "\n",
    "# Check the unique values in the 'label' column\n",
    "unique_labels = data['label'].unique()\n",
    "print(\"Unique label values:\", unique_labels)\n",
    "\n",
    "# Adjust the following lines based on your label values\n",
    "if 'ham' in unique_labels and 'spam' in unique_labels:\n",
    "    ham_messages = data[data['label'] == 'ham']['cleaned_text']\n",
    "    spam_messages = data[data['label'] == 'spam']['cleaned_text']\n",
    "else:\n",
    "    # Example adjustment: if 0 is for ham and 1 is for spam\n",
    "    ham_messages = data[data['label'] == 0]['cleaned_text']\n",
    "    spam_messages = data[data['label'] == 1]['cleaned_text']\n",
    "\n",
    "# If no valid messages are found, raise an error\n",
    "if ham_messages.empty or spam_messages.empty:\n",
    "    raise ValueError(\"Ensure the dataset contains correctly labeled messages (e.g., 'ham' and 'spam' or 0 and 1)\")\n",
    "\n",
    "# Step 1: Initialize CountVectorizer for Bag of Words\n",
    "vectorizer_ham = CountVectorizer()\n",
    "vectorizer_spam = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the ham and spam messages separately\n",
    "ham_bow = vectorizer_ham.fit_transform(ham_messages)\n",
    "spam_bow = vectorizer_spam.fit_transform(spam_messages)\n",
    "\n",
    "# Step 3: Sum word frequencies for both ham and spam\n",
    "ham_word_counts = np.asarray(ham_bow.sum(axis=0)).flatten()\n",
    "spam_word_counts = np.asarray(spam_bow.sum(axis=0)).flatten()\n",
    "\n",
    "# Step 4: Create DataFrames to store word counts for ham and spam separately\n",
    "ham_word_freq = pd.DataFrame({'word': vectorizer_ham.get_feature_names_out(), 'count': ham_word_counts})\n",
    "spam_word_freq = pd.DataFrame({'word': vectorizer_spam.get_feature_names_out(), 'count': spam_word_counts})\n",
    "\n",
    "# Step 5: Get top 10 words for ham and spam\n",
    "top_10_ham = ham_word_freq.sort_values(by='count', ascending=False).head(10)\n",
    "top_10_spam = spam_word_freq.sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "# Display the top 10 words for ham and spam\n",
    "print(\"Top 10 words in ham messages:\")\n",
    "print(top_10_ham)\n",
    "\n",
    "print(\"\\nTop 10 words in spam messages:\")\n",
    "print(top_10_spam)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_14144\\3049394148.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, 'html.parser')\n",
      "C:\\Users\\mktmi\\AppData\\Local\\Temp\\ipykernel_14144\\3049394148.py:13: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, 'html.parser')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3821</th>\n",
       "      <td>back -- we need to talkCall only my berry.</td>\n",
       "      <td>0</td>\n",
       "      <td>back we need to talkcall only my berry</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>What is mullen's first name?</td>\n",
       "      <td>0</td>\n",
       "      <td>what is mullens first name</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>Greetings from Dubai=2CThis letter must come t...</td>\n",
       "      <td>1</td>\n",
       "      <td>greetings from dubaicthis letter must come to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4231</th>\n",
       "      <td>Sullivan Jacob J &lt;SullivanJJ@state.gov&gt;Friday ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sullivan jacob friday december amiranthe eu me...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>Fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "3821         back -- we need to talkCall only my berry.      0   \n",
       "2407                       What is mullen's first name?      0   \n",
       "1173  Greetings from Dubai=2CThis letter must come t...      1   \n",
       "4231  Sullivan Jacob J <SullivanJJ@state.gov>Friday ...      0   \n",
       "1666                                                Fyi      0   \n",
       "\n",
       "                                      preprocessed_text  money_mark  \\\n",
       "3821             back we need to talkcall only my berry           0   \n",
       "2407                         what is mullens first name           0   \n",
       "1173  greetings from dubaicthis letter must come to ...           0   \n",
       "4231  sullivan jacob friday december amiranthe eu me...           0   \n",
       "1666                                                fyi           0   \n",
       "\n",
       "      suspicious_words  text_len  \n",
       "3821                 0        38  \n",
       "2407                 0        26  \n",
       "1173                 1      3726  \n",
       "4231                 0       575  \n",
       "1666                 0         3  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the clean_html function\n",
    "def clean_html(html_content):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    html_content = re.sub(r'<(script|style).*?>.*?</\\1>', '', html_content, flags=re.DOTALL)\n",
    "    # Remove HTML comments\n",
    "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "    # Remove remaining HTML tags\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    cleaned_text = soup.get_text()\n",
    "    # Remove special characters (keeping only letters and spaces)\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', cleaned_text)\n",
    "    # Remove numbers\n",
    "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
    "    # Remove single characters and substitute multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\b\\w\\b', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    # Convert to lowercase\n",
    "    return cleaned_text.lower().strip()\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"C:/Users/mktmi/Documents/ironhack/AI_Engineering/Work/Week4/lab-natural-language-processing-1/data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Apply the clean_html function to clean the 'text' column\n",
    "data['preprocessed_text'] = data['text'].apply(clean_html)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Lists for money symbols and suspicious words\n",
    "money_symbol_list = \"|\".join([r\"\\beuro\\b\", r\"\\bdollar\\b\", r\"\\bpound\\b\", r\"â‚¬\", r\"\\$\"])\n",
    "suspicious_words = \"|\".join([r\"\\bfree\\b\", r\"\\bcheap\\b\", r\"\\bsex\\b\", r\"\\bmoney\\b\", r\"\\baccount\\b\", r\"\\bbank\\b\", \n",
    "                             r\"\\bfund\\b\", r\"\\btransfer\\b\", r\"\\btransaction\\b\", r\"\\bwin\\b\", r\"\\bdeposit\\b\", r\"\\bpassword\\b\"])\n",
    "\n",
    "# Add indicators to the training set\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_symbol_list, regex=True) * 1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words, regex=True) * 1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "# Add indicators to the validation set\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_symbol_list, regex=True) * 1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words, regex=True) * 1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "# Show the updated training data\n",
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'love': 6, 'nlp': 8, 'is': 4, 'amazing': 0, 'machine': 7, 'learning': 5, 'and': 1, 'are': 2, 'great': 3}\n",
      "BoW representation (array):\n",
      " [[1 0 0 0 1 0 1 0 2]\n",
      " [0 0 0 0 0 1 1 1 0]\n",
      " [0 1 1 1 0 1 0 1 1]]\n",
      "Feature names (words): ['amazing' 'and' 'are' 'great' 'is' 'learning' 'love' 'machine' 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text corpus\n",
    "corpus = [\n",
    "    \"I love NLP. NLP is amazing.\",\n",
    "    \"I love machine learning.\",\n",
    "    \"NLP and machine learning are great.\"\n",
    "]\n",
    "\n",
    "# Step 1: Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit the CountVectorizer to the text data\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 3: Show the vocabulary (words and their index)\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# Step 4: Convert the document-term matrix to an array (for better visualization)\n",
    "print(\"BoW representation (array):\\n\", X.toarray())\n",
    "\n",
    "# Step 5: Display the feature names (words in the vocabulary)\n",
    "print(\"Feature names (words):\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorized dataset: (4771, 61758)\n",
      "Feature names (words): ['aa' 'aaadecfbbefdc' 'aabidjan' ... 'zusichere' 'zwei' 'zyedc']\n",
      "TF-IDF matrix:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your dataset (replace with the actual path to your dataset)\n",
    "data = pd.read_csv(\"C:/Users/mktmi/Documents/ironhack/AI_Engineering/Work/Week4/lab-natural-language-processing-1/data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Assuming the dataset has a 'preprocessed_text' column containing the cleaned text\n",
    "corpus = data_train['preprocessed_text'].tolist()\n",
    "\n",
    "# Step 1: Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the dataset (vectorize the corpus)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 3: Print the shape of the vectorized dataset (documents x terms)\n",
    "print(\"Shape of the vectorized dataset:\", X.shape)\n",
    "\n",
    "# Step 4: Display feature names (words in the vocabulary)\n",
    "print(\"Feature names (words):\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# (Optional) Step 5: Display the TF-IDF matrix in array form\n",
    "print(\"TF-IDF matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9591623036649215\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.96       538\n",
      "           1       0.99      0.91      0.95       417\n",
      "\n",
      "    accuracy                           0.96       955\n",
      "   macro avg       0.97      0.95      0.96       955\n",
      "weighted avg       0.96      0.96      0.96       955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "data = pd.read_csv(\"C:/Users/mktmi/Documents/ironhack/AI_Engineering/Work/Week4/lab-natural-language-processing-1/data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Step 2: Assuming the dataset has a 'preprocessed_text' column and a 'label' column (ham/spam)\n",
    "corpus = data_train['preprocessed_text'].tolist()\n",
    "labels = data_train['label'].tolist()\n",
    "\n",
    "# Step 3: Initialize the TfidfVectorizer and vectorize the dataset\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Initialize the classifier (Logistic Regression)\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Step 6: Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict the labels for the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 9: Print detailed classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_spam_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the dataset from the given Kaggle link (assuming CSV format)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_to_spam_dataset.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 1: Preprocess the dataset\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Assuming the dataset has a 'text' column and 'label' column ('spam' or 'ham')\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Clean the text (example cleaning function)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(text):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Lowercasing and removing punctuation, numbers, and extra spaces\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_spam_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the dataset from the given Kaggle link (assuming CSV format)\n",
    "data = pd.read_csv(\"path_to_spam_dataset.csv\", encoding='latin-1')\n",
    "\n",
    "# Step 1: Preprocess the dataset\n",
    "# Assuming the dataset has a 'text' column and 'label' column ('spam' or 'ham')\n",
    "# Clean the text (example cleaning function)\n",
    "def clean_text(text):\n",
    "    # Lowercasing and removing punctuation, numbers, and extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# Step 2: Feature Representation\n",
    "# Try different feature representations\n",
    "# Example: You can toggle between CountVectorizer and TfidfVectorizer\n",
    "\n",
    "# Option 1: Bag of Words (CountVectorizer)\n",
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "# Option 2: TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text data into features\n",
    "X = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "# Assuming 'label' column is binary (0 for ham, 1 for spam)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the Classifier (Multinomial Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Step 6: Print Accuracy and Classification Report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Optional: Display feature names (if needed)\n",
    "print(\"Feature names:\\n\", vectorizer.get_feature_names_out())\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
