{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_22092\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"C:/Users/ASUS/OneDrive/Desktop/ironhacklab/week_four/day-1/lab-natural-language-processing/data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = pd.read_csv(\"C:/Users/ASUS/OneDrive/Desktop/ironhacklab/week_four/day-1/lab-natural-language-processing/data/kg_train.csv\")\n",
    "\n",
    "data_train, data_val = train_test_split(train_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\asus\\anaconda3\\lib\\site-packages (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import re\n",
    "# For Javasript\n",
    "def remove_script_code(data):\n",
    "    clean = re.compile('<script>.*?</script>')\n",
    "    return [re.sub(clean, '', data)]\n",
    "# For CSS Style\n",
    "def remove_style_code2(data):\n",
    "    clean = re.compile('<style>.*?</style>')\n",
    "    return [re.sub(clean, '', data)]\n",
    "data_train['clean_data']=data_train['text'].apply(remove_script_code)\n",
    "data_train['clean_data']=data_train['text'].apply(remove_style_code2)\n",
    "data_val['clean_data2']=data_val['text'].apply(remove_script_code)\n",
    "data_val['clean_data2']=data_val['text'].apply(remove_style_code2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def clean_text(text):\n",
    "# Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Remove prefixed 'b' (sometimes appears from byte encoding issues)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "data_train['clean_data']=data_train['text'].apply(clean_text)\n",
    "data_val['clean_data2']=data_val['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import string\n",
    "\n",
    "# Define a set of common English stopwords\n",
    "common_stopwords = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "    \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\",\n",
    "    \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\",\n",
    "    \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\",\n",
    "    \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\",\n",
    "    \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\",\n",
    "    \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\"\n",
    "])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Tokenize by splitting on spaces\n",
    "    filtered_words = [word for word in words if word not in common_stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Apply the function to remove stopwords from the 'cleaned_text' column\n",
    "data_train['remove_stopwords'] = data_train['clean_data'].apply(remove_stopwords)\n",
    "data_val['remove_stopwords2'] = data_val['clean_data2'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_train['cleaned_text'] = data_train['remove_stopwords'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "data_val['cleaned_text'] = data_val['remove_stopwords2'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Spam Words:\n",
      "       Word  Count\n",
      "0      will   7966\n",
      "1     money   4416\n",
      "2      bank   3615\n",
      "3   account   3596\n",
      "4      fund   3110\n",
      "5       not   2756\n",
      "6         u   2447\n",
      "7       all   2271\n",
      "8  business   2031\n",
      "9       any   2002\n",
      "\n",
      "Top 10 Ham Words:\n",
      "        Word  Count\n",
      "0       will    996\n",
      "1        not    895\n",
      "2      state    797\n",
      "3         no    778\n",
      "4         pm    705\n",
      "5          u    606\n",
      "6      would    554\n",
      "7        can    538\n",
      "8  secretary    511\n",
      "9        all    490\n",
      "Top 10 Spam Words:\n",
      "       Word  Count\n",
      "0      will   7966\n",
      "1     money   4416\n",
      "2      bank   3615\n",
      "3   account   3596\n",
      "4      fund   3110\n",
      "5       not   2756\n",
      "6         u   2447\n",
      "7       all   2271\n",
      "8  business   2031\n",
      "9       any   2002\n",
      "\n",
      "Top 10 Ham Words:\n",
      "        Word  Count\n",
      "0       will    996\n",
      "1        not    895\n",
      "2      state    797\n",
      "3         no    778\n",
      "4         pm    705\n",
      "5          u    606\n",
      "6      would    554\n",
      "7        can    538\n",
      "8  secretary    511\n",
      "9        all    490\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "spam_words = \" \".join(data_train[data_train[\"label\"] == 1][\"cleaned_text\"]).split()\n",
    "ham_words = \" \".join(data_train[data_train[\"label\"] == 0][\"cleaned_text\"]).split()\n",
    "top_spam_words = Counter(spam_words).most_common(10)\n",
    "top_ham_words = Counter(ham_words).most_common(10)\n",
    "top_spam_df = pd.DataFrame(top_spam_words, columns=[\"Word\", \"Count\"])\n",
    "top_ham_df = pd.DataFrame(top_ham_words, columns=[\"Word\", \"Count\"])\n",
    "print(\"Top 10 Spam Words:\")\n",
    "print(top_spam_df)\n",
    "\n",
    "print(\"\\nTop 10 Ham Words:\")\n",
    "print(top_ham_df)\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "spam_words2 = \" \".join(data_val[data_val[\"label\"] == 1][\"cleaned_text\"]).split()\n",
    "ham_words2 = \" \".join(data_val[data_val[\"label\"] == 0][\"cleaned_text\"]).split()\n",
    "top_spam_words2 = Counter(spam_words).most_common(10)\n",
    "top_ham_words2 = Counter(ham_words).most_common(10)\n",
    "top_spam_df2 = pd.DataFrame(top_spam_words, columns=[\"Word\", \"Count\"])\n",
    "top_ham_df2 = pd.DataFrame(top_ham_words, columns=[\"Word\", \"Count\"])\n",
    "print(\"Top 10 Spam Words:\")\n",
    "print(top_spam_df2)\n",
    "\n",
    "print(\"\\nTop 10 Ham Words:\")\n",
    "print(top_ham_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_data</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3821</th>\n",
       "      <td>back -- we need to talkCall only my berry.</td>\n",
       "      <td>0</td>\n",
       "      <td>back we need to talkcall only my berry</td>\n",
       "      <td>back need talkcall only berry</td>\n",
       "      <td>back need talkcall only berry</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>What is mullen's first name?</td>\n",
       "      <td>0</td>\n",
       "      <td>what is mullens first name</td>\n",
       "      <td>mullens first name</td>\n",
       "      <td>mullens first name</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>Greetings from Dubai=2CThis letter must come t...</td>\n",
       "      <td>1</td>\n",
       "      <td>greetings from dubaicthis letter must come to ...</td>\n",
       "      <td>greetings dubaicthis letter must come big surp...</td>\n",
       "      <td>greeting dubaicthis letter must come big surpr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4231</th>\n",
       "      <td>Sullivan Jacob J &lt;SullivanJJ@state.gov&gt;Friday ...</td>\n",
       "      <td>0</td>\n",
       "      <td>sullivan jacob sullivanjjstategovfriday decemb...</td>\n",
       "      <td>sullivan jacob sullivanjjstategovfriday decemb...</td>\n",
       "      <td>sullivan jacob sullivanjjstategovfriday decemb...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>Fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "3821         back -- we need to talkCall only my berry.      0   \n",
       "2407                       What is mullen's first name?      0   \n",
       "1173  Greetings from Dubai=2CThis letter must come t...      1   \n",
       "4231  Sullivan Jacob J <SullivanJJ@state.gov>Friday ...      0   \n",
       "1666                                                Fyi      0   \n",
       "\n",
       "                                             clean_data  \\\n",
       "3821             back we need to talkcall only my berry   \n",
       "2407                         what is mullens first name   \n",
       "1173  greetings from dubaicthis letter must come to ...   \n",
       "4231  sullivan jacob sullivanjjstategovfriday decemb...   \n",
       "1666                                                fyi   \n",
       "\n",
       "                                       remove_stopwords  \\\n",
       "3821                      back need talkcall only berry   \n",
       "2407                                 mullens first name   \n",
       "1173  greetings dubaicthis letter must come big surp...   \n",
       "4231  sullivan jacob sullivanjjstategovfriday decemb...   \n",
       "1666                                                fyi   \n",
       "\n",
       "                                           cleaned_text  money_mark  \\\n",
       "3821                      back need talkcall only berry           1   \n",
       "2407                                 mullens first name           1   \n",
       "1173  greeting dubaicthis letter must come big surpr...           1   \n",
       "4231  sullivan jacob sullivanjjstategovfriday decemb...           1   \n",
       "1666                                                fyi           1   \n",
       "\n",
       "      suspicious_words  text_len  \n",
       "3821                 0        29  \n",
       "2407                 0        18  \n",
       "1173                 1      2680  \n",
       "4231                 1       445  \n",
       "1666                 0         3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['cleaned_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['cleaned_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['cleaned_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['cleaned_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['cleaned_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['cleaned_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# to ignor the word order and focuse on word frequency we can only do that with using count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF vectorized training data: (4771, 65078)\n",
      "Shape of TF-IDF vectorized validation data: (1193, 65078)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(data_train[\"clean_data\"])\n",
    "X_val_tfidf = vectorizer.transform(data_val[\"clean_data2\"])\n",
    "\n",
    "print(\"Shape of TF-IDF vectorized training data:\", X_train_tfidf.shape)\n",
    "print(\"Shape of TF-IDF vectorized validation data:\", X_val_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9580888516345348\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "model.fit(X_train_tfidf, data_train[\"label\"])\n",
    "\n",
    "y_pred = model.predict(X_val_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(data_val[\"label\"], y_pred)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "df = pd.read_csv('../input/spamemailsdataset/Spam.csv')\n",
    "#df.drop([''],axis=1, inplace=True)\n",
    "\n",
    "target = 'spam'\n",
    "labels = ['Ham','Spam']\n",
    "#Removal of any Duplicate rows \n",
    "df1 = df.copy()\n",
    "df1.drop_duplicates(inplace=True)\n",
    "df1.reset_index(drop=True,inplace=True)\n",
    "#Check for empty elements\n",
    "df1.isnull().count()\n",
    "\n",
    "X = df.drop([target],axis=1)\n",
    "Y = df[target]\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=0)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
