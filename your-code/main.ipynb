{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KK\\AppData\\Local\\Temp\\ipykernel_24588\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                           Will do.      0\n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                fyi      0\n",
      "Training set size: (800, 1), Test set size: (200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "print(data.head())\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' contains the features and 'label_column' is the target column\n",
    "X = data.drop(columns=['label'])  # remove label from the input ds to form training data\n",
    "y = data['label']  # Label data moved to y\n",
    "\n",
    "# Split the dataset into training and test sets, using 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the new sets\n",
    "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
      "535  I have not been able to reach oscar this am. W...\n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...\n",
      "557  I can have it announced here on Monday - can't...\n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_html_with_regex(raw_html):\n",
    "    # Step 1: Remove inline JavaScript/CSS (scripts and styles)\n",
    "    cleaned = re.sub(r'<(script|style).*?>.*?</\\1>', '', raw_html, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 2: Remove HTML comments\n",
    "    cleaned = re.sub(r'<!--.*?-->', '', cleaned, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    cleaned = re.sub(r'<.*?>', '', cleaned)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# Apply the cleaning function to X_train and X_test, assuming 'text_column' contains the HTML data\n",
    "X_train['text'] = X_train['text'].apply(clean_html_with_regex)\n",
    "X_test['text'] = X_test['text'].apply(clean_html_with_regex)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text\n",
      "29   regards mr nelson smithkindly reply me on my p...\n",
      "535  have not been able to reach oscar this am we a...\n",
      "695  huma abedin bim checking with pat on the will ...\n",
      "557    can have it announced here on monday cant today\n",
      "836  bank of africaagence san pedro bp san pedro co...\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Step 1: Remove special characters (except spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Step 2: Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Step 3: Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Step 4: Remove single characters from the start (e.g., 'a word' -> ' word')\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # Step 5: Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Step 6: Remove prefixed 'b' (common issue from byte strings)\n",
    "    text = re.sub(r'\\bb\\b', '', text)\n",
    "    \n",
    "    # Step 7: Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text.strip()  # Step 8: Strip leading/trailing spaces\n",
    "\n",
    "# Apply the function to your X_train and X_test cleaned_text columns\n",
    "X_train['text'] = X_train['text'].apply(clean_text)\n",
    "X_test['text'] = X_test['text'].apply(clean_text)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "29   regards mr nelson smithkindly reply me on my p...   \n",
      "535  have not been able to reach oscar this am we a...   \n",
      "695  huma abedin bim checking with pat on the will ...   \n",
      "557    can have it announced here on monday cant today   \n",
      "836  bank of africaagence san pedro bp san pedro co...   \n",
      "\n",
      "                                     text_no_stopwords  \n",
      "29   regards mr nelson smithkindly reply private em...  \n",
      "535         able reach oscar supposed send pdb receive  \n",
      "695  huma abedin bim checking pat work jack jake re...  \n",
      "557                        announced monday cant today  \n",
      "836  bank africaagence san pedro bp san pedro cote ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "# NLTK \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the English stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Split the text into words and remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Apply the function to your already cleaned text in X_train and X_test\n",
    "X_train['text_no_stopwords'] = X_train['text'].apply(remove_stopwords)\n",
    "X_test['text_no_stopwords'] = X_test['text'].apply(remove_stopwords)\n",
    "\n",
    "# Check the result\n",
    "print(X_train[['text', 'text_no_stopwords']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     text_no_stopwords  \\\n",
      "29   regards mr nelson smithkindly reply private em...   \n",
      "535         able reach oscar supposed send pdb receive   \n",
      "695  huma abedin bim checking pat work jack jake re...   \n",
      "557                        announced monday cant today   \n",
      "836  bank africaagence san pedro bp san pedro cote ...   \n",
      "\n",
      "                               cleaned_text_lemmatized  \n",
      "29   regard mr nelson smithkindly reply private ema...  \n",
      "535         able reach oscar supposed send pdb receive  \n",
      "695  huma abedin bim checking pat work jack jake re...  \n",
      "557                        announced monday cant today  \n",
      "836  bank africaagence san pedro bp san pedro cote ...  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "#import ntlk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download the WordNet lemmatizer data:\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map NLTK POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0]  # Get the first character of the POS tag\n",
    "    tag_dict = {\"J\": wordnet.ADJ,   # Adjective\n",
    "                \"N\": wordnet.NOUN,  # Noun\n",
    "                \"V\": wordnet.VERB,  # Verb\n",
    "                \"R\": wordnet.ADV}   # Adverb\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to NOUN if no match\n",
    "    \n",
    "# Function to lemmatize text with POS tagging\n",
    "def lemmatize_text(text):\n",
    "    # Lemmatize each word in the text using the POS tags\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply the function to your already cleaned text without stopwords\n",
    "X_train['cleaned_text_lemmatized'] = X_train['text_no_stopwords'].apply(lemmatize_text)\n",
    "X_test['cleaned_text_lemmatized'] = X_test['text_no_stopwords'].apply(lemmatize_text)\n",
    "\n",
    "# Check the result\n",
    "print(X_train[['text_no_stopwords', 'cleaned_text_lemmatized']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step Approach:\n",
    "1. Separate spam and ham messages.\n",
    "2. Count word frequencies for both spam and ham.\n",
    "3. Get the top 10 words for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.feature_extraction.text import CountVectorizer\\n\\nvectorizer = CountVectorizer(stop_words=\\'english\\')\\n\\nham_messages = X_train[y_train == 0][\\'cleaned_text_lemmatized\\'].dropna()\\nspam_messages = X_train[y_train == 1][\\'cleaned_text_lemmatized\\'].dropna()\\n\\nham_bow = vectorizer.fit_transform(ham_messages)\\nspam_bow = vectorizer.fit_transform(spam_messages)\\n\\n# Convert to DataFrame for word counts\\nham_word_counts = pd.DataFrame(ham_bow.toarray(), columns=vectorizer.get_feature_names_out()).sum().sort_values(ascending=False)\\nspam_word_counts = pd.DataFrame(spam_bow.toarray(), columns=vectorizer.get_feature_names_out()).sum().sort_values(ascending=False)\\n\\ntop_ham_words = ham_word_counts.head(10)\\ntop_spam_words = spam_word_counts.head(10)\\n\\nprint(\"Top 10 words in ham (Not Spam) messages:\\n\", top_ham_words)\\nprint(\"Top 10 words in spam messages:\\n\", top_spam_words) '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from IRONHACK AI\n",
    "\"\"\" from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "ham_messages = X_train[y_train == 0]['cleaned_text_lemmatized'].dropna()\n",
    "spam_messages = X_train[y_train == 1]['cleaned_text_lemmatized'].dropna()\n",
    "\n",
    "ham_bow = vectorizer.fit_transform(ham_messages)\n",
    "spam_bow = vectorizer.fit_transform(spam_messages)\n",
    "\n",
    "# Convert to DataFrame for word counts\n",
    "ham_word_counts = pd.DataFrame(ham_bow.toarray(), columns=vectorizer.get_feature_names_out()).sum().sort_values(ascending=False)\n",
    "spam_word_counts = pd.DataFrame(spam_bow.toarray(), columns=vectorizer.get_feature_names_out()).sum().sort_values(ascending=False)\n",
    "\n",
    "top_ham_words = ham_word_counts.head(10)\n",
    "top_spam_words = spam_word_counts.head(10)\n",
    "\n",
    "print(\"Top 10 words in ham (Not Spam) messages:\\n\", top_ham_words)\n",
    "print(\"Top 10 words in spam messages:\\n\", top_spam_words) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham (Not Spam) messages:\n",
      " state        92\n",
      "pm           89\n",
      "president    84\n",
      "percent      76\n",
      "secretary    71\n",
      "mr           70\n",
      "time         70\n",
      "work         63\n",
      "said         59\n",
      "american     58\n",
      "dtype: int64\n",
      "Top 10 words in spam messages:\n",
      " money          795\n",
      "account        662\n",
      "bank           606\n",
      "fund           565\n",
      "business       393\n",
      "transaction    347\n",
      "country        337\n",
      "transfer       326\n",
      "company        318\n",
      "million        315\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Remove NaN values from 'cleaned_text_lemmatized' column in X_train\n",
    "X_train['cleaned_text_lemmatized'] = X_train['cleaned_text_lemmatized'].replace('', float('NaN')).dropna()\n",
    "\n",
    "# Separate ham (Not Spam: label = 0) and spam (Spam: label = 1) messages using y_train\n",
    "ham_messages = X_train[y_train == 0]['cleaned_text_lemmatized']  # Not Spam\n",
    "spam_messages = X_train[y_train == 1]['cleaned_text_lemmatized']  # Spam\n",
    "\n",
    "# Ensure ham and spam messages are non-empty after cleaning\n",
    "ham_messages = ham_messages.dropna().str.strip()  # Drop NaN and strip whitespaces\n",
    "spam_messages = spam_messages.dropna().str.strip()  # Drop NaN and strip whitespaces\n",
    "\n",
    "# Filter out empty strings (just in case there are messages that are empty after cleaning)\n",
    "ham_messages = ham_messages[ham_messages != '']\n",
    "spam_messages = spam_messages[spam_messages != '']\n",
    "\n",
    "# Check if there are any remaining valid ham messages\n",
    "if ham_messages.empty:\n",
    "    print(\"No valid ham (Not Spam) messages after cleaning.\")\n",
    "else:\n",
    "    # Initialize the CountVectorizer (Bag of Words)\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    \n",
    "    # Fit the vectorizer to ham messages\n",
    "    ham_bow = vectorizer.fit_transform(ham_messages)\n",
    "    ham_words = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Convert the results to DataFrame and get top words\n",
    "    ham_word_counts = pd.DataFrame(ham_bow.toarray(), columns=ham_words).sum().sort_values(ascending=False)\n",
    "    top_ham_words = ham_word_counts.head(10)\n",
    "    print(\"Top 10 words in ham (Not Spam) messages:\\n\", top_ham_words)\n",
    "\n",
    "# Check if there are any remaining valid spam messages\n",
    "if spam_messages.empty:\n",
    "    print(\"No valid spam messages after cleaning.\")\n",
    "else:\n",
    "    # Fit the vectorizer to spam messages\n",
    "    spam_bow = vectorizer.fit_transform(spam_messages)\n",
    "    spam_words = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Convert the results to DataFrame and get top words\n",
    "    spam_word_counts = pd.DataFrame(spam_bow.toarray(), columns=spam_words).sum().sort_values(ascending=False)\n",
    "    top_spam_words = spam_word_counts.head(10)\n",
    "    print(\"Top 10 words in spam messages:\\n\", top_spam_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_stopwords</th>\n",
       "      <th>cleaned_text_lemmatized</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>regards mr nelson smithkindly reply me on my p...</td>\n",
       "      <td>regards mr nelson smithkindly reply private em...</td>\n",
       "      <td>regard mr nelson smithkindly reply private ema...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>have not been able to reach oscar this am we a...</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>huma abedin bim checking with pat on the will ...</td>\n",
       "      <td>huma abedin bim checking pat work jack jake re...</td>\n",
       "      <td>huma abedin bim checking pat work jack jake re...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>can have it announced here on monday cant today</td>\n",
       "      <td>announced monday cant today</td>\n",
       "      <td>announced monday cant today</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>bank of africaagence san pedro bp san pedro co...</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "29   regards mr nelson smithkindly reply me on my p...   \n",
       "535  have not been able to reach oscar this am we a...   \n",
       "695  huma abedin bim checking with pat on the will ...   \n",
       "557    can have it announced here on monday cant today   \n",
       "836  bank of africaagence san pedro bp san pedro co...   \n",
       "\n",
       "                                     text_no_stopwords  \\\n",
       "29   regards mr nelson smithkindly reply private em...   \n",
       "535         able reach oscar supposed send pdb receive   \n",
       "695  huma abedin bim checking pat work jack jake re...   \n",
       "557                        announced monday cant today   \n",
       "836  bank africaagence san pedro bp san pedro cote ...   \n",
       "\n",
       "                               cleaned_text_lemmatized money_mark  \\\n",
       "29   regard mr nelson smithkindly reply private ema...          1   \n",
       "535         able reach oscar supposed send pdb receive          1   \n",
       "695  huma abedin bim checking pat work jack jake re...          1   \n",
       "557                        announced monday cant today          1   \n",
       "836  bank africaagence san pedro bp san pedro cote ...          1   \n",
       "\n",
       "    suspicious_words  text_len  \n",
       "29                 0        75  \n",
       "535                0        42  \n",
       "695                0        79  \n",
       "557                0        27  \n",
       "836                1      1067  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "X_train['money_mark'] = X_train['cleaned_text_lemmatized'].str.contains(money_simbol_list)*1\n",
    "X_train['suspicious_words'] = X_train['cleaned_text_lemmatized'].str.contains(suspicious_words)*1\n",
    "\n",
    "X_test['money_mark'] = X_test['cleaned_text_lemmatized'].str.contains(money_simbol_list)*1\n",
    "X_test['suspicious_words'] = X_test['cleaned_text_lemmatized'].str.contains(suspicious_words)*1\n",
    "\n",
    "# Fill NaN values with an empty string before calculating text length\n",
    "X_train['cleaned_text_lemmatized'] = X_train['cleaned_text_lemmatized'].fillna('')\n",
    "X_train['text_len'] = X_train['cleaned_text_lemmatized'].apply(lambda x: len(x)) \n",
    "\n",
    "# Fill NaN values with an empty string before calculating text length\n",
    "X_test['cleaned_text_lemmatized'] = X_test['cleaned_text_lemmatized'].fillna('')\n",
    "X_test['text_len'] = X_test['cleaned_text_lemmatized'].apply(lambda x: len(x)) \n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 16740)\n",
      "['aac' 'aaclocated' 'aae' ... 'zumadirector' 'zumae' 'zurich']\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "# Create a Bag of Words model using CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer to the training data (cleaned and lemmatized text) & create Bag of Words(bow)\n",
    "X_train_bow = vectorizer.fit_transform(X_train['cleaned_text_lemmatized'])\n",
    "\n",
    "# Apply the vectorizer to the test data & create Bag of Words(bow)\n",
    "X_test_bow = vectorizer.transform(X_test['cleaned_text_lemmatized'])\n",
    "\n",
    "# View the shape of the transformed data\n",
    "print(X_train_bow.shape)  # (number_of_documents, size_of_vocabulary)\n",
    "\n",
    "# To view the feature names (words in the vocabulary)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Convert to dense array (for illustrative purposes only)\n",
    "X_train_bow_array = X_train_bow.toarray()\n",
    "\n",
    "# Example: Get the first transformed document\n",
    "print(X_train_bow_array[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF vectorized training data: (800, 16740)\n",
      "Shape of TF-IDF vectorized test data: (200, 16740)\n",
      "['aac' 'aaclocated' 'aae' ... 'zumadirector' 'zumae' 'zurich']\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizer on the training data and transform both train and test sets\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['cleaned_text_lemmatized'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['cleaned_text_lemmatized'])\n",
    "\n",
    "# Print the shape of the vectorized dataset\n",
    "print(f\"Shape of TF-IDF vectorized training data: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of TF-IDF vectorized test data: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Optional: To view the feature names (the words that make up the vocabulary)\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.50%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95       125\n",
      "           1       0.88      0.99      0.93        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.95      0.94       200\n",
      "weighted avg       0.95      0.94      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the TF-IDF vectorized training data\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (TF-IDF + N-grams): 94.50%\n",
      "Classification Report (TF-IDF + N-grams):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95       125\n",
      "           1       0.88      0.99      0.93        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.95      0.94       200\n",
      "weighted avg       0.95      0.94      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Combination of N-grams & TF-IDF:\n",
    "\n",
    "# TF-IDF Vectorizer with N-grams\n",
    "tfidf_ngram_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "X_train_tfidf_ngram = tfidf_ngram_vectorizer.fit_transform(X_train['cleaned_text_lemmatized'])\n",
    "X_test_tfidf_ngram = tfidf_ngram_vectorizer.transform(X_test['cleaned_text_lemmatized'])\n",
    "\n",
    "# Train the MultinomialNB Classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf_ngram, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred_tfidf_ngram = classifier.predict(X_test_tfidf_ngram)\n",
    "print(f\"Accuracy (TF-IDF + N-grams): {accuracy_score(y_test, y_pred_tfidf_ngram) * 100:.2f}%\")\n",
    "print(\"Classification Report (TF-IDF + N-grams):\\n\", classification_report(y_test, y_pred_tfidf_ngram))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
