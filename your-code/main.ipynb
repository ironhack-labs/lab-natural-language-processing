{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the sizes of the partitions\n",
    "print(\"Training Features:\", X_train.shape)\n",
    "print(\"Test Features:\", X_test.shape)\n",
    "print(\"Training Labels:\", y_train.shape)\n",
    "print(\"Test Labels:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_html(html_content):\n",
    "\n",
    " # Step 1: Remove inline JavaScript and CSS\n",
    "    html_content = re.sub(r'<script.*?>.*?</script>', '', html_content, flags=re.DOTALL)  # Remove JavaScript\n",
    "    html_content = re.sub(r'<style.*?>.*?</style>', '', html_content, flags=re.DOTALL)    # Remove CSS\n",
    "\n",
    "    # Step 2: Remove HTML comments\n",
    "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "\n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    html_content = re.sub(r'<[^>]+>', '', html_content)\n",
    "\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Step 1: Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Step 2: Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Step 3: Remove all single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Step 4: Remove single characters from the start\n",
    "    text = re.sub(r'^\\s*\\w\\s*', '', text)\n",
    "    \n",
    "    # Step 5: Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Step 6: Remove prefixed 'b'\n",
    "    text = re.sub(r'\\bb\\b', '', text)\n",
    "    \n",
    "    # Step 7: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "     # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "text = \"This is a sample text like the, is, and in it.\"\n",
    "\n",
    "cleaned_text = remove_stopwords(text)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(\"Text without stopwords:\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "     # Initialize the WordNet lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize each word and join back into a single string\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "\n",
    "text = \"The bats are hanging on their feet for best.\"\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_text = lemmatize_text(text)\n",
    "\n",
    "# Display the lemmatized text\n",
    "print(\"Lemmatized Text:\")\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"Win money now! Exclusive offer just for you.\",\n",
    "        \"Meeting at 3 PM tomorrow, don't forget.\",\n",
    "        \"Congratulations! You've been selected for a prize.\",\n",
    "        \"Can we reschedule our meeting to next week?\",\n",
    "        \"Limited time offer, click here to claim your reward!\"\n",
    "    ],\n",
    "    'label': ['spam', 'ham', 'spam', 'ham', 'spam']\n",
    "})\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_messages = data[data['label'] == 'ham']['text']\n",
    "spam_messages = data[data['label'] == 'spam']['text']\n",
    "\n",
    "# Define a function to get the top 10 words\n",
    "def get_top_words(messages, top_n=10):\n",
    "    \"\"\"\n",
    "    Get the top N words from the messages.\n",
    "    \"\"\"\n",
    "    # Initialize CountVectorizer with stopwords removed\n",
    "    vectorizer = CountVectorizer(stop_words=stopwords.words('english'))\n",
    "    \n",
    "    # Fit and transform the text data\n",
    "    word_counts = vectorizer.fit_transform(messages)\n",
    "    \n",
    "    # Sum the counts for each word\n",
    "    word_freq = word_counts.sum(axis=0).A1\n",
    "    \n",
    "    # Create a DataFrame for word frequencies\n",
    "    word_freq_df = pd.DataFrame({'word': vectorizer.get_feature_names_out(), 'count': word_freq})\n",
    "    \n",
    "    # Sort by frequency and return the top words\n",
    "    return word_freq_df.sort_values(by='count', ascending=False).head(top_n)\n",
    "\n",
    "# Get the top 10 words for ham and spam messages\n",
    "top_ham_words = get_top_words(ham_messages, top_n=10)\n",
    "top_spam_words = get_top_words(spam_messages, top_n=10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Words in Ham Messages:\")\n",
    "print(top_ham_words)\n",
    "\n",
    "print(\"\\nTop 10 Words in Spam Messages:\")\n",
    "print(top_spam_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (update the file path as needed)\n",
    "data_train = pd.DataFrame({\n",
    "    'preprocessed_text': [\n",
    "        \"win money now exclusive offer\",\n",
    "        \"meeting scheduled for tomorrow at noon\",\n",
    "        \"free account upgrade available\",\n",
    "        \"deposit money to secure your funds\",\n",
    "        \"bank transaction alert from your account\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "data_val = pd.DataFrame({\n",
    "    'preprocessed_text': [\n",
    "        \"click here to claim your prize\",\n",
    "        \"meeting rescheduled to next week\",\n",
    "        \"transfer money immediately for rewards\",\n",
    "        \"password reset required for security\",\n",
    "        \"free trial offer for premium access\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    \"Win money now! Exclusive offer for you.\",\n",
    "    \"Meeting scheduled tomorrow. Don’t forget!\",\n",
    "    \"Congratulations! You’ve won a free prize.\",\n",
    "    \"Reschedule meeting to next week.\",\n",
    "    \"Limited time offer: claim your reward.\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus to create the Bag of Words representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display the vocabulary\n",
    "print(\"Vocabulary (Features):\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix and display\n",
    "print(\"\\nBag of Words Matrix (Word Counts):\")\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example corpus (replace with your dataset)\n",
    "corpus = [\n",
    "    \"Win money now! Exclusive offer for you.\",\n",
    "    \"Meeting scheduled tomorrow. Don’t forget!\",\n",
    "    \"Congratulations! You’ve won a free prize.\",\n",
    "    \"Reschedule meeting to next week.\",\n",
    "    \"Limited time offer: claim your reward.\"\n",
    "]\n",
    "\n",
    "# Step 1: Load the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the dataset\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 3: Print the shape of the vectorized dataset\n",
    "print(\"Shape of the vectorized dataset:\", tfidf_matrix.shape)\n",
    "\n",
    "# Optional: Display feature names and a sample of the matrix\n",
    "print(\"\\nFeature Names (Vocabulary):\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix (Sparse Representation):\")\n",
    "print(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Example dataset (replace with your dataset)\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"Win money now! Exclusive offer for you.\",\n",
    "        \"Meeting scheduled tomorrow. Don’t forget!\",\n",
    "        \"Congratulations! You’ve won a free prize.\",\n",
    "        \"Reschedule meeting to next week.\",\n",
    "        \"Limited time offer: claim your reward.\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1] \n",
    "})\n",
    "\n",
    "# Step 1: Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Step 3: Train a Classifier (Logistic Regression)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Step 4: Make Predictions\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already done\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv (\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the input text.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    text = ' '.join(word for word in words if word not in stop_words)\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    text = ' '.join(stemmer.stem(word) for word in text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Define feature representation methods\n",
    "vectorizers = {\n",
    "    \"Bag of Words\": CountVectorizer(),\n",
    "    \"TF-IDF\": TfidfVectorizer()\n",
    "}\n",
    "\n",
    "# Split the dataset\n",
    "X = data['processed_text']\n",
    "y = data['label']  # Adjust column name as necessary\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate different feature representations\n",
    "best_score = 0\n",
    "best_vectorizer = None\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    # Fit and transform the training data\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Train MultinomialNB with default parameters\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nFeature Representation: {name}\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Track the best feature representation\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_vectorizer = name\n",
    "\n",
    "# Output the best feature representation\n",
    "print(f\"\\nBest Feature Representation: {best_vectorizer} with Accuracy: {best_score}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
