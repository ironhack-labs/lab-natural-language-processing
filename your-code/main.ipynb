{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s7/8db2fs716sj67tmf9cc2npbm0000gn/T/ipykernel_92795/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (800, 2)\n",
      "Validation set: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42, stratify=data['label'])\n",
    "print(f\"Training set: {data_train.shape}\")\n",
    "print(f\"Validation set: {data_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                           text_nojs  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                          text_nocss  \n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...  \n",
      "1                                           Will do.  \n",
      "2  Nora--Cheryl has emailed dozens of memos about...  \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "# Remove JS\n",
    "def remove_inline_js(text):\n",
    "\t# Remove <script>...</script> blocks (including multiline)\n",
    "\treturn re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "data['text_nojs'] = data['text'].apply(remove_inline_js)\n",
    "data_train['text_nojs'] = data_train['text'].apply(remove_inline_js)\n",
    "data_val['text_nojs'] = data_val['text'].apply(remove_inline_js)\n",
    "\n",
    "# Remove CSS\n",
    "def remove_inline_css(text):\n",
    "\t# Remove <style>...</style> blocks (including multiline)\n",
    "\treturn re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "data['text_nocss'] = data['text_nojs'].apply(remove_inline_css)\n",
    "data_train['text_nocss'] = data_train['text_nojs'].apply(remove_inline_css)\n",
    "data_val['text_nocss'] = data_val['text_nojs'].apply(remove_inline_css)\n",
    "\n",
    "# Print the first few rows to verify the result\n",
    "print(data[['text', 'text_nojs', 'text_nocss']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   preprocessed_text\n",
      "0  dear sir strictly private business proposal am...\n",
      "1                                            will do\n",
      "2  noracheryl has emailed dozens of memos about h...\n",
      "3  dear sirfmadamc know that this proposal might ...\n",
      "4                                                fyi\n",
      "                                     preprocessed_text\n",
      "442  dearc good day hope finecdear am writting this...\n",
      "962  from mr henry kaborethe chief auditor incharge...\n",
      "971                                            will do\n",
      "190  from the desk of dradamu ismalerauditing and a...\n",
      "551  dear friend my name is loi cestradathe wife of...\n",
      "                                     preprocessed_text\n",
      "222  strictly personalpermit me to introduce myself...\n",
      "824  dear nancyi very much want to meet and weve be...\n",
      "505  request for business partnershipi am patrick a...\n",
      "165  soneri bank limitedsilver jubilee centercbritt...\n",
      "832  dear partneri am fund manager with fidelity in...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Your code\n",
    "# Remove all special characters\n",
    "def remove_special_characters(text):\n",
    "\treturn re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "data['text_nospecial'] = data['text_nocss'].apply(remove_special_characters)\n",
    "data_train['text_nospecial'] = data_train['text_nocss'].apply(remove_special_characters)\n",
    "data_val['text_nospecial'] = data_val['text_nocss'].apply(remove_special_characters)\n",
    "\n",
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "\treturn re.sub(r'\\d+', '', text)\n",
    "\n",
    "data['text_nonum'] = data['text_nospecial'].apply(remove_numbers)\n",
    "data_train['text_nonum'] = data_train['text_nospecial'].apply(remove_numbers)\n",
    "data_val['text_nonum'] = data_val['text_nospecial'].apply(remove_numbers)\n",
    "\n",
    "# Remove all single characters\n",
    "def remove_single_characters(text):\n",
    "\treturn re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "data['text_nosingle'] = data['text_nonum'].apply(remove_single_characters)\n",
    "data_train['text_nosingle'] = data_train['text_nonum'].apply(remove_single_characters)\n",
    "data_val['text_nosingle'] = data_val['text_nonum'].apply(remove_single_characters)\n",
    "\n",
    "# Remove single characters from the start\n",
    "def remove_single_characters_start(text):\n",
    "\treturn re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "\n",
    "data['text_nosingle_start'] = data['text_nosingle'].apply(remove_single_characters_start)\n",
    "data_train['text_nosingle_start'] = data_train['text_nosingle'].apply(remove_single_characters_start)\n",
    "data_val['text_nosingle_start'] = data_val['text_nosingle'].apply(remove_single_characters_start)\n",
    "\n",
    "# Substitute multiple spaces with single space\n",
    "def substitute_multiple_spaces(text):\n",
    "\treturn re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "data['text_nospaces'] = data['text_nosingle_start'].apply(substitute_multiple_spaces)\n",
    "data_train['text_nospaces'] = data_train['text_nosingle_start'].apply(substitute_multiple_spaces)\n",
    "data_val['text_nospaces'] = data_val['text_nosingle_start'].apply(substitute_multiple_spaces)\n",
    "\n",
    "# Remove prefixed 'b'\n",
    "def remove_prefixed_b(text):\n",
    "\treturn re.sub(r'^b\\s+', '', text)\n",
    "\n",
    "data['text_noprefixb'] = data['text_nospaces'].apply(remove_prefixed_b)\n",
    "data_train['text_noprefixb'] = data_train['text_nospaces'].apply(remove_prefixed_b)\n",
    "data_val['text_noprefixb'] = data_val['text_nospaces'].apply(remove_prefixed_b)\n",
    "\n",
    "# Convert to Lowercase\n",
    "data['preprocessed_text'] = data['text_noprefixb'].str.lower()\n",
    "data_train['preprocessed_text'] = data_train['text_noprefixb'].str.lower()\n",
    "data_val['preprocessed_text'] = data_val['text_noprefixb'].str.lower()\n",
    "\n",
    "print(data[['preprocessed_text']].head())\n",
    "print(data_train[['preprocessed_text']].head())\n",
    "print(data_val[['preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   preprocessed_text  \\\n",
      "0  dear sir strictly private business proposal am...   \n",
      "1                                            will do   \n",
      "2  noracheryl has emailed dozens of memos about h...   \n",
      "3  dear sirfmadamc know that this proposal might ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        no_stopwords  \n",
      "0  dear sir strictly private business proposal mi...  \n",
      "1                                                     \n",
      "2  noracheryl emailed dozens memos haiti weekend ...  \n",
      "3  dear sirfmadamc know proposal might surprise e...  \n",
      "4                                                fyi  \n",
      "                                     preprocessed_text  \\\n",
      "442  dearc good day hope finecdear am writting this...   \n",
      "962  from mr henry kaborethe chief auditor incharge...   \n",
      "971                                            will do   \n",
      "190  from the desk of dradamu ismalerauditing and a...   \n",
      "551  dear friend my name is loi cestradathe wife of...   \n",
      "\n",
      "                                          no_stopwords  \n",
      "442  dearc good day hope finecdear writting mail du...  \n",
      "962  mr henry kaborethe chief auditor inchargeforei...  \n",
      "971                                                     \n",
      "190  desk dradamu ismalerauditing accounting manage...  \n",
      "551  dear friend name loi cestradathe wife mr josep...  \n",
      "                                     preprocessed_text  \\\n",
      "222  strictly personalpermit me to introduce myself...   \n",
      "824  dear nancyi very much want to meet and weve be...   \n",
      "505  request for business partnershipi am patrick a...   \n",
      "165  soneri bank limitedsilver jubilee centercbritt...   \n",
      "832  dear partneri am fund manager with fidelity in...   \n",
      "\n",
      "                                          no_stopwords  \n",
      "222  strictly personalpermit introduce mr joyce zum...  \n",
      "824  dear nancyi much want meet weve trying pin sch...  \n",
      "505  request business partnershipi patrick atike di...  \n",
      "165  soneri bank limitedsilver jubilee centercbritt...  \n",
      "832  dear partneri fund manager fidelity investment...  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\treturn ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "data['no_stopwords'] = data['preprocessed_text'].apply(remove_stopwords)\n",
    "data_train['no_stopwords'] = data_train['preprocessed_text'].apply(remove_stopwords)\n",
    "data_val['no_stopwords'] = data_val['preprocessed_text'].apply(remove_stopwords)\n",
    "\n",
    "print(data[['preprocessed_text', 'no_stopwords']].head())\n",
    "print(data_train[['preprocessed_text', 'no_stopwords']].head())\n",
    "print(data_val[['preprocessed_text', 'no_stopwords']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        no_stopwords  \\\n",
      "0  dear sir strictly private business proposal mi...   \n",
      "1                                                      \n",
      "2  noracheryl emailed dozens memos haiti weekend ...   \n",
      "3  dear sirfmadamc know proposal might surprise e...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [dear, sir, strictly, private, business, propo...   \n",
      "1                                                 []   \n",
      "2  [noracheryl, emailed, dozens, memos, haiti, we...   \n",
      "3  [dear, sirfmadamc, know, proposal, might, surp...   \n",
      "4                                              [fyi]   \n",
      "\n",
      "                                          lemmatized  \n",
      "0  [dear, sir, strict, privat, busi, propos, mike...  \n",
      "1                                                 []  \n",
      "2  [noracheryl, email, dozen, memo, haiti, weeken...  \n",
      "3  [dear, sirfmadamc, know, propos, might, surpri...  \n",
      "4                                              [fyi]  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Ensure 'no_stopwords' column exists before proceeding\n",
    "if 'no_stopwords' in data.columns:\n",
    "\t# Break sentences into words\n",
    "\tdata['tokens'] = data['no_stopwords'].apply(lambda x: x.split())\n",
    "\tdata_train['tokens'] = data_train['no_stopwords'].apply(lambda x: x.split())\n",
    "\tdata_val['tokens'] = data_val['no_stopwords'].apply(lambda x: x.split())\n",
    "\n",
    "\t# Use lemmatization (actually stemming here)\n",
    "\tdata['lemmatized'] = data['tokens'].apply(lambda tokens: [snowball.stem(token) for token in tokens])\n",
    "\tdata_train['lemmatized'] = data_train['tokens'].apply(lambda tokens: [snowball.stem(token) for token in tokens])\n",
    "\tdata_val['lemmatized'] = data_val['tokens'].apply(lambda tokens: [snowball.stem(token) for token in tokens])\n",
    "\n",
    "\t# Print the result\n",
    "\tprint(data[['no_stopwords', 'tokens', 'lemmatized']].head())\n",
    "else:\n",
    "\tprint(\"Column 'no_stopwords' not found. Please run the cell that creates it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in HAM messages: [('work', 94), ('call', 92), ('would', 91), ('presid', 91), ('us', 88), ('obama', 79), ('percent', 76), ('state', 72), ('mr', 65), ('time', 63)]\n",
      "Top 10 words in SPAM messages: [('money', 723), ('bank', 648), ('account', 633), ('fund', 553), ('us', 440), ('transfer', 404), ('foreign', 388), ('busi', 383), ('contact', 357), ('assist', 335)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Your code\n",
    "# Flatten lemmatized tokens for ham and spam\n",
    "ham_words = [word for tokens in data_train[data_train['label'] == 0]['lemmatized'] for word in tokens]\n",
    "spam_words = [word for tokens in data_train[data_train['label'] == 1]['lemmatized'] for word in tokens]\n",
    "\n",
    "# Count and get top 10\n",
    "top_ham = Counter(ham_words).most_common(10)\n",
    "top_spam = Counter(spam_words).most_common(10)\n",
    "\n",
    "print(\"Top 10 words in HAM messages:\", top_ham)\n",
    "print(\"Top 10 words in SPAM messages:\", top_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_nojs</th>\n",
       "      <th>text_nocss</th>\n",
       "      <th>text_nospecial</th>\n",
       "      <th>text_nonum</th>\n",
       "      <th>text_nosingle</th>\n",
       "      <th>text_nosingle_start</th>\n",
       "      <th>text_nospaces</th>\n",
       "      <th>text_noprefixb</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>DearC Good day hope fineCdear am writting this...</td>\n",
       "      <td>DearC Good day hope fineCdear am writting this...</td>\n",
       "      <td>DearC Good day hope fineCdear am writting this...</td>\n",
       "      <td>DearC Good day hope fineCdear am writting this...</td>\n",
       "      <td>DearC Good day hope fineCdear am writting this...</td>\n",
       "      <td>DearC Good day hope fineCdear am writting this...</td>\n",
       "      <td>dearc good day hope finecdear am writting this...</td>\n",
       "      <td>dearc good day hope finecdear writting mail du...</td>\n",
       "      <td>[dearc, good, day, hope, finecdear, writting, ...</td>\n",
       "      <td>[dearc, good, day, hope, finecdear, writ, mail...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>1</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>from mr henry kaborethe chief auditor incharge...</td>\n",
       "      <td>mr henry kaborethe chief auditor inchargeforei...</td>\n",
       "      <td>[mr, henry, kaborethe, chief, auditor, incharg...</td>\n",
       "      <td>[mr, henri, kaboreth, chief, auditor, incharge...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>Will do.</td>\n",
       "      <td>Will do.</td>\n",
       "      <td>Will do</td>\n",
       "      <td>Will do</td>\n",
       "      <td>Will do</td>\n",
       "      <td>Will do</td>\n",
       "      <td>Will do</td>\n",
       "      <td>Will do</td>\n",
       "      <td>will do</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>1</td>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>FROM THE DESK OF DRADAMU  ISMALERAUDITING AND ...</td>\n",
       "      <td>FROM THE DESK OF DRADAMU  ISMALERAUDITING AND ...</td>\n",
       "      <td>FROM THE DESK OF DRADAMU  ISMALERAUDITING AND ...</td>\n",
       "      <td>FROM THE DESK OF DRADAMU  ISMALERAUDITING AND ...</td>\n",
       "      <td>FROM THE DESK OF DRADAMU ISMALERAUDITING AND A...</td>\n",
       "      <td>FROM THE DESK OF DRADAMU ISMALERAUDITING AND A...</td>\n",
       "      <td>from the desk of dradamu ismalerauditing and a...</td>\n",
       "      <td>desk dradamu ismalerauditing accounting manage...</td>\n",
       "      <td>[desk, dradamu, ismalerauditing, accounting, m...</td>\n",
       "      <td>[desk, dradamu, ismaleraudit, account, manager...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>Dear Friend My name is LOI CESTRADAThe wife of...</td>\n",
       "      <td>Dear Friend My name is LOI CESTRADAThe wife of...</td>\n",
       "      <td>Dear Friend My name is LOI CESTRADAThe wife of...</td>\n",
       "      <td>Dear Friend My name is LOI CESTRADAThe wife of...</td>\n",
       "      <td>Dear Friend My name is LOI CESTRADAThe wife of...</td>\n",
       "      <td>Dear Friend My name is LOI CESTRADAThe wife of...</td>\n",
       "      <td>dear friend my name is loi cestradathe wife of...</td>\n",
       "      <td>dear friend name loi cestradathe wife mr josep...</td>\n",
       "      <td>[dear, friend, name, loi, cestradathe, wife, m...</td>\n",
       "      <td>[dear, friend, name, loi, cestradath, wife, mr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "442  Dear=2C Good day hope fine=2Cdear am writting ...      1   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...      1   \n",
       "971                                           Will do.      0   \n",
       "190  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...      1   \n",
       "551  Dear Friend, My name is LOI C.ESTRADA,The wife...      1   \n",
       "\n",
       "                                             text_nojs  \\\n",
       "442  Dear=2C Good day hope fine=2Cdear am writting ...   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "971                                           Will do.   \n",
       "190  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...   \n",
       "551  Dear Friend, My name is LOI C.ESTRADA,The wife...   \n",
       "\n",
       "                                            text_nocss  \\\n",
       "442  Dear=2C Good day hope fine=2Cdear am writting ...   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "971                                           Will do.   \n",
       "190  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...   \n",
       "551  Dear Friend, My name is LOI C.ESTRADA,The wife...   \n",
       "\n",
       "                                        text_nospecial  \\\n",
       "442  DearC Good day hope fineCdear am writting this...   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "971                                            Will do   \n",
       "190  FROM THE DESK OF DRADAMU  ISMALERAUDITING AND ...   \n",
       "551  Dear Friend My name is LOI CESTRADAThe wife of...   \n",
       "\n",
       "                                            text_nonum  \\\n",
       "442  DearC Good day hope fineCdear am writting this...   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "971                                            Will do   \n",
       "190  FROM THE DESK OF DRADAMU  ISMALERAUDITING AND ...   \n",
       "551  Dear Friend My name is LOI CESTRADAThe wife of...   \n",
       "\n",
       "                                         text_nosingle  \\\n",
       "442  DearC Good day hope fineCdear am writting this...   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "971                                            Will do   \n",
       "190  FROM THE DESK OF DRADAMU  ISMALERAUDITING AND ...   \n",
       "551  Dear Friend My name is LOI CESTRADAThe wife of...   \n",
       "\n",
       "                                   text_nosingle_start  \\\n",
       "442  DearC Good day hope fineCdear am writting this...   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "971                                            Will do   \n",
       "190  FROM THE DESK OF DRADAMU  ISMALERAUDITING AND ...   \n",
       "551  Dear Friend My name is LOI CESTRADAThe wife of...   \n",
       "\n",
       "                                         text_nospaces  \\\n",
       "442  DearC Good day hope fineCdear am writting this...   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "971                                            Will do   \n",
       "190  FROM THE DESK OF DRADAMU ISMALERAUDITING AND A...   \n",
       "551  Dear Friend My name is LOI CESTRADAThe wife of...   \n",
       "\n",
       "                                        text_noprefixb  \\\n",
       "442  DearC Good day hope fineCdear am writting this...   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "971                                            Will do   \n",
       "190  FROM THE DESK OF DRADAMU ISMALERAUDITING AND A...   \n",
       "551  Dear Friend My name is LOI CESTRADAThe wife of...   \n",
       "\n",
       "                                     preprocessed_text  \\\n",
       "442  dearc good day hope finecdear am writting this...   \n",
       "962  from mr henry kaborethe chief auditor incharge...   \n",
       "971                                            will do   \n",
       "190  from the desk of dradamu ismalerauditing and a...   \n",
       "551  dear friend my name is loi cestradathe wife of...   \n",
       "\n",
       "                                          no_stopwords  \\\n",
       "442  dearc good day hope finecdear writting mail du...   \n",
       "962  mr henry kaborethe chief auditor inchargeforei...   \n",
       "971                                                      \n",
       "190  desk dradamu ismalerauditing accounting manage...   \n",
       "551  dear friend name loi cestradathe wife mr josep...   \n",
       "\n",
       "                                                tokens  \\\n",
       "442  [dearc, good, day, hope, finecdear, writting, ...   \n",
       "962  [mr, henry, kaborethe, chief, auditor, incharg...   \n",
       "971                                                 []   \n",
       "190  [desk, dradamu, ismalerauditing, accounting, m...   \n",
       "551  [dear, friend, name, loi, cestradathe, wife, m...   \n",
       "\n",
       "                                            lemmatized  money_mark  \\\n",
       "442  [dearc, good, day, hope, finecdear, writ, mail...           1   \n",
       "962  [mr, henri, kaboreth, chief, auditor, incharge...           1   \n",
       "971                                                 []           1   \n",
       "190  [desk, dradamu, ismaleraudit, account, manager...           1   \n",
       "551  [dear, friend, name, loi, cestradath, wife, mr...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "442                 1      1538  \n",
       "962                 1      2910  \n",
       "971                 0         7  \n",
       "190                 1       497  \n",
       "551                 1      2022  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Bag of Words (CountVectorizer) X_train: (800, 18482)\n",
      "Shape of Bag of Words (CountVectorizer) X_val: (200, 18482)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Your code\n",
    "# Use the preprocessed text (without stopwords) for vectorization\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(data_train['no_stopwords'])\n",
    "X_val_bow = vectorizer.transform(data_val['no_stopwords'])\n",
    "\n",
    "print(\"Shape of Bag of Words (CountVectorizer) X_train:\", X_train_bow.shape)\n",
    "print(\"Shape of Bag of Words (CountVectorizer) X_val:\", X_val_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF X_train: (800, 18482)\n",
      "Shape of TF-IDF X_val: (200, 18482)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Load vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorize all dataset\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['no_stopwords'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val['no_stopwords'])\n",
    "\n",
    "# Print shape\n",
    "print(\"Shape of TF-IDF X_train:\", X_train_tfidf.shape)\n",
    "print(\"Shape of TF-IDF X_val:\", X_val_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Your code\n",
    "# Train MultinomialNB classifier using TF-IDF features (as an example of \"best\" feature representation)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, data_train['label'])\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = clf.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(data_val['label'], y_pred))\n",
    "print(classification_report(data_val['label'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (TF-IDF + extra features): 0.755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.56      0.72       112\n",
      "           1       0.64      1.00      0.78        88\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.82      0.78      0.75       200\n",
      "weighted avg       0.84      0.76      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Your code\n",
    "# Try combining TF-IDF features with extra features (money_mark, suspicious_words, text_len)\n",
    "\n",
    "# Select extra features\n",
    "extra_features_train = data_train[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "extra_features_val = data_val[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "\n",
    "# Combine TF-IDF with extra features\n",
    "X_train_combined = hstack([X_train_tfidf, extra_features_train])\n",
    "X_val_combined = hstack([X_val_tfidf, extra_features_val])\n",
    "\n",
    "# Train MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_combined, data_train['label'])\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_val_combined)\n",
    "print(\"Accuracy (TF-IDF + extra features):\", accuracy_score(data_val['label'], y_pred))\n",
    "print(classification_report(data_val['label'], y_pred))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
