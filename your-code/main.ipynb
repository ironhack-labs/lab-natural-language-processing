{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Latif-Calderón\\AppData\\Local\\Temp\\ipykernel_6360\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>So what's the latest? It sounds contradictory ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>TRANSFER OF 36,759,000.00 MILLION POUNDS TO YO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Barb I will call to explain. Are you back in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Yang on travelNot free tonite.May work tomorrow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>sbwhoeopSunday February 21 2010 7:42 PMHShaunH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                             Will do.      0\n",
       "2    Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3    Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                  fyi      0\n",
       "..                                                 ...    ...\n",
       "995  So what's the latest? It sounds contradictory ...      0\n",
       "996  TRANSFER OF 36,759,000.00 MILLION POUNDS TO YO...      1\n",
       "997  Barb I will call to explain. Are you back in t...      0\n",
       "998    Yang on travelNot free tonite.May work tomorrow      0\n",
       "999  sbwhoeopSunday February 21 2010 7:42 PMHShaunH...      0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_test.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usiness is for the fact that the deceased man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They are happy to adjust to the afternoon. I a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lael Brainard was confirmed 78-19 this afterno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H &lt;hrod17@clintonemail.com&gt;Friday March 26 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n;\"&gt; Dear Good Friend,&lt;br&gt;&lt;br&gt;&lt;br&gt;I am happy t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Dear Friend,Do accept my sincere apologies if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>FROM THE DESK OF:MR.WANG QINHANG SENG BANK LTD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>He can speak at 7pm. He said that it does not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Mary Landrieu asked me to attend a dinner abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Oscar should have printed gates shangrila conf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    usiness is for the fact that the deceased man ...\n",
       "1    They are happy to adjust to the afternoon. I a...\n",
       "2    Lael Brainard was confirmed 78-19 this afterno...\n",
       "3    H <hrod17@clintonemail.com>Friday March 26 201...\n",
       "4    n;\"> Dear Good Friend,<br><br><br>I am happy t...\n",
       "..                                                 ...\n",
       "995  Dear Friend,Do accept my sincere apologies if ...\n",
       "996  FROM THE DESK OF:MR.WANG QINHANG SENG BANK LTD...\n",
       "997  He can speak at 7pm. He said that it does not ...\n",
       "998  Mary Landrieu asked me to attend a dinner abou...\n",
       "999  Oscar should have printed gates shangrila conf...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data: (1000, 2)\n",
      "Training features shape: (800,)\n",
      "Training labels shape: (800,)\n",
      "Test features shape: (200,)\n",
      "Test labels shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"../data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the dataset size for faster development\n",
    "data = data.head(1000)\n",
    "print(f\"Shape of the data: {data.shape}\")\n",
    "\n",
    "# Fill in missing values with an empty string\n",
    "data.fillna(\"\", inplace=True)\n",
    "\n",
    "# Assume your dataset has a 'label' column that you want to predict,\n",
    "# and the rest of the columns are features\n",
    "target_column = 'label'  # Replace with your actual target column name\n",
    "features = data.drop(columns=[target_column])\n",
    "target = data[target_column]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'], \n",
    "    data ['label'], \n",
    "    #features, \n",
    "    #target, \n",
    "    test_size=0.2, \n",
    "    random_state=42  # 20% for testing\n",
    ")\n",
    "\n",
    "# Print the shape of each partition to confirm the split\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  dear sir strict privat busi propos mike chukwu...  \n",
      "1                                                     \n",
      "2  noracheryl email dozen memo haiti weekend plea...  \n",
      "3  dear sir2fmadam2c know propos might surpris em...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Define the HTML cleaning function\n",
    "def clean_html(content):\n",
    "    # Remove inline JavaScript/CSS using regex\n",
    "    content = re.sub(r'<(script|style).*?>.*?(</\\1>)', '', content, flags=re.S)\n",
    "\n",
    "    # Remove HTML comments\n",
    "    content = re.sub(r'<!--.*?-->', '', content, flags=re.S)\n",
    "\n",
    "    # Remove remaining HTML tags\n",
    "    content = re.sub(r'<.*?>', '', content)\n",
    "\n",
    "    # Strip extra spaces, newlines, and tabs\n",
    "    content = re.sub(r'\\s+', ' ', content).strip()\n",
    "\n",
    "    # Remove punctuation\n",
    "    content = content.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    content = content.lower()\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = content.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Apply stemming\n",
    "    stemmed_words = [snowball.stem(word) for word in filtered_words]\n",
    "\n",
    "    # Join the words back into a cleaned string\n",
    "    cleaned_content = ' '.join(stemmed_words)\n",
    "\n",
    "    return cleaned_content\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data_path = '../data/kg_train.csv' # adjust the path to where your CSV file is\n",
    "data = pd.read_csv(data_path, encoding='latin-1')  # Adjust encoding if necessary\n",
    "\n",
    "# Assuming the text data is in a column named 'text', replace 'text' with the actual column name\n",
    "data['cleaned_text'] = data['text'].apply(clean_html)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(data[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'html_content' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 63\u001b[0m\n\u001b[0;32m     59\u001b[0m     cleaned_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stemmed_words)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_content\n\u001b[1;32m---> 63\u001b[0m cleaned_text \u001b[38;5;241m=\u001b[39m \u001b[43mclean_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_text)\n",
      "Cell \u001b[1;32mIn[85], line 42\u001b[0m, in \u001b[0;36mclean_html\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m     39\u001b[0m content \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<.*?>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, content)\n\u001b[0;32m     41\u001b[0m  \u001b[38;5;66;03m# Strip extra spaces, newlines, and tabs\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m html_content \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mhtml_content\u001b[49m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Optionally, you may want to remove punctuation or transform the text \u001b[39;00m\n\u001b[0;32m     45\u001b[0m content \u001b[38;5;241m=\u001b[39m content\u001b[38;5;241m.\u001b[39mtranslate(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, string\u001b[38;5;241m.\u001b[39mpunctuation))  \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'html_content' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Example HTML content\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <style>\n",
    "    p {color: red;}\n",
    "    body {background-color: lightblue;}\n",
    "  </style>\n",
    "  <script>\n",
    "    console.log(\"Hello, World!\");\n",
    "  </script>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>This is a heading</h1>\n",
    "  <p>This is a paragraph.</p>\n",
    "  <!-- This is a comment -->\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "def clean_html(content):\n",
    "    # Remove inline JavaScript/CSS using regex\n",
    "    content = re.sub(r'<(script|style).*?>.*?(</\\1>)', '', content, flags=re.S)\n",
    "\n",
    "    # Remove HTML comments\n",
    "    content = re.sub(r'<!--.*?-->', '', content, flags=re.S)\n",
    "\n",
    "    # Remove remaining HTML tags\n",
    "    content = re.sub(r'<.*?>', '', content)\n",
    "\n",
    "     # Strip extra spaces, newlines, and tabs\n",
    "    html_content = re.sub(r'\\s+', ' ', html_content).strip()\n",
    "\n",
    "    # Optionally, you may want to remove punctuation or transform the text \n",
    "    content = content.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "\n",
    "    # Convert to lowercase\n",
    "    content = content.lower()\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = content.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Apply stemming\n",
    "    stemmed_words = [snowball.stem(word) for word in filtered_words]\n",
    "\n",
    "    # Join the words back into a cleaned string\n",
    "    cleaned_content = ' '.join(stemmed_words)\n",
    "\n",
    "    return cleaned_content\n",
    "\n",
    "cleaned_text = clean_html(html_content)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a heading This is a paragraph.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(html_content):\n",
    "    # Remove inline JavaScript/CSS:\n",
    "    # Matches anything inside `<script>` or `<style>`, including new lines\n",
    "    html_content = re.sub(r'<(script|style).*?>.*?</\\1>', '', html_content, flags=re.DOTALL)\n",
    "\n",
    "    # Remove HTML comments:\n",
    "    # Matches anything inside `<!-- ... -->`\n",
    "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "\n",
    "    # Remove remaining HTML tags:\n",
    "    # Matches `<tag>` while allowing handling of mismatched HTML like `<img src='...'/>`\n",
    "    html_content = re.sub(r'<.*?>', '', html_content)\n",
    "\n",
    "    # Strip extra spaces, newlines, and tabs\n",
    "    html_content = re.sub(r'\\s+', ' ', html_content).strip()\n",
    "\n",
    "    return html_content\n",
    "\n",
    "# Example HTML content\n",
    "html_data = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <style>\n",
    "    p {color: red;}\n",
    "    body {background-color: lightblue;}\n",
    "  </style>\n",
    "  <script type=\"text/javascript\">\n",
    "    console.log(\"Hello, World!\");\n",
    "  </script>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>This is a heading</h1>\n",
    "  <p>This is a paragraph.</p>\n",
    "  <!-- This is a comment -->\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Clean the HTML\n",
    "cleaned_text = clean_html(html_data)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  dear sir strictly private business proposal am...  \n",
      "1                                            will do  \n",
      "2  noracheryl has emailed dozens of memos about h...  \n",
      "3  dear sirfmadamc know that this proposal might ...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Remove numbers specifically, if any remain\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Optionally remove prefixed 'b' if you are dealing with byte strings\n",
    "    text = text.replace(\"b'\", '')  # Adjust according to actual data format\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Adjust the path to where your CSV file is located\n",
    "data_path = '../data/kg_train.csv'\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, encoding='latin-1')\n",
    "\n",
    "# Assuming the text data is in a column named 'text' (adjust this if necessary)\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is sample text with numbers and special characters lets clean this up inline styles bboldb single characters like numbers such as etc prefix such as bsample prefixed binary string\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Remove numbers specifically, if any remain\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Optionally remove prefixed 'b' if you are dealing with byte strings\n",
    "    text = re.sub(r\"\\bb'\", '', text)  # This depends on the actual input format\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Sample text for demonstration\n",
    "sample_text = \"\"\"\n",
    "This is a sample text with numbers 123 and special characters! Let's clean this up:\n",
    "Inline styles, <b>bold</b>, single characters like a, numbers such as 7, etc.\n",
    "Prefix such as b'Sample prefixed binary string.'\n",
    "\"\"\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  dear sir strictly private business proposal am...  \n",
      "1                                            will do  \n",
      "2  noracheryl has emailed dozens of memos about h...  \n",
      "3  dear sirfmadamc know that this proposal might ...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Remove numbers specifically, if any remain\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Optionally remove prefixed 'b' if you are dealing with byte strings\n",
    "    text = text.replace(\"b'\", '')  # Adjust according to actual data format\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Adjust the path to where your CSV file is located\n",
    "data_path = '../data/kg_train.csv'\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, encoding='latin-1')\n",
    "\n",
    "# Assuming the text data is in a column named 'text' (adjust this if necessary)\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "clean text: sample text numbers special characters lets clean inline styles bboldb single characters like numbers etc prefix bsample prefixed binary string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Latif-\n",
      "[nltk_data]     Calderón\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Remove numbers specifically, if any remain\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    filtered_text = ' '.join([word for word in tokens if word not in stop_words])\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "# Sample text for demonstration\n",
    "sample_text = \"\"\"\n",
    "This is a sample text with numbers 123 and special characters! Let's clean this up:\n",
    "Inline styles, <b>bold</b>, single characters like a, numbers such as 7, etc.\n",
    "Prefix such as b'Sample prefixed binary string.'\n",
    "\"\"\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(\"\\nclean text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Latif-\n",
      "[nltk_data]     Calderón\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Latif-\n",
      "[nltk_data]     Calderón\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Latif-\n",
      "[nltk_data]     Calderón\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  dear sir strictly private business proposal mi...  \n",
      "1                                                     \n",
      "2  noracheryl emailed dozen memo haiti weekend pl...  \n",
      "3  dear sirfmadamc know proposal might surprise e...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary resources are downloaded from NLTK\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('wordnet')  # WordNet Lemmatizer needs this\n",
    "nltk.download('stopwords')  # For stopwords\n",
    "\n",
    "def clean_and_lemmatize(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Initialize the WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize words (removing stopwords if desired)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Adjust the path to where your CSV file is located\n",
    "data_path = '../data/kg_train.csv'  # Change this to the correct file path\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, encoding='latin-1')  # Ensure the correct encoding\n",
    "\n",
    "# Assuming the text data is in a column named 'text', adjust if necessary\n",
    "df['cleaned_text'] = df['text'].apply(clean_and_lemmatize)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(df[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running forest adventurous fox fellow animal enjoying playful carefree afternoon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Latif-\n",
      "[nltk_data]     Calderón\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Latif-\n",
      "[nltk_data]     Calderón\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Latif-\n",
      "[nltk_data]     Calderón\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary resources are downloaded from NLTK\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('wordnet')  # WordNet Lemmatizer needs this\n",
    "nltk.download('stopwords')  # For stopwords\n",
    "\n",
    "def clean_and_lemmatize(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Initialize the WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize words (removing stopwords if desired)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Sample text for demonstration\n",
    "sample_text = \"\"\"\n",
    "Running through the forest, the adventurous foxes and their fellow animals were enjoying a playful and carefree afternoon.\n",
    "\"\"\"\n",
    "\n",
    "# Clean and lemmatize the text\n",
    "cleaned_lemmatized_text = clean_and_lemmatize(sample_text)\n",
    "print(cleaned_lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Latif-\n",
      "[nltk_data]     Calderón\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: Index(['text', 'label'], dtype='object')\n",
      "Top 10 words in ham messages: []\n",
      "Top 10 words in spam messages: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Removing special characters and converting to lowercase\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "def get_top_words(messages, top_n=10):\n",
    "    # Concatenate all messages\n",
    "    all_words = ' '.join(messages)\n",
    "\n",
    "    # Tokenize words\n",
    "    words = all_words.split()\n",
    "\n",
    "    # Remove stopwords and get the frequency distribution\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Use Counter to get the most common words\n",
    "    word_counts = Counter(filtered_words)\n",
    "    return word_counts.most_common(top_n)\n",
    "\n",
    "# Load your data from the CSV file\n",
    "data_path = '../data/kg_train.csv'  # Change this to the path where your CSV is located\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path, encoding='latin-1')  # Ensure correct encoding if necessary\n",
    "\n",
    "# Debugging step: print column names to identify correct ones\n",
    "print(\"Column names:\", df.columns)\n",
    "\n",
    "# Assuming there are columns named 'text' and 'label', adjust based on actual CSV\n",
    "# Update these lines based on column names\n",
    "text_column = 'text'  # Replace 'text' with actual column name with text data\n",
    "label_column = 'label'  # Replace 'label' with actual column name with label data\n",
    "\n",
    "df['cleaned_text'] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_messages = df[df[label_column] == 'ham']['cleaned_text']\n",
    "spam_messages = df[df[label_column] == 'spam']['cleaned_text']\n",
    "\n",
    "# Get top words for ham and spam\n",
    "top_ham_words = get_top_words(ham_messages, top_n=10)\n",
    "top_spam_words = get_top_words(spam_messages, top_n=10)\n",
    "\n",
    "print(\"Top 10 words in ham messages:\", top_ham_words)\n",
    "print(\"Top 10 words in spam messages:\", top_spam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ham messages: [('hello', 1), ('today', 1), ('lets', 1), ('meet', 1), ('lunch', 1), ('tomorrow', 1), ('still', 1), ('golf', 1), ('weekend', 1)]\n",
      "Top 10 words in spam messages: [('win', 1), ('money', 1), ('special', 1), ('offer', 1), ('congratulations', 1), ('youve', 1), ('free', 1), ('gift', 1), ('card', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Latif-\n",
      "[nltk_data]     Calderón\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Removing special characters and converting to lowercase\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text).lower()\n",
    "    return text\n",
    "\n",
    "def get_top_words(messages, top_n=10):\n",
    "    # Concatenate all messages\n",
    "    all_words = ' '.join(messages)\n",
    "\n",
    "    # Tokenize words\n",
    "    words = all_words.split()\n",
    "\n",
    "    # Remove stopwords and get the frequency distribution\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Use Counter to get the most common words\n",
    "    word_counts = Counter(filtered_words)\n",
    "    return word_counts.most_common(top_n)\n",
    "\n",
    "# Sample dataframe simulating loaded dataset\n",
    "data = {\n",
    "    'label': ['ham', 'spam', 'ham', 'spam', 'ham'],\n",
    "    'message': [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"Win money now! Special offer just for you!!\",\n",
    "        \"Let's meet up for lunch tomorrow.\",\n",
    "        \"Congratulations, you've won a free gift card.\",\n",
    "        \"Are we still on for golf this weekend?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess messages\n",
    "df['cleaned_message'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_messages = df[df['label'] == 'ham']['cleaned_message']\n",
    "spam_messages = df[df['label'] == 'spam']['cleaned_message']\n",
    "\n",
    "# Get top words for ham and spam\n",
    "top_ham_words = get_top_words(ham_messages, top_n=10)\n",
    "top_spam_words = get_top_words(spam_messages, top_n=10)\n",
    "\n",
    "print(\"Top 10 words in ham messages:\", top_ham_words)\n",
    "print(\"Top 10 words in spam messages:\", top_spam_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               preprocessed_text  money_mark  \\\n",
      "0                          win a free iphone now           1   \n",
      "1     hello, let's catch up over coffee tomorrow           1   \n",
      "2  transfer your funds to this account for cheap           1   \n",
      "3              enjoy the best euro holiday deals           1   \n",
      "4           your bank account has been suspended           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 1        21  \n",
      "1                 0        42  \n",
      "2                 1        45  \n",
      "3                 0        33  \n",
      "4                 1        36  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataframe (replace this with your actual data)\n",
    "data_train = pd.DataFrame({\n",
    "    'preprocessed_text': [\n",
    "        \"win a free iphone now\",\n",
    "        \"hello, let's catch up over coffee tomorrow\",\n",
    "        \"transfer your funds to this account for cheap\",\n",
    "        \"enjoy the best euro holiday deals\",\n",
    "        \"your bank account has been suspended\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "data_val = pd.DataFrame({\n",
    "    'preprocessed_text': [\n",
    "        \"cheap medication for sale\",\n",
    "        \"meeting at the bank tomorrow\",\n",
    "        \"urgent password reset required\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Define patterns for monetary symbols and suspicious words\n",
    "money_simbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \n",
    "                             \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "# Add new features for training data\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list).astype(int)\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words).astype(int)\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(len)\n",
    "\n",
    "# Add new features for validation data\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list).astype(int)\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words).astype(int)\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(len)\n",
    "\n",
    "# Display the augmented training data\n",
    "print(data_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>win a free iphone now</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello, let's catch up over coffee tomorrow</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transfer your funds to this account for cheap</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enjoy the best euro holiday deals</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>your bank account has been suspended</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               preprocessed_text  money_mark  \\\n",
       "0                          win a free iphone now           1   \n",
       "1     hello, let's catch up over coffee tomorrow           1   \n",
       "2  transfer your funds to this account for cheap           1   \n",
       "3              enjoy the best euro holiday deals           1   \n",
       "4           your bank account has been suspended           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1        21  \n",
       "1                 0        42  \n",
       "2                 1        45  \n",
       "3                 0        33  \n",
       "4                 1        36  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in CSV: Index(['text', 'label'], dtype='object')\n",
      "Feature names: ['00' '000' '0000' ... 'â½x60ã' 'â½x70' 'â½ã']\n",
      "Word count matrix:\n",
      " [[1 2 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data_path = '../data/kg_train.csv'  # Adjust path according to your folder structure\n",
    "df = pd.read_csv(data_path, encoding='latin-1')  # Adjust encoding if necessary\n",
    "\n",
    "# Print columns to check names\n",
    "print(\"Column names in CSV:\", df.columns)\n",
    "\n",
    "# Assuming the text data is in a column named 'text'\n",
    "# Replace 'text' with the actual column name containing text data\n",
    "text_column = 'text'  # Update this with the correct column name\n",
    "documents = df[text_column].dropna().tolist()  # Convert to a list of strings\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names, which are the unique words in the corpus\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to an array for easy viewing\n",
    "word_count_array = X.toarray()\n",
    "\n",
    "# Display the matrix\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Word count matrix:\\n\", word_count_array)\n",
    "\n",
    "# Each entry in word_count_array corresponds to a count of each feature_name in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['brown' 'by' 'dog' 'fox' 'high' 'jump' 'jumping' 'jumps' 'lazy' 'never'\n",
      " 'over' 'quick' 'quickly' 'race' 'the' 'wins']\n",
      "Word count matrix:\n",
      " [[1 0 1 1 0 0 0 1 1 0 1 1 0 0 2 0]\n",
      " [0 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0]\n",
      " [1 1 0 1 1 0 1 0 0 0 0 1 0 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample data\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Never jump over the lazy dog quickly.\",\n",
    "    \"The quick brown fox wins the race by jumping high.\",\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names, which are the unique words in the corpus\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to an array for easy viewing\n",
    "word_count_array = X.toarray()\n",
    "\n",
    "# Display the matrix\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Word count matrix:\\n\", word_count_array)\n",
    "\n",
    "# Here's how you can interpret the feature_names and word_count_array:\n",
    "# Each entry in word_count_array corresponds to a count of each feature_name in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in CSV: Index(['text', 'label'], dtype='object')\n",
      "Shape of TF-IDF vectorized dataset: (5964, 79210)\n",
      "TF-IDF Feature Names: ['00' '000' '0000' ... 'â½x60ã' 'â½x70' 'â½ã']\n",
      "TF-IDF Dense Array:\n",
      " [[0.03375527 0.06271964 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.02894602 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data_path = '../data/kg_train.csv'  # Adjust path to the location of your file\n",
    "df = pd.read_csv(data_path, encoding='latin-1')  # Adjust encoding if necessary\n",
    "\n",
    "# Print columns to check their names\n",
    "print(\"Column names in CSV:\", df.columns)\n",
    "\n",
    "# Assuming the text data is in a column named 'text'\n",
    "# Replace 'text' with the actual column name containing text data\n",
    "text_column = 'text'  # Update this with the correct column name\n",
    "documents = df[text_column].dropna().tolist()  # Convert to a list, remove NaN entries\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents into TF-IDF vectors\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print shape of the vectorized dataset\n",
    "print(\"Shape of TF-IDF vectorized dataset:\", X_tfidf.shape)\n",
    "\n",
    "# Get and print feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"TF-IDF Feature Names:\", feature_names)\n",
    "\n",
    "# Convert matrix to array for easier inspection (optional, can be large)\n",
    "tfidf_array = X_tfidf.toarray()\n",
    "\n",
    "# Print the dense matrix representation, might be largeprint(\"TF-IDF Feature Names:\", feature_names)\n",
    "print(\"TF-IDF Dense Array:\\n\", tfidf_array[:5])  # Print only first 5 rows as a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF vectorized dataset: (3, 16)\n",
      "TF-IDF Feature Names: ['brown' 'by' 'dog' 'fox' 'high' 'jump' 'jumping' 'jumps' 'lazy' 'never'\n",
      " 'over' 'quick' 'quickly' 'race' 'the' 'wins']\n",
      "TF-IDF Dense Array:\n",
      " [[0.31401745 0.         0.31401745 0.31401745 0.         0.\n",
      "  0.         0.41289521 0.31401745 0.         0.31401745 0.31401745\n",
      "  0.         0.         0.48772512 0.        ]\n",
      " [0.         0.         0.33729513 0.         0.         0.44350256\n",
      "  0.         0.         0.33729513 0.44350256 0.33729513 0.\n",
      "  0.44350256 0.         0.26193976 0.        ]\n",
      " [0.2667197  0.35070436 0.         0.2667197  0.35070436 0.\n",
      "  0.35070436 0.         0.         0.         0.         0.2667197\n",
      "  0.         0.35070436 0.41426329 0.35070436]]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample data\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Never jump over the lazy dog quickly.\",\n",
    "    \"The quick brown fox wins the race by jumping high.\",\n",
    "]\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents into TF-IDF vectors\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Print shape of the vectorized dataset\n",
    "print(\"Shape of TF-IDF vectorized dataset:\", X_tfidf.shape)\n",
    "\n",
    "# It's often useful to view the feature names (vocabulary)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to array for illustration\n",
    "tfidf_array = X_tfidf.toarray()\n",
    "\n",
    "# Print the TF-IDF feature names and the dense representation\n",
    "print(\"TF-IDF Feature Names:\", feature_names)\n",
    "print(\"TF-IDF Dense Array:\\n\", tfidf_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultinomialNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Classifier will make prediction on my data.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Train the classifier (MultinomialNB)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMultinomialNB\u001b[49m()\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_tfidf, data_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Make predictions on the validation set\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MultinomialNB' is not defined"
     ]
    }
   ],
   "source": [
    "# Classifier will make prediction on my data.\n",
    "# Train the classifier (MultinomialNB)\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(X_train_tfidf, data_train['label'])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "\n",
    "y_pred = model.predict(X_val_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset - replace this with your actual dataset\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is a sample text with some special words and phrases.\",\n",
    "        \"Another text to showcase this classifier.\",\n",
    "        \"We are going to predict if there's a pattern or not.\",\n",
    "        \"Learning is fun with examples.\",\n",
    "        \"Data can tell many things if observed well.\"\n",
    "    ],\n",
    "    'label': [0, 1, 0, 1, 0]  # Example labels\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the training and validation data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "# Initialize and train the Multinomial Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions = model.predict(X_val_tfidf)\n",
    "\n",
    "# Output predictions\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Latif-Calderón\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Latif-Calderón\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Latif-Calderón\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Latif-Calderón\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Latif-Calderón\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Latif-Calderón\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Assume we have the following sample dataset\n",
    "data = {\n",
    "    'text': [\n",
    "        \"Get a free trial now\",\n",
    "        \"Join us for the meeting tomorrow\",\n",
    "        \"Congratulations, you've won a prize\",\n",
    "        \"Please confirm your email address\",\n",
    "        \"Check out this amazing offer\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1]  # Example labels: 1 for spam, 0 for ham\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the dataset into a training and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the training and validation data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "predictions = model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in CSV: Index(['text', 'label'], dtype='object')\n",
      "Accuracy with tfidf_unigram: 0.93\n",
      "Accuracy with tfidf_bigram: 0.90\n",
      "Accuracy with count_unigram: 0.93\n",
      "Accuracy with count_bigram: 0.94\n",
      "Best feature representation: count_bigram with accuracy of 0.94\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "# Load your data from the CSV file\n",
    "data_path = '../data/kg_train.csv'  # Adjust the path as needed\n",
    "df = pd.read_csv(data_path, encoding='latin-1')  # Adjust encoding if necessary\n",
    "\n",
    "# Print column names to verify correct usage\n",
    "print(\"Column names in CSV:\", df.columns) \n",
    "\n",
    "# Assume the column with text data is labeled 'text' and labels are 'label'\n",
    "# Update these variables with actual column names from your CSV\n",
    "text_column = 'text'  # Update with actual text column name\n",
    "label_column = 'label'  # Update with actual label column name\n",
    "\n",
    "# Preprocess the text column\n",
    "df[text_column] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[text_column], df[label_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Explore different vectorizers\n",
    "vectorizers = {\n",
    "    'tfidf_unigram': TfidfVectorizer(),\n",
    "    'tfidf_bigram': TfidfVectorizer(ngram_range=(1, 2)),\n",
    "    'count_unigram': CountVectorizer(),\n",
    "    'count_bigram': CountVectorizer(ngram_range=(1, 2)),\n",
    "}\n",
    "\n",
    "best_accuracy = 0\n",
    "best_vectorizer = None\n",
    "best_model = None\n",
    "\n",
    "# Train and evaluate with each vectorizer\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    # Transform datasets\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "    \n",
    "    # Initialize and train the classifier\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_val_vec)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_val, predictions)\n",
    "    print(f\"Accuracy with {name}: {accuracy:.2f}\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_vectorizer = name\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best feature representation: {best_vectorizer} with accuracy of {best_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with tfidf_unigram: 0.00\n",
      "Accuracy with tfidf_bigram: 0.00\n",
      "Accuracy with count_unigram: 0.00\n",
      "Accuracy with count_bigram: 0.00\n",
      "Best feature representation: None with accuracy of 0.00\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load your data from Kaggle or a local CSV file\n",
    "# For demonstration purposes, I'm creating a mock dataset\n",
    "data = {\n",
    "    'text': [\n",
    "        \"Free trial offer just for you\", \n",
    "        \"Join the meeting at noon\", \n",
    "        \"Congratulations, you won!\", \n",
    "        \"Please confirm your email subscription\", \n",
    "        \"Earn $$ with this offer!\"\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1]  # Example labels: 1 for spam, 0 for ham\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Basic text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    return text.lower()\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Explore different vectorizers\n",
    "vectorizers = {\n",
    "    'tfidf_unigram': TfidfVectorizer(),\n",
    "    'tfidf_bigram': TfidfVectorizer(ngram_range=(1, 2)),\n",
    "    'count_unigram': CountVectorizer(),\n",
    "    'count_bigram': CountVectorizer(ngram_range=(1, 2)),\n",
    "}\n",
    "\n",
    "best_accuracy = 0\n",
    "best_vectorizer = None\n",
    "best_model = None\n",
    "\n",
    "# Train and evaluate with each vectorizer\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    # Transform datasets\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "    \n",
    "    # Initialize and train the classifier\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_val_vec)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_val, predictions)\n",
    "    print(f\"Accuracy with {name}: {accuracy:.2f}\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_vectorizer = name\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best feature representation: {best_vectorizer} with accuracy of {best_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
