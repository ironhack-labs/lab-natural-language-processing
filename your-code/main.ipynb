{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (800,) Test set: (200,)\n",
      "Spam ratio in train: 0.4425 Spam ratio in test: 0.44\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and labels\n",
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "# Train/test split (80/20 by default, stratify keeps same spam/ham ratio in both)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train set:\", X_train.shape, \"Test set:\", X_test.shape)\n",
    "print(\"Spam ratio in train:\", y_train.mean(), \"Spam ratio in test:\", y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, html\n",
    "\n",
    "_HTML_SCRIPTS_STYLES = re.compile(r'(?is)<(script|style)\\b.*?>.*?</\\1>')\n",
    "_HTML_COMMENTS       = re.compile(r'(?is)<!--.*?-->')\n",
    "_HTML_TAGS           = re.compile(r'(?is)<[^>]+>')\n",
    "\n",
    "def strip_html_min(s: str) -> str:\n",
    "    s = html.unescape(str(s))\n",
    "    s = _HTML_SCRIPTS_STYLES.sub(\" \", s)   # 1) scripts/styles\n",
    "    s = _HTML_COMMENTS.sub(\" \", s)         # 2) comments (before tags!)\n",
    "    s = _HTML_TAGS.sub(\" \", s)             # 3) remaining tags\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()  # tidy spaces\n",
    "\n",
    "X_train_clean = X_train.apply(strip_html_min)\n",
    "X_test_clean  = X_test.apply(strip_html_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "\n",
    "    # 1) remove special characters (keep only letters/numbers/spaces)\n",
    "    s = re.sub(r'[^A-Za-z0-9\\s]', ' ', s)\n",
    "\n",
    "    # 2) remove numbers\n",
    "    s = re.sub(r'\\d+', ' ', s)\n",
    "\n",
    "    # 3) remove all single characters\n",
    "    s = re.sub(r'\\b[a-zA-Z]\\b', ' ', s)\n",
    "\n",
    "    # 4) remove single characters from the start\n",
    "    s = re.sub(r'^\\s*[a-zA-Z]\\s+', ' ', s)\n",
    "\n",
    "    # 5) substitute multiple spaces with single space\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "\n",
    "    # 6) remove prefixed 'b' (artifact from byte strings: b'text')\n",
    "    s = re.sub(r'^b\\s+', '', s)\n",
    "\n",
    "    # 7) convert to lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = X_train_clean.apply(normalize_text)\n",
    "X_test_clean  = X_test_clean.apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(s: str) -> str:\n",
    "    tokens = s.split()  # already cleaned and lowercased\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = X_train_clean.apply(remove_stopwords)\n",
    "X_test_clean  = X_test_clean.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(s: str) -> str:\n",
    "    tokens = s.split()  # text already cleaned + stopwords removed\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = X_train_clean.apply(lemmatize_text)\n",
    "X_test_clean  = X_test_clean.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Ham Words:\n",
      "state        97\n",
      "president    95\n",
      "would        92\n",
      "mr           90\n",
      "percent      80\n",
      "obama        80\n",
      "call         77\n",
      "work         72\n",
      "time         70\n",
      "one          69\n",
      "dtype: int64\n",
      "\n",
      "Top 10 Spam Words:\n",
      "money          761\n",
      "account        674\n",
      "bank           615\n",
      "fund           600\n",
      "transaction    435\n",
      "business       412\n",
      "country        401\n",
      "mr             384\n",
      "million        364\n",
      "company        340\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer turns text into BoW representation\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit on the cleaned training text\n",
    "X_bow = cv.fit_transform(X_train_clean)\n",
    "\n",
    "# Make a DataFrame: rows = docs, cols = words\n",
    "bow_df = pd.DataFrame(X_bow.toarray(), columns=cv.get_feature_names_out(), index=X_train.index)\n",
    "\n",
    "# Attach labels\n",
    "bow_df[\"label\"] = y_train\n",
    "\n",
    "# Separate ham (0) and spam (1)\n",
    "ham_words  = bow_df[bow_df[\"label\"]==0].drop(columns=\"label\").sum().sort_values(ascending=False)\n",
    "spam_words = bow_df[bow_df[\"label\"]==1].drop(columns=\"label\").sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 Ham Words:\")\n",
    "print(ham_words.head(10))\n",
    "\n",
    "print(\"\\nTop 10 Spam Words:\")\n",
    "print(spam_words.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap train/val back into DataFrames\n",
    "data_train = pd.DataFrame({\"preprocessed_text\": X_train_clean, \"label\": y_train})\n",
    "data_val   = pd.DataFrame({\"preprocessed_text\": X_test_clean, \"label\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>label</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>dear good day hope fine cdear writting mail du...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>mr henry kaborethe chief auditor inchargeforei...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>desk dr adamu ismalerauditing accounting manag...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>dear friend name loi estrada wife mr josephest...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     preprocessed_text  label  money_mark  \\\n",
       "442  dear good day hope fine cdear writting mail du...      1           1   \n",
       "962  mr henry kaborethe chief auditor inchargeforei...      1           0   \n",
       "971                                                         0           0   \n",
       "190  desk dr adamu ismalerauditing accounting manag...      1           1   \n",
       "551  dear friend name loi estrada wife mr josephest...      1           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "442                 1       998  \n",
       "962                 1      1946  \n",
       "971                 0         0  \n",
       "190                 1       383  \n",
       "551                 1      1475  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does this even mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF train shape: (800, 28009)\n",
      "TF-IDF test shape : (200, 28009)\n",
      "TF-IDF all data shape: (1000, 25301)\n"
     ]
    }
   ],
   "source": [
    "# 1) Load the vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "# 2) Fit on training set, transform both train and test\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_clean)\n",
    "X_test_tfidf  = tfidf.transform(X_test_clean)\n",
    "\n",
    "# 3) Print the shape of the vectorized dataset\n",
    "print(\"TF-IDF train shape:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF test shape :\", X_test_tfidf.shape)\n",
    "\n",
    "# (optional) Full dataset if you want to vectorize all at once\n",
    "X_all_tfidf = tfidf.fit_transform(data[\"text\"])\n",
    "print(\"TF-IDF all data shape:\", X_all_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n",
      "[[112   0]\n",
      " [  5  83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.957     1.000     0.978       112\n",
      "           1      1.000     0.943     0.971        88\n",
      "\n",
      "    accuracy                          0.975       200\n",
      "   macro avg      0.979     0.972     0.974       200\n",
      "weighted avg      0.976     0.975     0.975       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# assume you already have:\n",
    "# X_train_tfidf, X_test_tfidf, y_train, y_test\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, n_jobs=None)  # n_jobs arg removed in recent sklearn; omit if error\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
