{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/2v0z1h_n6vj6jkt8s3sjsbv40000gn/T/ipykernel_76777/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv('/Users/iz/Desktop/incomplete labs/lab-natural-language-processing/data/kg_train.csv',encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "#X = data['text']  \n",
    "#y = data['label']\n",
    "\n",
    "# Split the data (20% for testing)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# print shapes\n",
    "#print(f'Train size: {X_train.shape}, Test size: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (1000,), Test size: (5964,)\n"
     ]
    }
   ],
   "source": [
    "#Note: The following code assumes that the dataset has already been split into train and test sets as specified by instructor. So now I'm not resplitting hte data.\n",
    "\n",
    "# Load the already-split datasets\n",
    "train_df = pd.read_csv('/Users/iz/Desktop/incomplete labs/lab-natural-language-processing/data/kg_train.csv', encoding='latin-1')\n",
    "test_df = pd.read_csv('/Users/iz/Desktop/incomplete labs/lab-natural-language-processing/data/kg_test.csv', encoding='latin-1')\n",
    "\n",
    "# Optional: reduce size to speed up development (for training only)\n",
    "train_df = train_df.head(1000)\n",
    "\n",
    "# Handle missing values\n",
    "train_df.fillna(\"\", inplace=True)\n",
    "test_df.fillna(\"\", inplace=True)\n",
    "\n",
    "# Assign preprocessed variables\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_test = test_df['text']\n",
    "#y_test = test_df['label']\n",
    "\n",
    "# Confirm shape\n",
    "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/2v0z1h_n6vj6jkt8s3sjsbv40000gn/T/ipykernel_76777/2677811227.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "/var/folders/6x/2v0z1h_n6vj6jkt8s3sjsbv40000gn/T/ipykernel_76777/2677811227.py:8: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "!pip install beautifulsoup4\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html_bs(text):\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.extract()\n",
    "\n",
    "    # strips the rest of the HTML tags by converting parsed object back to plain text with .get_text()\n",
    "        # Get text content only (strips all tags)\n",
    "    cleaned_text = soup.get_text()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply to the training and test sets\n",
    "X_train = X_train.apply(clean_html_bs)\n",
    "X_test = X_test.apply(clean_html_bs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretokenization preprocessing (cleaning data before spliting words into tokens, lammatize, and split). This is specific to html. You can remove the comments after you remove the javascript\n",
    "\n",
    "pre-tokenization preprocessing:\n",
    "- Remove all the special characters - dont need special NLP methods. Can use string methods (lambda, etc of the corresponding columns you want to change. String methods are from the beginning in the functions labs. Like to identify special characters)\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)               # Remove all non-word characters (keep only letters, numbers, _)\n",
    "    text = re.sub(r'\\d+', '', text)               # Remove numbers\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)   # Remove single characters between spaces (e.g., ' a ')\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)      # Remove single characters at start\n",
    "    text = re.sub(r'\\s+', ' ', text)              # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"^b[\\'\\\"]\", '', text)          # Remove prefix b' or b\" at start \n",
    "    return text.lower()                           # Convert to lowercase\n",
    "\n",
    "# Apply to cleaned HTML text\n",
    "X_train = X_train.apply(clean_text)\n",
    "X_test = X_test.apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords.\n",
    "\n",
    "- from here NLP methods we learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the English stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#loop version to remove stop words\n",
    "def remove_stopwords(text): #defines the function to remove stopwords (takes a string as input and returns a string without stopwords)\n",
    "    words = text.split()  # Split sentence into words\n",
    "    filtered = []          # Create empty list to hold non-stop words\n",
    "    \n",
    "    for word in words: #loop through each word, \n",
    "        if word not in stop_words: # If the word is not in the stopword list, append it to filtered,\n",
    "            filtered.append(word) #append it to the filtered list\n",
    "    \n",
    "    return ' '.join(filtered)  # Join the filtered words back into a sentence with space separations\n",
    "\n",
    "#list comprehension method to remove stopwords\n",
    "#def remove_stopwords(text):\n",
    "    #words = text.split() #tokenizes the text into words (splits into words)\n",
    "    #filtered = [word for word in words if word not in stop_words] #Filters out words that are in the stopword list\n",
    "    #return ' '.join(filtered) #Rejoins the filtered words into a cleaned string\n",
    "\n",
    "# Apply remove stopwords function to both training and test sets\n",
    "X_train = X_train.apply(remove_stopwords)\n",
    "X_test = X_test.apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 54]\n",
      "[nltk_data]     Connection reset by peer>\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/iz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download wordnet if not already done\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # For extended lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # Initialize the lemmatizer\n",
    "\n",
    "def lemmatize_text(text): #takes a string as input and returns a lemmatized string\n",
    "    words = text.split() #split the sentance/text into words\n",
    "    lemmatized = [] # Initialize an empty list to store lemmatized words\n",
    "    for word in words: #loop through each word in the text/sentane\n",
    "        lemmatized_word = lemmatizer.lemmatize(word) # Lemmatize the word. Works by converting each word to its lemma base form using wordnet dictionary. Default (wihout POS) is noun\n",
    "        lemmatized.append(lemmatized_word) # Append the lemmatized word to the list\n",
    "    return ' '.join(lemmatized) #join the lemmatized words back into a single string with space separations\n",
    "\n",
    "# Apply to your cleaned and stopword-filtered text\n",
    "X_train = X_train.apply(lemmatize_text)\n",
    "X_test = X_test.apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid ham messages after preprocessing.\n",
      "No valid spam messages after preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Combine text and labels into a DataFrame for easier manipulation\n",
    "train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "\n",
    "# Remove rows with empty or whitespace-only text\n",
    "train_df = train_df[train_df['text'].str.strip() != '']\n",
    "\n",
    "# Separate the df into 2 parts: ham and spam texts\n",
    "ham_texts = train_df[train_df['label'] == 'ham']['text']\n",
    "spam_texts = train_df[train_df['label'] == 'spam']['text']\n",
    "\n",
    "# EDA portion: Check if either set is empty to prevent errors in vectorizaton, otherwise vectorize (turn words into frequency counts using countvectorizer), then convert into a dataframe and sum word counts across all messages in that label group, then sort and shot the top 10 most common words.\n",
    "#fit_transform... = sparse matrix of word counts\n",
    "#.toarray() = numpy matrix (an array)\n",
    "#pd.datafram... = converted numpy matrix/array into a word counts dataframe (labeled, pandas dataframe/table)\n",
    "#.sort..= top 10 mst common words.\n",
    "if ham_texts.empty:\n",
    "    print(\"No valid ham messages after preprocessing.\")\n",
    "else:\n",
    "    vectorizer_ham = CountVectorizer()\n",
    "    ham_matrix = vectorizer_ham.fit_transform(ham_texts)\n",
    "    ham_counts = pd.DataFrame(ham_matrix.toarray(), columns=vectorizer_ham.get_feature_names_out()).sum().sort_values(ascending=False)\n",
    "    print(\"Top 10 Ham Words:\")\n",
    "    print(ham_counts.head(10))\n",
    "\n",
    "if spam_texts.empty:\n",
    "    print(\"No valid spam messages after preprocessing.\")\n",
    "else:\n",
    "    vectorizer_spam = CountVectorizer()\n",
    "    spam_matrix = vectorizer_spam.fit_transform(spam_texts)\n",
    "    spam_counts = pd.DataFrame(spam_matrix.toarray(), columns=vectorizer_spam.get_feature_names_out()).sum().sort_values(ascending=False)\n",
    "    print(\"\\nTop 10 Spam Words:\")\n",
    "    print(spam_counts.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Sample:\n",
      "                                   preprocessed_text  label\n",
      "0  dear sir strictly private business proposal mi...      1\n",
      "1                                                         0\n",
      "2  nora cheryl emailed dozen memo haiti weekend p...      0\n",
      "3  dear sir fmadam know proposal might surprise e...      1\n",
      "4                                                fyi      0\n",
      "\n",
      "Validation Data Sample:\n",
      "                                   preprocessed_text\n",
      "0  usiness fact deceased man foreigner authorized...\n",
      "1  happy adjust afternoon going suggest pm start ...\n",
      "2  lael brainard confirmed afternoon miguel rodri...\n",
      "3  friday march sbwhoeopã â ã â ã â extended cong...\n",
      "4  dear good friend happy inform succe ssin getti...\n"
     ]
    }
   ],
   "source": [
    "# in order to run the below code cell (data_trrain/val was not defined), I need to Create data_train and data_val from cleaned data in order to run the next code cell.\n",
    "# Create data_train and data_val using cleaned text\n",
    "data_train = pd.DataFrame({'preprocessed_text': X_train, 'label': y_train})\n",
    "data_val = pd.DataFrame({'preprocessed_text': X_test})  # no label in test set\n",
    "\n",
    "\n",
    "print(\"Training Data Sample:\")\n",
    "print(data_train.head())\n",
    "print(\"\\nValidation Data Sample:\")\n",
    "print(data_val.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>label</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nora cheryl emailed dozen memo haiti weekend p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear sir fmadam know proposal might surprise e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   preprocessed_text  label  money_mark  \\\n",
       "0  dear sir strictly private business proposal mi...      1           1   \n",
       "1                                                         0           1   \n",
       "2  nora cheryl emailed dozen memo haiti weekend p...      0           1   \n",
       "3  dear sir fmadam know proposal might surprise e...      1           1   \n",
       "4                                                fyi      0           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1      1493  \n",
       "1                 0         0  \n",
       "2                 0       111  \n",
       "3                 1      1350  \n",
       "4                 0         3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?\n",
    "- Bag of words works by converting words/text/docs into vectors (numeric data).\n",
    "- Bag of words works by: tokenizing the text, counting word fre4quencies,and then representing each document as a row of word counts. \n",
    "- essentially, each document becomes a vector of word freequenies (which are now numeric data), forming a matrix of numbers (one row per document, one column per word) that the model can use to count.\n",
    "\n",
    "1. Start with a list of texts (the documents).\n",
    "2. Build a vocabulary of all unique words across all the docs.\n",
    "3. For each document, count how many times each word in the vocabulary appears.\n",
    "\n",
    "Below is an example of how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['call' 'free' 'me' 'money' 'now' 'offer']\n",
      "\n",
      "Bag of Words Matrix:\n",
      " [[0 1 0 1 1 0]\n",
      " [1 0 1 0 1 0]\n",
      " [0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#1. start with a list of texts (documents)\n",
    "# Sample documents\n",
    "corpus = [\"free money now\", \"call me now\", \"free offer\"]\n",
    "\n",
    "#2. Build the vocabulary of unique words from all the documents\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get feature names\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "#3. count number of times each word apperas in each document and create a matrix where each row is a document and each column is a word from the vocabulary.\n",
    "# View the Bag of Words matrix\n",
    "print(\"\\nBag of Words Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape (train): (1000, 22015)\n",
      "TF-IDF shape (test): (5964, 22015)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize and fit TF-IDF vectorizer on training data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit on train, transform both\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Shape info\n",
    "print(\"TF-IDF shape (train):\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF shape (test):\", X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?\n",
    "- tp distinguish between 0s (ham) and 1s (spam)\n",
    "- use which ever classifier you want. You dont have to hyperparamter tune it its just proof of concept-training classifier for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 predictions on test set:\n",
      "[1 0 0 0 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train model\n",
    "model_tfidf = LogisticRegression()\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Show predictions (you don’t have y_test)\n",
    "print(\"First 10 predictions on test set:\")\n",
    "print(y_pred_tfidf[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB predictions on test set (first 10):\n",
      "[1 0 0 0 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the classifier (default parameters)\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train on TF-IDF-transformed training data\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on test set (you don’t have y_test — just show preds)\n",
    "nb_predictions = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Preview predictions\n",
    "print(\"MultinomialNB predictions on test set (first 10):\")\n",
    "print(nb_predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Othe rimportant:\n",
    "- vectorizer best practice is like with scalers: you fit it to your traning data and transform entire data. Dont fit to training and test, only fit to training and then apply it to train and test (to avoid data leakage). basically only fit to training then apply to train and test.\n",
    "- pretokenization cleaning you apply to both sets, training and test. \n",
    "\n",
    "- one of the labels....basically looking for predominant words appearing only in spam and not so predominant in ham. \n",
    "- you can do plots to visualize dist of data and frequency of words. \n",
    "\n",
    "- keep the steps from the lab as a quick reference guide for cleaning HTML data...might not need all the steps but good to be aware. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
