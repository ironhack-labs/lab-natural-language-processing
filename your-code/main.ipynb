{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g2/dsm99_hd0c9_m_6qhbpwpvl80000gn/T/ipykernel_70120/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML # Fix depracted instruction\n",
    "\n",
    "# Configurar el ancho del contenedor\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Training Data Shape: (1000, 2)\n",
      "                                                text  label\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                           Will do.      0\n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                fyi      0\n",
      "\n",
      "Reduced Test Data Shape: (1000, 1)\n",
      "                                                text\n",
      "0  usiness is for the fact that the deceased man ...\n",
      "1  They are happy to adjust to the afternoon. I a...\n",
      "2  Lael Brainard was confirmed 78-19 this afterno...\n",
      "3  H <hrod17@clintonemail.com>Friday March 26 201...\n",
      "4  n;\"> Dear Good Friend,<br><br><br>I am happy t...\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "train_file_path = f\"{dir_path}kg_train.csv\"  # Train DataSet\n",
    "test_file_path = f\"{dir_path}kg_test.csv\"    # Test DataSet\n",
    "\n",
    "# Load the training and test data\n",
    "train_data = pd.read_csv(train_file_path, encoding='latin-1')\n",
    "test_data = pd.read_csv(test_file_path, encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development (e.g., first 1000 rows)\n",
    "train_data = train_data.head(1000)\n",
    "test_data = test_data.head(1000)  # Fixed this line to reduce test_data, not train_data\n",
    "\n",
    "# Fill missing values with an empty string to avoid errors during processing\n",
    "train_data.fillna(\"\", inplace=True)\n",
    "test_data.fillna(\"\", inplace=True)\n",
    "\n",
    "# Display the shape and preview of the reduced dataset\n",
    "print(f\"Reduced Training Data Shape: {train_data.shape}\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(f\"\\nReduced Test Data Shape: {test_data.shape}\")\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Partition 1 Shape: (500, 2)\n",
      "Train Partition 2 Shape: (500, 2)\n",
      "Test Partition 1 Shape: (500, 1)\n",
      "Test Partition 2 Shape: (500, 1)\n",
      "\n",
      "Preview of Train Partition 1:\n",
      "                                                  text  label\n",
      "680  Please for more confidentiality of this transa...      1\n",
      "177  Rich---Thanks for all you did to make the past...      0\n",
      "395  K - will call in a bit - in line at dmv to sur...      0\n",
      "911  Just wanted to tell you that speech yesterday ...      0\n",
      "793  FROM MR JOHNSON MUGABE=2E E-MAIL =3B mm=5Fm=5F...      1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split training data into two partitions: train_partition_1 and train_partition_2\n",
    "train_partition_1, train_partition_2 = train_test_split(train_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Split test data into two partitions: test_partition_1 and test_partition_2\n",
    "test_partition_1, test_partition_2 = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Display the shapes of the partitions\n",
    "print(f\"Train Partition 1 Shape: {train_partition_1.shape}\")\n",
    "print(f\"Train Partition 2 Shape: {train_partition_2.shape}\")\n",
    "print(f\"Test Partition 1 Shape: {test_partition_1.shape}\")\n",
    "print(f\"Test Partition 2 Shape: {test_partition_2.shape}\")\n",
    "\n",
    "# Optional: Preview one of the partitions\n",
    "print(\"\\nPreview of Train Partition 1:\")\n",
    "print(train_partition_1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean HTML content\n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)  # Remove <script>...</script>\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)    # Remove <style>...</style>\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Clean HTML\n",
    "    text = clean_html(text)\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Apply stemming\n",
    "    text = \" \".join([snowball.stem(word) for word in text.split()])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "680  Please for more confidentiality of this transa...   \n",
      "177  Rich---Thanks for all you did to make the past...   \n",
      "395  K - will call in a bit - in line at dmv to sur...   \n",
      "911  Just wanted to tell you that speech yesterday ...   \n",
      "793  FROM MR JOHNSON MUGABE=2E E-MAIL =3B mm=5Fm=5F...   \n",
      "\n",
      "                                          cleaned_text  \n",
      "680  pleas confidenti transact herebi advis kind re...  \n",
      "177  richthank make past year success one hope fami...  \n",
      "395          k call bit line dmv surrend ny licens sad  \n",
      "911  want tell speech yesterday terrif import say n...  \n",
      "793  mr johnson mugabe2 email 3b mm5fm5f17johnson40...  \n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the 'text' column\n",
    "train_partition_1['cleaned_text'] = train_partition_1['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the processed data\n",
    "print(train_partition_1[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "680  Please for more confidentiality of this transa...   \n",
      "177  Rich---Thanks for all you did to make the past...   \n",
      "395  K - will call in a bit - in line at dmv to sur...   \n",
      "911  Just wanted to tell you that speech yesterday ...   \n",
      "793  FROM MR JOHNSON MUGABE=2E E-MAIL =3B mm=5Fm=5F...   \n",
      "\n",
      "                                          cleaned_text  \n",
      "680  pleas confidenti transact herebi advis kind re...  \n",
      "177  richthank make past year success one hope fami...  \n",
      "395            call bit line dmv surrend ny licens sad  \n",
      "911  want tell speech yesterday terrif import say n...  \n",
      "793  mr johnson mugabe email mmfmfjohnsonyahooecome...  \n"
     ]
    }
   ],
   "source": [
    "# Initialize the Snowball Stemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)  # Remove <script>...</script>\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)    # Remove <style>...</style>\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove prefixed 'b' (if present due to byte representation)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Clean the text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Step 2: Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Step 3: Apply stemming\n",
    "    text = \" \".join([snowball.stem(word) for word in text.split()])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "train_partition_1['cleaned_text'] = train_partition_1['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the processed data\n",
    "print(train_partition_1[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "680  Please for more confidentiality of this transa...   \n",
      "177  Rich---Thanks for all you did to make the past...   \n",
      "395  K - will call in a bit - in line at dmv to sur...   \n",
      "911  Just wanted to tell you that speech yesterday ...   \n",
      "793  FROM MR JOHNSON MUGABE=2E E-MAIL =3B mm=5Fm=5F...   \n",
      "\n",
      "                                          cleaned_text  \n",
      "680  pleas confidenti transact herebi advis kind re...  \n",
      "177  richthank make past year success one hope fami...  \n",
      "395            call bit line dmv surrend ny licens sad  \n",
      "911  want tell speech yesterday terrif import say n...  \n",
      "793  mr johnson mugabe email mmfmfjohnsonyahooecome...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Initialize the Snowball Stemmer\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Load stopwords from NLTK\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)  # Remove <script>...</script>\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)    # Remove <style>...</style>\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Clean the text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    # Apply stemming\n",
    "    text = \" \".join([snowball.stem(word) for word in text.split()])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "train_partition_1['cleaned_text'] = train_partition_1['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the processed data\n",
    "print(train_partition_1[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  \\\n",
      "680  Please for more confidentiality of this transa...   \n",
      "177  Rich---Thanks for all you did to make the past...   \n",
      "395  K - will call in a bit - in line at dmv to sur...   \n",
      "911  Just wanted to tell you that speech yesterday ...   \n",
      "793  FROM MR JOHNSON MUGABE=2E E-MAIL =3B mm=5Fm=5F...   \n",
      "\n",
      "                                          cleaned_text  \n",
      "680  please confidentiality transaction hereby advi...  \n",
      "177  richthanks make past year successful one hope ...  \n",
      "395       call bit line dmv surrender ny license sadly  \n",
      "911  wanted tell speech yesterday terrific importan...  \n",
      "793  mr johnson mugabee email mmfmfjohnsonyahooecom...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load stopwords from NLTK\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)  # Remove <script>...</script>\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)    # Remove <style>...</style>\n",
    "    \n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Function to apply lemmatization\n",
    "def apply_lemmatization(text):\n",
    "    words = word_tokenize(text)  # Tokenize the text into words\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Clean the text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Step 2: Remove stopwords\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    # Step 3: Apply lemmatization\n",
    "    text = apply_lemmatization(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "train_partition_1['cleaned_text'] = train_partition_1['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the processed data\n",
    "print(train_partition_1[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words in Ham Messages:\n",
      "           word  count\n",
      "4718         us     76\n",
      "4979      would     69\n",
      "3273         pm     63\n",
      "4158      state     53\n",
      "3393  president     52\n",
      "3035        one     44\n",
      "3200    percent     40\n",
      "2792         mr     38\n",
      "4961       work     36\n",
      "2991     office     36\n",
      "\n",
      "Top 10 Words in Spam Messages:\n",
      "             word  count\n",
      "5609        money    486\n",
      "89        account    351\n",
      "971          bank    318\n",
      "9133           us    267\n",
      "1405     business    237\n",
      "3719         fund    208\n",
      "5520      million    202\n",
      "8844  transaction    199\n",
      "8898     transfer    197\n",
      "1838      company    186\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Separate spam and ham messages\n",
    "ham_messages = train_partition_1[train_partition_1['label'] == 0]['text']\n",
    "spam_messages = train_partition_1[train_partition_1['label'] == 1]['text']\n",
    "\n",
    "# Define a function to preprocess and clean the text\n",
    "def preprocess_text_simple(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    # Lowercase, remove punctuation, and remove stopwords\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Preprocess messages\n",
    "ham_messages_cleaned = ham_messages.apply(preprocess_text_simple)\n",
    "spam_messages_cleaned = spam_messages.apply(preprocess_text_simple)\n",
    "\n",
    "# Bag of Words for Ham Messages\n",
    "vectorizer_ham = CountVectorizer()\n",
    "ham_bow = vectorizer_ham.fit_transform(ham_messages_cleaned)\n",
    "ham_word_counts = pd.DataFrame({'word': vectorizer_ham.get_feature_names_out(),\n",
    "                                'count': ham_bow.sum(axis=0).A1})\n",
    "ham_top_words = ham_word_counts.sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "# Bag of Words for Spam Messages\n",
    "vectorizer_spam = CountVectorizer()\n",
    "spam_bow = vectorizer_spam.fit_transform(spam_messages_cleaned)\n",
    "spam_word_counts = pd.DataFrame({'word': vectorizer_spam.get_feature_names_out(),\n",
    "                                 'count': spam_bow.sum(axis=0).A1})\n",
    "spam_top_words = spam_word_counts.sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "# Display Top Words\n",
    "print(\"Top 10 Words in Ham Messages:\")\n",
    "print(ham_top_words)\n",
    "\n",
    "print(\"\\nTop 10 Words in Spam Messages:\")\n",
    "print(spam_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "680  Please for more confidentiality of this transa...      1   \n",
      "177  Rich---Thanks for all you did to make the past...      0   \n",
      "395  K - will call in a bit - in line at dmv to sur...      0   \n",
      "911  Just wanted to tell you that speech yesterday ...      0   \n",
      "793  FROM MR JOHNSON MUGABE=2E E-MAIL =3B mm=5Fm=5F...      1   \n",
      "\n",
      "                                          cleaned_text  money_mark  \\\n",
      "680  pleas confidenti transact herebi advis kind re...           1   \n",
      "177  richthank make past year success one hope fami...           1   \n",
      "395            call bit line dmv surrend ny licens sad           1   \n",
      "911  want tell speech yesterday terrif import say n...           1   \n",
      "793  mr johnson mugabe email mmfmfjohnsonyahooecome...           1   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "680                 1      3989  \n",
      "177                 0       707  \n",
      "395                 0        81  \n",
      "911                 0       580  \n",
      "793                 1      3633  \n"
     ]
    }
   ],
   "source": [
    "print(train_partition_1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sir strictly private business proposal mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>noracheryl emailed dozen memo haiti weekend pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear sirfmadamc know proposal might surprise e...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
       "1                                           Will do.      0   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
       "4                                                fyi      0   \n",
       "\n",
       "                                        cleaned_text  money_mark  \\\n",
       "0  dear sir strictly private business proposal mi...           1   \n",
       "1                                                              1   \n",
       "2  noracheryl emailed dozen memo haiti weekend pl...           1   \n",
       "3  dear sirfmadamc know proposal might surprise e...           1   \n",
       "4                                                fyi           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 0      2292  \n",
       "1                 0         8  \n",
       "2                 0       197  \n",
       "3                 1      2199  \n",
       "4                 0         3  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "train_partition_1['money_mark'] = train_partition_1['text'].str.contains(money_simbol_list)*1\n",
    "train_partition_1['suspicious_words'] = train_partition_1['text'].str.contains(suspicious_words)*1\n",
    "train_partition_1['text_len'] = train_partition_1['text'].apply(lambda x: len(x)) \n",
    "\n",
    "train_partition_2['money_mark'] = train_partition_2['text'].str.contains(money_simbol_list)*1\n",
    "train_partition_2['suspicious_words'] = train_partition_2['text'].str.contains(suspicious_words)*1\n",
    "train_partition_2['text_len'] = train_partition_2['text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['aaronovitchon' 'abacha' 'abachabefor' ... 'zuma' 'zumadirector' 'zurich']\n",
      "\n",
      "Bag of Words Representation (Count Matrix):\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the data and transform the documents\n",
    "X = vectorizer.fit_transform(train_partition_1['cleaned_text'])\n",
    "\n",
    "# Get the feature names (vocabulary)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the matrix to an array for better readability\n",
    "word_counts = X.toarray()\n",
    "\n",
    "# Display results\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"\\nBag of Words Representation (Count Matrix):\")\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorized dataset: (500, 10761)\n",
      "Vocabulary size: 10761\n",
      "Shape of the vectorized dataset Removing Stopwords Technique: (500, 10621)\n",
      "Shape of the vectorized dataset with N-Grams Technique: (500, 46536)\n",
      "Shape of the vectorized dataset with Limit Vocabulary Size Technique: (500, 46536)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the entire dataset\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_partition_1['cleaned_text'])\n",
    "\n",
    "# Print the shape of the TF-IDF vectorized dataset\n",
    "print(f\"Shape of the vectorized dataset: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Optionally, you can inspect the feature names (vocabulary)\n",
    "vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "# Applying Stopwords\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# Fit and transform the entire dataset\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_partition_1['cleaned_text'])\n",
    "\n",
    "# Print the shape of the TF-IDF vectorized dataset\n",
    "print(f\"Shape of the vectorized dataset Removing Stopwords Technique: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Applying N-grams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Unigrams and bigrams\n",
    "# Fit and transform the entire dataset\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_partition_1['cleaned_text'])\n",
    "\n",
    "# Print the shape of the TF-IDF vectorized dataset\n",
    "print(f\"Shape of the vectorized dataset with N-Grams Technique: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Applying Limit Vocabulary Size\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Unigrams and bigrams\n",
    "# Fit and transform the entire dataset\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_partition_1['cleaned_text'])\n",
    "\n",
    "# Print the shape of the TF-IDF vectorized dataset\n",
    "print(f\"Shape of the vectorized dataset with Limit Vocabulary Size Technique: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94       283\n",
      "           1       0.97      0.87      0.92       217\n",
      "\n",
      "    accuracy                           0.93       500\n",
      "   macro avg       0.94      0.92      0.93       500\n",
      "weighted avg       0.94      0.93      0.93       500\n",
      "\n",
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Step 1: Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 2: Vectorize the Dataset\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_partition_1['text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(train_partition_2['text'])\n",
    "\n",
    "# Step 3: Extract Target (Labels)\n",
    "y_train = train_partition_1['label']\n",
    "y_test = train_partition_2['label']\n",
    "\n",
    "# Step 4: Train a Classifier (e.g., Logistic Regression)\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Step 5: Make Predictions\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Step 6: Evaluate the Classifier\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle competitions download -c dsub-fraudulentemails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: Index(['text', 'label'], dtype='object')\n",
      "First few rows of the dataset:\n",
      "                                                text  label\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                           Will do.      0\n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                fyi      0\n",
      "Unique values in 'label' column before mapping:\n",
      "[1 0]\n",
      "Unique values in 'label' column after mapping:\n",
      "[nan]\n",
      "Dataset shape after dropping NaN labels: (0, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The dataset is empty after preprocessing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[216], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Ensure the dataset is not empty\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset is empty after preprocessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Print dataset shape after preprocessing\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The dataset is empty after preprocessing."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = f\"{dir_path}kg_train.csv\"  # Replace with your dataset path\n",
    "data = pd.read_csv(dataset_path, encoding='latin-1')\n",
    "\n",
    "# Debug: Inspect dataset structure\n",
    "print(\"Dataset columns:\", data.columns)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Skip mapping as the labels are already numeric\n",
    "# Debug: Check unique values in the 'label' column\n",
    "print(\"Unique values in 'label':\", data['label'].unique())\n",
    "\n",
    "# Fill missing values in the 'text' column with an empty string\n",
    "data['text'] = data['text'].fillna('')\n",
    "\n",
    "# Apply preprocessing to the text\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove rows with empty cleaned_text\n",
    "data = data[data['cleaned_text'].str.strip() != '']\n",
    "\n",
    "# Ensure the dataset is not empty\n",
    "if data.shape[0] == 0:\n",
    "    raise ValueError(\"The dataset is empty after preprocessing.\")\n",
    "\n",
    "# Print dataset shape after preprocessing\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['cleaned_text'], data['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Check for null values in y_train and y_test\n",
    "print(f\"Null values in y_train: {y_train.isnull().sum()}\")\n",
    "print(f\"Null values in y_test: {y_test.isnull().sum()}\")\n",
    "\n",
    "# Feature Representation\n",
    "# Bag of Words\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train and Evaluate MultinomialNB\n",
    "# Bag of Words\n",
    "nb_count = MultinomialNB()\n",
    "nb_count.fit(X_train_count, y_train)\n",
    "y_pred_count = nb_count.predict(X_test_count)\n",
    "\n",
    "print(\"Results with Bag of Words (CountVectorizer):\")\n",
    "print(classification_report(y_test, y_pred_count))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_count):.2f}\")\n",
    "\n",
    "# TF-IDF\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nResults with TF-IDF (TfidfVectorizer):\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tfidf):.2f}\")\n",
    "\n",
    "# Compare and Select Best Representation\n",
    "bag_accuracy = accuracy_score(y_test, y_pred_count)\n",
    "tfidf_accuracy = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "print(f\"Accuracy Bag of Words: {bag_accuracy:.2f}\")\n",
    "print(f\"Accuracy TF-IDF: {tfidf_accuracy:.2f}\")\n",
    "\n",
    "if bag_accuracy > tfidf_accuracy:\n",
    "    print(\"\\nBest Feature Representation: Bag of Words (CountVectorizer)\")\n",
    "else:\n",
    "    print(\"\\nBest Feature Representation: TF-IDF (TfidfVectorizer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
