{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Example function to clean HTML\n",
    "def clean_html(text):\n",
    "\n",
    "    text = re.sub(r'<(script|style).*?>.*?(<\\/\\1>)', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=\" \")  # Extract only the text\n",
    "    \n",
    "    # Optional: You can also remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage on your dataset\n",
    "data['cleaned_text'] = data['text'].apply(clean_html)\n",
    "\n",
    "# Print the cleaned data\n",
    "print(data[['text', 'cleaned_text']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean HTML and remove stopwords\n",
    "def clean_html_and_remove_stopwords(text):\n",
    "    # Step 1: Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<(script|style).*?>.*?(<\\/\\1>)', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 2: Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=\" \")  # Extract only the text\n",
    "    \n",
    "    # Optional: Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    # Step 4: Remove stopwords\n",
    "    cleaned_words = [word for word in cleaned_text.split() if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the remaining words back into a single string\n",
    "    final_text = ' '.join(cleaned_words)\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "# Example usage on your dataset\n",
    "data['cleaned_text'] = data['text'].apply(clean_html_and_remove_stopwords)\n",
    "\n",
    "# Print the cleaned data without stopwords\n",
    "print(data[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')  # For tokenization\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean HTML, remove stopwords, and lemmatize text\n",
    "def clean_html_and_lemmatize(text):\n",
    "    # Step 1: Remove inline JavaScript/CSS\n",
    "    text = re.sub(r'<(script|style).*?>.*?(<\\/\\1>)', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 2: Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 3: Remove remaining HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=\" \")  # Extract only the text\n",
    "    \n",
    "    # Optional: Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    # Step 4: Tokenize the text\n",
    "    words = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Step 5: Remove stopwords and apply lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the lemmatized words back into a single string\n",
    "    final_text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return final_text\n",
    "\n",
    "# Example usage on your dataset\n",
    "data['cleaned_text'] = data['text'].apply(clean_html_and_lemmatize)\n",
    "\n",
    "# Print the cleaned and lemmatized data\n",
    "print(data[['text', 'cleaned_text']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words in Ham Messages:\n",
      "           frequency\n",
      "state            125\n",
      "pm               113\n",
      "would            107\n",
      "president         99\n",
      "call              94\n",
      "mr                91\n",
      "time              88\n",
      "obama             84\n",
      "2010              82\n",
      "30                81\n",
      "\n",
      "Top 10 Words in Spam Messages:\n",
      "             frequency\n",
      "2e                1857\n",
      "money              979\n",
      "2c                 923\n",
      "account            887\n",
      "bank               799\n",
      "fund               755\n",
      "transaction        549\n",
      "business           511\n",
      "country            504\n",
      "transfer           423\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Assuming the cleaned and lemmatized text is stored in 'cleaned_text' column\n",
    "# and the 'label' column is used to classify spam (1) or ham (0).\n",
    "\n",
    "# Separate spam and ham messages\n",
    "ham_messages = data[data['label'] == 0]['cleaned_text']\n",
    "spam_messages = data[data['label'] == 1]['cleaned_text']\n",
    "\n",
    "# Initialize CountVectorizer and fit on the entire dataset\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(data['cleaned_text'])\n",
    "\n",
    "# Transform ham and spam messages using the same vocabulary\n",
    "ham_word_counts = vectorizer.transform(ham_messages)\n",
    "spam_word_counts = vectorizer.transform(spam_messages)\n",
    "\n",
    "# Sum up word occurrences\n",
    "ham_word_sum = ham_word_counts.sum(axis=0)\n",
    "spam_word_sum = spam_word_counts.sum(axis=0)\n",
    "\n",
    "# Get the words from the vocabulary\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Create a DataFrame for each to easily sort and get top 10 words\n",
    "ham_word_freq = pd.DataFrame(ham_word_sum.A.flatten(), index=words, columns=['frequency']).sort_values(by='frequency', ascending=False)\n",
    "spam_word_freq = pd.DataFrame(spam_word_sum.A.flatten(), index=words, columns=['frequency']).sort_values(by='frequency', ascending=False)\n",
    "\n",
    "# Get the top 10 words for both ham and spam messages\n",
    "top_ham_words = ham_word_freq.head(10)\n",
    "top_spam_words = spam_word_freq.head(10)\n",
    "\n",
    "# Display the top 10 words\n",
    "print(\"Top 10 Words in Ham Messages:\")\n",
    "print(top_ham_words)\n",
    "\n",
    "print(\"\\nTop 10 Words in Spam Messages:\")\n",
    "print(top_spam_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Bag of Words matrix for training data: (800, 23374)\n",
      "Shape of Bag of Words matrix for test data: (200, 23374)\n",
      "Vocabulary: ['00', '000', '000000', '00000e25', '00000e251', '00000eur', '000066', '0000ff', '000m', '000million']\n",
      "Training Data BoW Representation:\n",
      "   00  000  000000  00000e25  00000e251  00000eur  000066  0000ff  000m  \\\n",
      "0   0    0       0         0          0         0       0       0     0   \n",
      "1   0    0       0         0          0         0       0       0     0   \n",
      "2   0    0       0         0          0         0       0       0     0   \n",
      "3   0    0       0         0          0         0       0       0     0   \n",
      "4   0    0       0         0          0         0       0       0     0   \n",
      "\n",
      "   000million  ...  â½s  â½t  â½ta  â½te  â½tica  â½to  â½trangers  â½x60ã  \\\n",
      "0           0  ...    0    0     0     0       0     0           0       0   \n",
      "1           0  ...    0    0     0     0       0     0           0       0   \n",
      "2           0  ...    0    0     0     0       0     0           0       0   \n",
      "3           0  ...    0    0     0     0       0     0           0       0   \n",
      "4           0  ...    0    0     0     0       0     0           0       0   \n",
      "\n",
      "   â½x70  â½ã  \n",
      "0      0    0  \n",
      "1      0    0  \n",
      "2      0    0  \n",
      "3      0    0  \n",
      "4      0    0  \n",
      "\n",
      "[5 rows x 23374 columns]\n",
      "Test Data BoW Representation:\n",
      "   00  000  000000  00000e25  00000e251  00000eur  000066  0000ff  000m  \\\n",
      "0   0    0       0         0          0         0       0       0     0   \n",
      "1   0    0       0         0          0         0       0       0     0   \n",
      "2   0    0       0         0          0         0       0       0     0   \n",
      "3   0    0       0         0          0         0       0       0     0   \n",
      "4   0    0       0         0          0         0       0       0     0   \n",
      "\n",
      "   000million  ...  â½s  â½t  â½ta  â½te  â½tica  â½to  â½trangers  â½x60ã  \\\n",
      "0           0  ...    0    0     0     0       0     0           0       0   \n",
      "1           0  ...    0    0     0     0       0     0           0       0   \n",
      "2           0  ...    0    0     0     0       0     0           0       0   \n",
      "3           0  ...    0    0     0     0       0     0           0       0   \n",
      "4           0  ...    0    0     0     0       0     0           0       0   \n",
      "\n",
      "   â½x70  â½ã  \n",
      "0      0    0  \n",
      "1      0    0  \n",
      "2      0    0  \n",
      "3      0    0  \n",
      "4      0    0  \n",
      "\n",
      "[5 rows x 23374 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training data into Bag of Words\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data (using the same vocabulary from the training data)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Get the vocabulary (the unique words in the dataset)\n",
    "vocab = vectorizer.get_feature_names()  # Use get_feature_names() instead of get_feature_names_out()\n",
    "\n",
    "# Print the shape of the matrices (documents, unique words)\n",
    "print(f\"Shape of Bag of Words matrix for training data: {X_train_bow.shape}\")\n",
    "print(f\"Shape of Bag of Words matrix for test data: {X_test_bow.shape}\")\n",
    "\n",
    "# Show the first 10 words from the vocabulary\n",
    "print(f\"Vocabulary: {vocab[:10]}\")  # Show only the first 10 words for brevity\n",
    "\n",
    "# Convert the training Bag of Words matrix into a DataFrame to visualize\n",
    "bow_df_train = pd.DataFrame(X_train_bow.toarray(), columns=vocab)\n",
    "\n",
    "# Convert the test Bag of Words matrix into a DataFrame to visualize\n",
    "bow_df_test = pd.DataFrame(X_test_bow.toarray(), columns=vocab)\n",
    "\n",
    "# Show the first few rows of the training data Bag of Words matrix\n",
    "print(\"Training Data BoW Representation:\")\n",
    "print(bow_df_train.head())\n",
    "\n",
    "# Show the first few rows of the test data Bag of Words matrix\n",
    "print(\"Test Data BoW Representation:\")\n",
    "print(bow_df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix for training data: (800, 23374)\n",
      "Shape of TF-IDF matrix for test data: (200, 23374)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training data into TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same vocabulary learned from the training data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the shape of the resulting TF-IDF matrices\n",
    "print(f\"Shape of TF-IDF matrix for training data: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of TF-IDF matrix for test data: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Optionally, get the vocabulary (unique words in the dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.50%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89       125\n",
      "           1       0.75      1.00      0.86        75\n",
      "\n",
      "    accuracy                           0.88       200\n",
      "   macro avg       0.88      0.90      0.87       200\n",
      "weighted avg       0.91      0.88      0.88       200\n",
      "\n",
      "Confusion Matrix:\n",
      "[[100  25]\n",
      " [  0  75]]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data (X_train_tfidf and y_train)\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels for the test data (X_test_tfidf)\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model by calculating the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Display the classification report for more detailed evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
