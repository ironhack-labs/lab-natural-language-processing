{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_5712\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(r\"C:\\Users\\alvar\\Downloads\\kg_train.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (800, 2)\n",
      "Test set shape: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 80% training, 20% testing\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes to confirm the split\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Test set shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
      "535    I have not been able to reach oscar this am. W...\n",
      "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
      "557    I can have it announced here on Monday - can't...\n",
      "836    BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_5712\\2726123964.py:6: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "C:\\Users\\alvar\\AppData\\Local\\Temp\\ipykernel_5712\\2726123964.py:16: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  cleaned_text = BeautifulSoup(cleaned_text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Remove JavaScript & CSS\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()  # Removes the script/style tag and content\n",
    "\n",
    "    # Remove HTML comments\n",
    "    cleaned_text = re.sub(r\"<!--.*?-->\", \"\", str(soup), flags=re.DOTALL)\n",
    "\n",
    "    # Remove remaining HTML tags\n",
    "    cleaned_text = BeautifulSoup(cleaned_text, \"html.parser\").get_text()\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Apply cleaning to the dataset\n",
    "train_data[\"text\"] = train_data[\"text\"].apply(clean_html)\n",
    "test_data[\"text\"] = test_data[\"text\"].apply(clean_html)\n",
    "\n",
    "# Show sample cleaned text\n",
    "print(train_data[\"text\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     regards mr nelson smithkindly reply me on my p...\n",
      "535    have not been able to reach oscar this am we a...\n",
      "695    huma abedin bi checking with pat on the k will...\n",
      "557       can have it announced here on monday can today\n",
      "836    bank of africaagence san pedro bp san pedro co...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\b\\w\\b', ' ', text)  # Remove single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)  # Remove single characters from start\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    text = re.sub(r'^b\\s+', '', text)  # Remove prefixed 'b'\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the dataset\n",
    "train_data[\"text\"] = train_data[\"text\"].apply(clean_text)\n",
    "test_data[\"text\"] = test_data[\"text\"].apply(clean_text)\n",
    "\n",
    "# Show sample cleaned text\n",
    "print(train_data[\"text\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     regards mr nelson smithkindly reply private em...\n",
      "535           able reach oscar supposed send pdb receive\n",
      "695    huma abedin bi checking pat k work jack jake r...\n",
      "557                               announced monday today\n",
      "836    bank africaagence san pedro bp san pedro cote ...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords (only needs to be run once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply to datasets\n",
    "train_data['text'] = train_data['text'].apply(remove_stopwords)\n",
    "test_data['text'] = test_data['text'].apply(remove_stopwords)\n",
    "\n",
    "# Show sample cleaned data\n",
    "print(train_data['text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     regard mr nelson smithkindly reply private ema...\n",
      "535           able reach oscar supposed send pdb receive\n",
      "695    huma abedin bi checking pat k work jack jake r...\n",
      "557                               announced monday today\n",
      "836    bank africaagence san pedro bp san pedro cote ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet data (only once)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for lemmatizing text\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply to datasets\n",
    "train_data['text'] = train_data['text'].apply(lemmatize_text)\n",
    "test_data['text'] = test_data['text'].apply(lemmatize_text)\n",
    "\n",
    "# Print sample of cleaned and lemmatized text\n",
    "print(train_data['text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Ham Words:\n",
      " [('would', 93), ('state', 92), ('pm', 88), ('president', 84), ('percent', 76), ('obama', 74), ('call', 73), ('secretary', 71), ('time', 70), ('mr', 70)]\n",
      "\n",
      "Top 10 Spam Words:\n",
      " [('money', 799), ('account', 679), ('bank', 607), ('fund', 569), ('u', 436), ('business', 399), ('transaction', 350), ('country', 342), ('transfer', 329), ('company', 319)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_messages = train_data[train_data['label'] == 0]['text']\n",
    "spam_messages = train_data[train_data['label'] == 1]['text']\n",
    "\n",
    "# Combine all words into one list for each\n",
    "all_ham_words = ' '.join(ham_messages).split()\n",
    "all_spam_words = ' '.join(spam_messages).split()\n",
    "\n",
    "# Count word frequencies\n",
    "ham_word_counts = Counter(all_ham_words)\n",
    "spam_word_counts = Counter(all_spam_words)\n",
    "\n",
    "# Top 10 words\n",
    "top_10_ham = ham_word_counts.most_common(10)\n",
    "top_10_spam = spam_word_counts.most_common(10)\n",
    "\n",
    "# Print results\n",
    "print(\"Top 10 Ham Words:\\n\", top_10_ham)\n",
    "print(\"\\nTop 10 Spam Words:\\n\", top_10_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>regard mr nelson smithkindly reply private ema...</td>\n",
       "      <td>1</td>\n",
       "      <td>regard mr nelson smithkindly reply private ema...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>0</td>\n",
       "      <td>able reach oscar supposed send pdb receive</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>huma abedin bi checking pat k work jack jake r...</td>\n",
       "      <td>0</td>\n",
       "      <td>huma abedin bi checking pat k work jack jake r...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>announced monday today</td>\n",
       "      <td>0</td>\n",
       "      <td>announced monday today</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>bank africaagence san pedro bp san pedro cote ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   regard mr nelson smithkindly reply private ema...      1   \n",
       "535         able reach oscar supposed send pdb receive      0   \n",
       "695  huma abedin bi checking pat k work jack jake r...      0   \n",
       "557                             announced monday today      0   \n",
       "836  bank africaagence san pedro bp san pedro cote ...      1   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "29   regard mr nelson smithkindly reply private ema...           1   \n",
       "535         able reach oscar supposed send pdb receive           1   \n",
       "695  huma abedin bi checking pat k work jack jake r...           1   \n",
       "557                             announced monday today           1   \n",
       "836  bank africaagence san pedro bp san pedro cote ...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0        75  \n",
       "535                 0        42  \n",
       "695                 0        80  \n",
       "557                 0        22  \n",
       "836                 1      1063  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename train/test data to match exercise format\n",
    "data_train = train_data.copy()\n",
    "data_val = test_data.copy()\n",
    "\n",
    "# Assuming 'text' column is the preprocessed one, we rename it for clarity\n",
    "data_train['preprocessed_text'] = data_train['text']\n",
    "data_val['preprocessed_text'] = data_val['text']\n",
    "\n",
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 17235)\n",
      "(200, 17235)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "\n",
    "# Transform validation/test data\n",
    "X_val = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Shape of the BoW matrices (samples x vocabulary size)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Select additional features\n",
    "extra_features_train = data_train[['money_mark', 'suspicious_words', 'text_len']]\n",
    "extra_features_val = data_val[['money_mark', 'suspicious_words', 'text_len']]\n",
    "\n",
    "# Combine BoW and extra features\n",
    "X_train_full = hstack([X_train, extra_features_train])\n",
    "X_val_full = hstack([X_val, extra_features_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (5964, 70194)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_tfidf = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (5964, 70194)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Select extra features\n",
    "extra_features_train = data_train[['money_mark', 'suspicious_words', 'text_len']]\n",
    "extra_features_val = data_val[['money_mark', 'suspicious_words', 'text_len']]\n",
    "\n",
    "# Combine\n",
    "X_train_final = hstack([X_train_tfidf, extra_features_train])\n",
    "X_val_final = hstack([X_val_tfidf, extra_features_val])\n",
    "\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       125\n",
      "           1       0.92      0.89      0.91        75\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.93      0.92      0.92       200\n",
      "weighted avg       0.93      0.93      0.93       200\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[119   6]\n",
      " [  8  67]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_train = data_train['label']\n",
    "y_val = data_val['label']\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val_final)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Training: Logistic Regression\n",
      "==============================\n",
      "Accuracy: 0.9300\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94       125\n",
      "           1       0.92      0.89      0.91        75\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.93      0.92      0.92       200\n",
      "weighted avg       0.93      0.93      0.93       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[119   6]\n",
      " [  8  67]]\n",
      "\n",
      "==============================\n",
      "Training: Multinomial Naive Bayes\n",
      "==============================\n",
      "Accuracy: 0.7600\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.62      0.76       125\n",
      "           1       0.61      0.99      0.76        75\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.80      0.81      0.76       200\n",
      "weighted avg       0.85      0.76      0.76       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[78 47]\n",
      " [ 1 74]]\n",
      "\n",
      "==============================\n",
      "Training: Random Forest\n",
      "==============================\n",
      "Accuracy: 0.9550\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       125\n",
      "           1       0.99      0.89      0.94        75\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.96      0.94      0.95       200\n",
      "weighted avg       0.96      0.95      0.95       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[124   1]\n",
      " [  8  67]]\n",
      "\n",
      "==============================\n",
      "Training: Linear SVM\n",
      "==============================\n",
      "Accuracy: 0.9350\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       125\n",
      "           1       0.94      0.88      0.91        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.94      0.92      0.93       200\n",
      "weighted avg       0.94      0.94      0.93       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[121   4]\n",
      " [  9  66]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Linear SVM\": LinearSVC(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*30}\\nTraining: {model_name}\\n{'='*30}\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_final, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_val_final)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
