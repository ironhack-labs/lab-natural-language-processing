{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larry\\AppData\\Local\\Temp\\ipykernel_17368\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                           Will do.      0\n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                fyi      0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    1000 non-null   object\n",
      " 1   label   1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Display a summary of the dataset\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800\n",
      "Test set size: 200\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'],  # Input feature\n",
    "    data['label'], # Target labels\n",
    "    test_size=0.2, # 20% of the data for testing\n",
    "    random_state=42 # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Display the sizes of each partition\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Training Data:\n",
      "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
      "535    I have not been able to reach oscar this am. W...\n",
      "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
      "557    I can have it announced here on Monday - can't...\n",
      "836        BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Cleaned Test Data:\n",
      "521    Dear Sir=2C I wish you go through this offer t...\n",
      "737    To take your mind off the Balkans for a second...\n",
      "740                         Pls keep the updates coming!\n",
      "660    CHRIST BETHEL HOSPITAL11 RUE ABOBOTE,ABIDJANIV...\n",
      "411    sbwhoeopFriday February 5 2010 7:11 AMHRe: Bra...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript/CSS: Any text within <script> or <style> tags\n",
    "    text = re.sub(r'<(script|style).*?>.*?(</\\1>)', '', text, flags=re.S)\n",
    "\n",
    "    # Remove HTML comments: <!-- ... -->\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.S)\n",
    "\n",
    "    # Remove remaining HTML tags: <...>\n",
    "    text = re.sub(r'<.*?>', '', text, flags=re.S)\n",
    "\n",
    "    # Return cleaned text\n",
    "    return text\n",
    "\n",
    "# Clean HTML from training and test sets\n",
    "X_train_cleaned = X_train.apply(clean_html)\n",
    "X_test_cleaned = X_test.apply(clean_html)\n",
    "\n",
    "# Display the first 5 rows of the cleaned training set to verify\n",
    "print(\"Cleaned Training Data:\")\n",
    "print(X_train_cleaned.head())\n",
    "\n",
    "print(\"\\nCleaned Test Data:\")\n",
    "print(X_test_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Further Cleaned Training Data:\n",
      "29     regards mr nelson smithkindly reply me on my p...\n",
      "535    have not been able to reach oscar this am we a...\n",
      "695    huma abedin bim checking with pat on the will ...\n",
      "557      can have it announced here on monday cant today\n",
      "836    bank of africaagence san pedro bp san pedro co...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Further Cleaned Test Data:\n",
      "521    dear sirc wish you go through this offer to co...\n",
      "737    to take your mind off the balkans for second s...\n",
      "740                          pls keep the updates coming\n",
      "660    christ bethel hospital rue aboboteabidjanivory...\n",
      "411    sbwhoeopfriday february amhre bravo brava issu...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "def additional_cleaning(text):\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\s*\\w\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = re.sub(r'\\bb\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the additional cleaning to the training and test sets\n",
    "X_train_cleaned = X_train_cleaned.apply(additional_cleaning)\n",
    "X_test_cleaned = X_test_cleaned.apply(additional_cleaning)\n",
    "\n",
    "# Display the first 5 rows of the cleaned training set\n",
    "print(\"Further Cleaned Training Data:\")\n",
    "print(X_train_cleaned.head())\n",
    "\n",
    "print(\"\\nFurther Cleaned Test Data:\")\n",
    "print(X_test_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\larry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\larry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data after Stopwords Removal:\n",
      "29     regards mr nelson smithkindly reply private em...\n",
      "535           able reach oscar supposed send pdb receive\n",
      "695    huma abedin bim checking pat work jack jake re...\n",
      "557                          announced monday cant today\n",
      "836    bank africaagence san pedro bp san pedro cote ...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Test Data after Stopwords Removal:\n",
      "521    dear sirc wish go offer consider partnerei mre...\n",
      "737    take mind balkans second see great plug global...\n",
      "740                              pls keep updates coming\n",
      "660    christ bethel hospital rue aboboteabidjanivory...\n",
      "411    sbwhoeopfriday february amhre bravo brava issu...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Set of English stopwords\n",
    "    words = word_tokenize(text)  # Tokenize the text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)  # Reconstruct the text without stopwords\n",
    "\n",
    "# Apply the function to the training and test sets\n",
    "X_train_cleaned = X_train_cleaned.apply(remove_stopwords)\n",
    "X_test_cleaned = X_test_cleaned.apply(remove_stopwords)\n",
    "\n",
    "# Display the first 5 rows of the cleaned training set\n",
    "print(\"Training Data after Stopwords Removal:\")\n",
    "print(X_train_cleaned.head())\n",
    "\n",
    "print(\"\\nTest Data after Stopwords Removal:\")\n",
    "print(X_test_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\larry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data after Lemmatization:\n",
      "29     regard mr nelson smithkindly reply private ema...\n",
      "535           able reach oscar supposed send pdb receive\n",
      "695    huma abedin bim checking pat work jack jake re...\n",
      "557                          announced monday cant today\n",
      "836    bank africaagence san pedro bp san pedro cote ...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Test Data after Lemmatization:\n",
      "521    dear sirc wish go offer consider partnerei mre...\n",
      "737    take mind balkan second see great plug global ...\n",
      "740                               pls keep update coming\n",
      "660    christ bethel hospital rue aboboteabidjanivory...\n",
      "411    sbwhoeopfriday february amhre bravo brava issu...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')  # Download WordNet for lemmatization\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to perform lemmatization\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)  # Tokenize the text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
    "    return ' '.join(lemmatized_words)  # Reconstruct the text with lemmatized words\n",
    "\n",
    "# Apply the function to the training and test sets\n",
    "X_train_cleaned = X_train_cleaned.apply(lemmatize_text)\n",
    "X_test_cleaned = X_test_cleaned.apply(lemmatize_text)\n",
    "\n",
    "# Display the first 5 rows of the lemmatized training set\n",
    "print(\"Training Data after Lemmatization:\")\n",
    "print(X_train_cleaned.head())\n",
    "\n",
    "print(\"\\nTest Data after Lemmatization:\")\n",
    "print(X_test_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in spam messages:\n",
      "[('money', 795), ('account', 662), ('bank', 606), ('fund', 565), ('u', 444), ('business', 393), ('transaction', 347), ('country', 337), ('transfer', 326), ('company', 318)]\n",
      "\n",
      "Top 10 words in ham messages:\n",
      "[('u', 99), ('would', 93), ('state', 92), ('pm', 89), ('president', 84), ('percent', 76), ('call', 73), ('secretary', 71), ('time', 70), ('mr', 70)]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Separate spam and ham messages\n",
    "spam_messages = X_train_cleaned[y_train == 1]\n",
    "ham_messages = X_train_cleaned[y_train == 0]\n",
    "\n",
    "# Define a function to extract the top N words\n",
    "def get_top_n_words(messages, n=10):\n",
    "    all_words = ' '.join(messages).split()  # Combine all messages and split into words\n",
    "    word_counts = Counter(all_words)  # Count the frequency of each word\n",
    "    return word_counts.most_common(n)  # Return the top N words and their counts\n",
    "\n",
    "# Get top 10 words for spam and ham messages\n",
    "top_spam_words = get_top_n_words(spam_messages, n=10)\n",
    "top_ham_words = get_top_n_words(ham_messages, n=10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 words in spam messages:\")\n",
    "print(top_spam_words)\n",
    "\n",
    "print(\"\\nTop 10 words in ham messages:\")\n",
    "print(top_ham_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set with additional features:\n",
      "                                   preprocessed_text  money_mark  \\\n",
      "0  regard mr nelson smithkindly reply private ema...           1   \n",
      "1         able reach oscar supposed send pdb receive           1   \n",
      "2  huma abedin bim checking pat work jack jake re...           1   \n",
      "3                        announced monday cant today           1   \n",
      "4  bank africaagence san pedro bp san pedro cote ...           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 0        75  \n",
      "1                 0        42  \n",
      "2                 0        79  \n",
      "3                 0        27  \n",
      "4                 1      1067  \n",
      "\n",
      "Validation set with additional features:\n",
      "                                   preprocessed_text  money_mark  \\\n",
      "0  dear sirc wish go offer consider partnerei mre...           1   \n",
      "1  take mind balkan second see great plug global ...           1   \n",
      "2                             pls keep update coming           1   \n",
      "3  christ bethel hospital rue aboboteabidjanivory...           1   \n",
      "4  sbwhoeopfriday february amhre bravo brava issu...           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 1      1326  \n",
      "1                 0       332  \n",
      "2                 0        22  \n",
      "3                 1      1266  \n",
      "4                 0       161  \n"
     ]
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "# Convert Series to lists explicitly before creating DataFrames\n",
    "data_train = pd.DataFrame({\"preprocessed_text\": X_train_cleaned.tolist()})\n",
    "data_val = pd.DataFrame({\"preprocessed_text\": X_test_cleaned.tolist()})\n",
    "\n",
    "# Add features to the training set\n",
    "data_train[\"money_mark\"] = data_train[\"preprocessed_text\"].str.contains(money_symbol_list, regex=True).astype(int)\n",
    "data_train[\"suspicious_words\"] = data_train[\"preprocessed_text\"].str.contains(suspicious_words, regex=True).astype(int)\n",
    "data_train[\"text_len\"] = data_train[\"preprocessed_text\"].apply(len)\n",
    "\n",
    "# Add features to the validation set\n",
    "data_val[\"money_mark\"] = data_val[\"preprocessed_text\"].str.contains(money_symbol_list, regex=True).astype(int)\n",
    "data_val[\"suspicious_words\"] = data_val[\"preprocessed_text\"].str.contains(suspicious_words, regex=True).astype(int)\n",
    "data_val[\"text_len\"] = data_val[\"preprocessed_text\"].apply(len)\n",
    "\n",
    "# Display the first rows of the training data with the new features\n",
    "print(\"Training set with additional features:\")\n",
    "print(data_train.head())\n",
    "\n",
    "print(\"\\nValidation set with additional features:\")\n",
    "print(data_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in training data:\n",
      "[('money', 799), ('account', 668), ('bank', 615), ('fund', 566), ('business', 415), ('country', 384), ('transaction', 347), ('company', 338), ('transfer', 327), ('million', 324)]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training set\n",
    "X_train_counts = vectorizer.fit_transform(data_train[\"preprocessed_text\"])\n",
    "\n",
    "# Transform the validation set using the same vectorizer\n",
    "X_val_counts = vectorizer.transform(data_val[\"preprocessed_text\"])\n",
    "\n",
    "# Get the feature names (words) and their frequencies for the training set\n",
    "word_counts_train = X_train_counts.toarray().sum(axis=0)  # Sum frequencies across all documents in train set\n",
    "word_frequencies_train = dict(zip(vectorizer.get_feature_names_out(), word_counts_train))\n",
    "\n",
    "# Sort and get the top 10 words for the training set\n",
    "top_words_train = sorted(word_frequencies_train.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 words in training data:\")\n",
    "print(top_words_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data (TF-IDF): (800, 16931)\n",
      "Shape of validation data (TF-IDF): (200, 16931)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train[\"preprocessed_text\"])\n",
    "\n",
    "# Transform the validation set using the same vectorizer\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val[\"preprocessed_text\"])\n",
    "\n",
    "# Print the shapes of the vectorized datasets\n",
    "print(\"Shape of training data (TF-IDF):\", X_train_tfidf.shape)\n",
    "print(\"Shape of validation data (TF-IDF):\", X_val_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in data_train:\n",
      "Index(['preprocessed_text', 'money_mark', 'suspicious_words', 'text_len'], dtype='object')\n",
      "\n",
      "First rows of data_train:\n",
      "                                   preprocessed_text  money_mark  \\\n",
      "0  regard mr nelson smithkindly reply private ema...           1   \n",
      "1         able reach oscar supposed send pdb receive           1   \n",
      "2  huma abedin bim checking pat work jack jake re...           1   \n",
      "3                        announced monday cant today           1   \n",
      "4  bank africaagence san pedro bp san pedro cote ...           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 0        75  \n",
      "1                 0        42  \n",
      "2                 0        79  \n",
      "3                 0        27  \n",
      "4                 1      1067  \n",
      "\n",
      "The column 'label' is missing in data_train!\n"
     ]
    }
   ],
   "source": [
    "# Check the columns in data_train\n",
    "print(\"Columns in data_train:\")\n",
    "print(data_train.columns)\n",
    "\n",
    "# Display the first few rows to inspect the structure\n",
    "print(\"\\nFirst rows of data_train:\")\n",
    "print(data_train.head())\n",
    "\n",
    "# Check if the labels are present in the DataFrame\n",
    "if \"label\" not in data_train.columns:\n",
    "    print(\"\\nThe column 'label' is missing in data_train!\")\n",
    "else:\n",
    "    print(\"\\nThe column 'label' exists in data_train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated columns in data_train:\n",
      "Index(['preprocessed_text', 'money_mark', 'suspicious_words', 'text_len',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "\n",
      "First rows of data_train after adding label:\n",
      "                                   preprocessed_text  money_mark  \\\n",
      "0  regard mr nelson smithkindly reply private ema...           1   \n",
      "1         able reach oscar supposed send pdb receive           1   \n",
      "2  huma abedin bim checking pat work jack jake re...           1   \n",
      "3                        announced monday cant today           1   \n",
      "4  bank africaagence san pedro bp san pedro cote ...           1   \n",
      "\n",
      "   suspicious_words  text_len  label  \n",
      "0                 0        75      1  \n",
      "1                 0        42      0  \n",
      "2                 0        79      0  \n",
      "3                 0        27      0  \n",
      "4                 1      1067      1  \n",
      "\n",
      "Updated columns in data_val:\n",
      "Index(['preprocessed_text', 'money_mark', 'suspicious_words', 'text_len',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "\n",
      "First rows of data_val after adding label:\n",
      "                                   preprocessed_text  money_mark  \\\n",
      "0  dear sirc wish go offer consider partnerei mre...           1   \n",
      "1  take mind balkan second see great plug global ...           1   \n",
      "2                             pls keep update coming           1   \n",
      "3  christ bethel hospital rue aboboteabidjanivory...           1   \n",
      "4  sbwhoeopfriday february amhre bravo brava issu...           1   \n",
      "\n",
      "   suspicious_words  text_len  label  \n",
      "0                 1      1326      1  \n",
      "1                 0       332      0  \n",
      "2                 0        22      0  \n",
      "3                 1      1266      1  \n",
      "4                 0       161      0  \n"
     ]
    }
   ],
   "source": [
    "# Add the labels to the training and validation DataFrames\n",
    "data_train[\"label\"] = y_train.values\n",
    "data_val[\"label\"] = y_test.values\n",
    "\n",
    "# Verify the structure of data_train and data_val\n",
    "print(\"Updated columns in data_train:\")\n",
    "print(data_train.columns)\n",
    "\n",
    "print(\"\\nFirst rows of data_train after adding label:\")\n",
    "print(data_train.head())\n",
    "\n",
    "print(\"\\nUpdated columns in data_val:\")\n",
    "print(data_val.columns)\n",
    "\n",
    "print(\"\\nFirst rows of data_val after adding label:\")\n",
    "print(data_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on the validation set:\n",
      "[1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 0\n",
      " 0 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the classifier\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "model.fit(X_train_tfidf, data_train[\"label\"])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_val_tfidf)\n",
    "\n",
    "# Print the predictions (optional)\n",
    "print(\"Predictions on the validation set:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       125\n",
      "           1       0.85      0.99      0.91        75\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.92      0.94      0.93       200\n",
      "weighted avg       0.94      0.93      0.93       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(data_val[\"label\"], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Generate a classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(data_val[\"label\"], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TfidfVectorizer:\n",
      "Accuracy: 0.96\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       125\n",
      "           1       0.94      0.96      0.95        75\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.96      0.96      0.96       200\n",
      "weighted avg       0.96      0.96      0.96       200\n",
      "\n",
      "\n",
      "Using CountVectorizer:\n",
      "Accuracy: 0.90\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       125\n",
      "           1       0.79      0.97      0.87        75\n",
      "\n",
      "    accuracy                           0.90       200\n",
      "   macro avg       0.89      0.91      0.89       200\n",
      "weighted avg       0.91      0.90      0.90       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.895"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define function to train and evaluate the classifier\n",
    "def train_and_evaluate(vectorizer):\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "    # Train the classifier\n",
    "    pipeline.fit(data_train[\"preprocessed_text\"], data_train[\"label\"])\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = pipeline.predict(data_val[\"preprocessed_text\"])\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(data_val[\"label\"], y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(data_val[\"label\"], y_pred))\n",
    "    return accuracy\n",
    "\n",
    "# Try different vectorizers\n",
    "print(\"Using TfidfVectorizer:\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, ngram_range=(1, 2))\n",
    "train_and_evaluate(tfidf_vectorizer)\n",
    "\n",
    "print(\"\\nUsing CountVectorizer:\")\n",
    "count_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "train_and_evaluate(count_vectorizer)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
