{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"kg_train.csv\", encoding = 'latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.fillna(\"\", inplace = True)\n",
    "\n",
    "display(data)\n",
    "\n",
    "X = data['text']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "def remove_inline_js_css(text):\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL) # Regex to remove <script>...</script> tags and their content\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL) # Regex to remove <style>...</style> tags and their content\n",
    "    return text\n",
    "\n",
    "def remove_html_comments(text):\n",
    "    return re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL) # Regex to match HTML comments\n",
    "\n",
    "# Remove inline_JS_CSS\n",
    "X_train = data['text'].apply(remove_inline_js_css)\n",
    "X_test =  data['text'].apply(remove_inline_js_css)\n",
    "\n",
    "# Apply the function to the 'text' column to remove HTML comments\n",
    "X_train = X_train.apply(remove_html_comments)\n",
    "X_test =  X_test.apply(remove_html_comments)\n",
    "\n",
    "X_train.head()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def remove_others(text):\n",
    "    text = re.sub(r'[^A-Za-z\\s]+', '', text, flags=re.DOTALL) # Regex to remove all the special characters and numbers\n",
    "    text = re.sub(r'\\b\\w\\b', '', text, flags=re.DOTALL) # Regex to remove all single characters\n",
    "    text = re.sub(r' {2,}', ' ', text, flags=re.DOTALL) # Regex to substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\bb', '', text, flags=re.DOTALL) # Regex to remove prefixed 'b'\n",
    "    text = re.sub(r'\\s\\s+', ' ', text, flags=re.DOTALL) # Substitute multiple spaces with single space\n",
    "\n",
    "    return text.lower() # Convert to Lowercase\n",
    "\n",
    "# Remove inline_JS_CSS\n",
    "X_train = X_train.apply(remove_others)\n",
    "X_test =  X_test.apply(remove_others)\n",
    "\n",
    "X_train.head()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def removal(text): # Alex Code\n",
    "    stop_words = set(stopwords.words('english')) # stopwords\n",
    "\n",
    "    words = text.split()  # Split for tokenization\n",
    "    words = [word for word in words if word not in stop_words] # Process everything except the stopwords\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Remove stopwords\n",
    "X_train = X_train.apply(removal)\n",
    "X_test =  X_test.apply(removal)\n",
    "\n",
    "X_train.head()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def lemmatization(text): # Alex Code\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    words = text.split()  # Split for tokenization\n",
    "    words = [lemmatizer.lemmatize(word) for word in words] # Lemmatize\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Lemmatization\n",
    "X_train = X_train.apply(lemmatization)\n",
    "X_test =  X_test.apply(lemmatization)\n",
    "\n",
    "X_train.head()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "# ham 0, spam = 1\n",
    "\n",
    "all_words = ' '.join(X_train)\n",
    "\n",
    "# Tokenize words\n",
    "word_list = all_words.split()\n",
    "\n",
    "word_counts = Counter(word_list)\n",
    "print(word_counts)\n",
    "\n",
    "print(\"\\nTo 10 words:\")\n",
    "for i in word_counts.most_common(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data['money_mark'] = X_train.str.contains(money_simbol_list) * 1\n",
    "data['suspicious_words'] = X_train.str.contains(suspicious_words) * 1\n",
    "data['text_len'] = X_train.apply(lambda x: len(x))\n",
    "\n",
    "'''\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "'''\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    " \n",
    "vectorizer.fit(X_train)\n",
    " \n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(X_train)\n",
    " \n",
    "# Summarizing the Encoded Texts\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "tf_idf_vectorizer = TfidfVectorizer() # TfidfVectorizer(ngram_range = (1, 2), max_features = 10000)\n",
    "\n",
    "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_tf_idf = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# Classifier will make prediction on my data.\n",
    "\n",
    "# Train the classifier (MultinomialNB)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tf_idf, y)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_test_tf_idf)\n",
    "\n",
    "print(\"Predictions:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
