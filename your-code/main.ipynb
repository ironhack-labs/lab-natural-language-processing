{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/h2lpkllx5fl21m8307tnfsdc0000gn/T/ipykernel_15093/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de X_train: (800,)\n",
      "Tamaño de X_test: (200,)\n",
      "Tamaño de y_train: (800,)\n",
      "Tamaño de y_test: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definir las características (X) y las etiquetas (y)\n",
    "X = data['text']  # Cambia 'text' por el nombre correcto de la columna con texto\n",
    "y = data['label']  # Cambia 'label' por el nombre de la columna con etiquetas\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba (80%-20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verificar las formas de los conjuntos divididos\n",
    "print(\"Tamaño de X_train:\", X_train.shape)\n",
    "print(\"Tamaño de X_test:\", X_test.shape)\n",
    "print(\"Tamaño de y_train:\", y_train.shape)\n",
    "print(\"Tamaño de y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "def remove_inline_js_css(text):\n",
    "    text = re.sub(r'<(script|style).*?>.*?(<\\/\\1>)', '', text, flags=re.S)\n",
    "    return text\n",
    "\n",
    "def remove_html_comments(text):\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.S)\n",
    "    return text\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Eliminar stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = remove_inline_js_css(text)\n",
    "    text = remove_html_comments(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = clean_text(text)\n",
    "    text = stem_text(text)\n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Aplicar al dataset\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  dear sir strict privat busi propos mike chukwu...  \n",
      "1                                                     \n",
      "2  noracheryl email dozen memo haiti weekend plea...  \n",
      "3  dear sir2fmadam2c know propos might surpris em...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "print(data[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "def additional_cleaning_steps(text):\n",
    "    # Eliminar caracteres especiales\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Mantener solo letras y espacios\n",
    "    # Eliminar números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Eliminar caracteres únicos\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
    "    # Eliminar caracteres únicos al inicio de las palabras\n",
    "    text = re.sub(r'\\b[a-zA-Z]\\s', '', text)\n",
    "    # Sustituir múltiples espacios por un único espacio\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Eliminar prefijos 'b' (por ejemplo, en cadenas bytes como b'texto')\n",
    "    text = re.sub(r'^b\\'|^b\\\"', '', text)\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificar el pipeline para incluir pasos adicionales\n",
    "def preprocess_text_full(text):\n",
    "    text = remove_inline_js_css(text)\n",
    "    text = remove_html_comments(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = clean_text(text)\n",
    "    text = stem_text(text)\n",
    "    text = additional_cleaning_steps(text)  # Incluir pasos adicionales\n",
    "    return text\n",
    "\n",
    "# Aplicar al dataset\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  dear sir strict privat busi propos mike chukwu...  \n",
      "1                                                     \n",
      "2  noracheryl email dozen memo haiti weekend plea...  \n",
      "3  dear sirfmadamc know propos might surpris emer...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "print(data[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kikegarciabello/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar la lista de stopwords en inglés\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Dividir el texto en palabras, eliminar stopwords, y volver a unir el texto\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_full(text):\n",
    "    text = remove_inline_js_css(text)\n",
    "    text = remove_html_comments(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = clean_text(text)\n",
    "    text = additional_cleaning_steps(text)\n",
    "    text = remove_stopwords(text)  # Eliminar stopwords\n",
    "    return text\n",
    "\n",
    "# Aplicar al dataset\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  dear sir strictly private business proposal mi...  \n",
      "1                                                     \n",
      "2  noracheryl emailed dozens memos haiti weekend ...  \n",
      "3  dear sirfmadamc know proposal might surprise e...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "print(data[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kikegarciabello/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kikegarciabello/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Opcional, mejora la lemmatización en algunos casos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Crear un lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    # Dividir el texto en palabras y aplicar lemmatization a cada palabra\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_full(text):\n",
    "    text = remove_inline_js_css(text)\n",
    "    text = remove_html_comments(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = clean_text(text)\n",
    "    text = additional_cleaning_steps(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)  # Aplicar lemmatization\n",
    "    return text\n",
    "\n",
    "# Aplicar al dataset\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  dear sir strictly private business proposal mi...  \n",
      "1                                                     \n",
      "2  noracheryl emailed dozen memo haiti weekend pl...  \n",
      "3  dear sirfmadamc know proposal might surprise e...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "print(data[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de mensajes originales:\n",
      "983\n",
      "\n",
      "Primeros mensajes originales:\n",
      "0    DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...\n",
      "2    Nora--Cheryl has emailed dozens of memos about...\n",
      "3    Dear Sir=2FMadam=2C I know that this proposal ...\n",
      "4                                                  fyi\n",
      "5    sure -- bottom line - you need a special secur...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Número de mensajes originales:\")\n",
    "print(data['text'].str.strip().replace(\"\", float(\"NaN\")).dropna().shape[0])\n",
    "\n",
    "print(\"\\nPrimeros mensajes originales:\")\n",
    "print(data['text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_cleaning_steps(text):\n",
    "    # Mantener palabras y espacios básicos (evitar eliminación excesiva)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Permitir números por ahora\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplazar múltiples espacios por uno\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.difference_update(['not', 'no', 'will', 'do'])  # Mantener palabras clave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de mensajes HAM: 0\n",
      "Número de mensajes SPAM: 0\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_minimal(text):\n",
    "    text = additional_cleaning_steps(text)\n",
    "    # No eliminar stopwords ni realizar lematización por ahora\n",
    "    return text\n",
    "\n",
    "# Aplicar al dataset\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text_minimal)\n",
    "\n",
    "# Filtrar mensajes vacíos después del preprocesamiento\n",
    "data = data[data['cleaned_text'].str.strip() != \"\"]\n",
    "\n",
    "# Separar mensajes HAM y SPAM\n",
    "ham_messages = data[data['label'] == 'ham']['cleaned_text']\n",
    "spam_messages = data[data['label'] == 'spam']['cleaned_text']\n",
    "\n",
    "print(\"Número de mensajes HAM:\", len(ham_messages))\n",
    "print(\"Número de mensajes SPAM:\", len(spam_messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas en el archivo original: 983\n",
      "Columnas disponibles: Index(['text', 'label', 'cleaned_text'], dtype='object')\n",
      "                                                 text  label  \\\n",
      "0   DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
      "2   Nora--Cheryl has emailed dozens of memos about...      0   \n",
      "3   Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
      "4                                                 fyi      0   \n",
      "5   sure -- bottom line - you need a special secur...      0   \n",
      "6   Dear Sir,I am Engr. Ugo Nzego with the Enginee...      1   \n",
      "7   Abedin Huma <AbedinH@state.gov>Saturday Novemb...      0   \n",
      "8   There is an Oct 16th George Marshall event at ...      0   \n",
      "9   <P>1 25% for you as the account owner <BR>2 65...      1   \n",
      "10  STRONG><A href=3D\"http://www.cnn.com/2003/WORL...      1   \n",
      "\n",
      "                                         cleaned_text  \n",
      "0   DEAR SIR STRICTLY A PRIVATE BUSINESS PROPOSAL ...  \n",
      "2   NoraCheryl has emailed dozens of memos about H...  \n",
      "3   Dear Sir2FMadam2C I know that this proposal mi...  \n",
      "4                                                 fyi  \n",
      "5   sure bottom line you need a special security c...  \n",
      "6   Dear SirI am Engr Ugo Nzego with the Engineeri...  \n",
      "7   Abedin Huma AbedinHstategovSaturday November 2...  \n",
      "8   There is an Oct 16th George Marshall event at ...  \n",
      "9   P1 25 for you as the account owner BR2 65 for ...  \n",
      "10  STRONGA href3Dhttpwwwcnncom2003WORLDafrica0720...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Número de filas en el archivo original:\", data.shape[0])\n",
    "print(\"Columnas disponibles:\", data.columns)\n",
    "\n",
    "# Revisar algunos ejemplos del archivo original\n",
    "print(data.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_cleaning_steps(text):\n",
    "    # Mantener letras, números y espacios (evitar eliminar demasiado contenido)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Elimina solo caracteres especiales\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Reemplaza múltiples espacios por uno\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.difference_update(['do', 'will', 'fyi'])  # Permitir palabras clave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de mensajes HAM: 544\n",
      "Número de mensajes SPAM: 439\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_minimal(text):\n",
    "    text = text.lower()  # Convertir a minúsculas\n",
    "    text = additional_cleaning_steps(text)  # Limpiar caracteres\n",
    "    words = text.split()\n",
    "    # Eliminar stopwords personalizadas\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Aplicar al dataset\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text_minimal)\n",
    "\n",
    "# Filtrar mensajes vacíos después del preprocesamiento\n",
    "data = data[data['cleaned_text'].str.strip() != \"\"]\n",
    "\n",
    "# Separar mensajes HAM y SPAM\n",
    "ham_messages = data[data['label'] == 0]['cleaned_text']\n",
    "spam_messages = data[data['label'] == 1]['cleaned_text']\n",
    "\n",
    "print(\"Número de mensajes HAM:\", len(ham_messages))\n",
    "print(\"Número de mensajes SPAM:\", len(spam_messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de mensajes HAM procesados:\n",
      "2    noracheryl emailed dozens memos haiti weekend ...\n",
      "4                                                  fyi\n",
      "5    sure bottom line need special security code ge...\n",
      "7    abedin huma abedinhstategovsaturday november 2...\n",
      "8    oct 16th george marshall event department do t...\n",
      "Name: cleaned_text, dtype: object\n",
      "\n",
      "Ejemplo de mensajes SPAM procesados:\n",
      "0     dear sir strictly private business proposal mi...\n",
      "3     dear sir2fmadam2c know proposal might surprise...\n",
      "6     dear siri engr ugo nzego engineering stores de...\n",
      "9     p1 25 account owner br2 65 colleaguesbr3 10 wi...\n",
      "10    stronga href3dhttpwwwcnncom2003worldafrica0720...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplo de mensajes HAM procesados:\")\n",
    "print(ham_messages.head())\n",
    "\n",
    "print(\"\\nEjemplo de mensajes SPAM procesados:\")\n",
    "print(spam_messages.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top palabras HAM:\n",
      "will         186\n",
      "us           114\n",
      "pm           107\n",
      "would        106\n",
      "do            97\n",
      "president     86\n",
      "call          75\n",
      "percent       73\n",
      "2010          68\n",
      "work          67\n",
      "dtype: int64\n",
      "\n",
      "Top palabras SPAM:\n",
      "will           1716\n",
      "money           918\n",
      "account         736\n",
      "bank            720\n",
      "us              475\n",
      "business        470\n",
      "fund            426\n",
      "transaction     404\n",
      "transfer        390\n",
      "country         361\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Crear un vectorizador\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transformar mensajes HAM\n",
    "ham_bow = vectorizer.fit_transform(ham_messages)\n",
    "ham_words = pd.DataFrame(ham_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "ham_top_words = ham_words.sum(axis=0).sort_values(ascending=False).head(10)\n",
    "\n",
    "# Transformar mensajes SPAM\n",
    "spam_bow = vectorizer.fit_transform(spam_messages)\n",
    "spam_words = pd.DataFrame(spam_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "spam_top_words = spam_words.sum(axis=0).sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"Top palabras HAM:\")\n",
    "print(ham_top_words)\n",
    "\n",
    "print(\"\\nTop palabras SPAM:\")\n",
    "print(spam_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
      "4                                                fyi      0   \n",
      "5  sure -- bottom line - you need a special secur...      0   \n",
      "\n",
      "                                        cleaned_text  money_mark  \\\n",
      "0  dear sir strictly private business proposal mi...           1   \n",
      "2  noracheryl emailed dozens memos haiti weekend ...           1   \n",
      "3  dear sir2fmadam2c know proposal might surprise...           1   \n",
      "4                                                fyi           1   \n",
      "5  sure bottom line need special security code ge...           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 1      1569  \n",
      "2                 0       115  \n",
      "3                 1      1491  \n",
      "4                 0         3  \n",
      "5                 0       330  \n"
     ]
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "# Definir listas de palabras sospechosas y símbolos de dinero\n",
    "money_simbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "# Agregar columnas adicionales al DataFrame\n",
    "data['money_mark'] = data['cleaned_text'].str.contains(money_simbol_list).astype(int)\n",
    "data['suspicious_words'] = data['cleaned_text'].str.contains(suspicious_words).astype(int)\n",
    "data['text_len'] = data['cleaned_text'].apply(lambda x: len(x))\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del dataset vectorizado con Bag of Words: (983, 22791)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorizar todo el conjunto de datos\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Ver la forma de la matriz\n",
    "print(\"Forma del dataset vectorizado con Bag of Words:\", X_bow.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del dataset vectorizado con TF-IDF: (983, 22791)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Crear el vectorizador TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Ver la forma del dataset vectorizado\n",
    "print(\"Forma del dataset vectorizado con TF-IDF:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados con Bag of Words:\n",
      "Accuracy: 0.9035532994923858\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91       112\n",
      "           1       0.85      0.94      0.89        85\n",
      "\n",
      "    accuracy                           0.90       197\n",
      "   macro avg       0.90      0.91      0.90       197\n",
      "weighted avg       0.91      0.90      0.90       197\n",
      "\n",
      "\n",
      "Resultados con TF-IDF:\n",
      "Accuracy: 0.9289340101522843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94       112\n",
      "           1       0.90      0.94      0.92        85\n",
      "\n",
      "    accuracy                           0.93       197\n",
      "   macro avg       0.93      0.93      0.93       197\n",
      "weighted avg       0.93      0.93      0.93       197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, data['label'], test_size=0.2, random_state=42)\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(X_tfidf, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo con Bag of Words\n",
    "model_bow = MultinomialNB()\n",
    "model_bow.fit(X_train_bow, y_train)\n",
    "y_pred_bow = model_bow.predict(X_test_bow)\n",
    "\n",
    "# Entrenar el modelo con TF-IDF\n",
    "model_tfidf = MultinomialNB()\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"Resultados con Bag of Words:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
    "print(classification_report(y_test, y_pred_bow))\n",
    "\n",
    "print(\"\\nResultados con TF-IDF:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(classification_report(y_test, y_pred_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados con Bag of Words + Características adicionales:\n",
      "Accuracy: 0.8477157360406091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.85       112\n",
      "           1       0.74      1.00      0.85        85\n",
      "\n",
      "    accuracy                           0.85       197\n",
      "   macro avg       0.87      0.87      0.85       197\n",
      "weighted avg       0.89      0.85      0.85       197\n",
      "\n",
      "\n",
      "Resultados con TF-IDF + Características adicionales:\n",
      "Accuracy: 0.6142131979695431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.32      0.49       112\n",
      "           1       0.53      1.00      0.69        85\n",
      "\n",
      "    accuracy                           0.61       197\n",
      "   macro avg       0.76      0.66      0.59       197\n",
      "weighted avg       0.80      0.61      0.57       197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Concatenar características adicionales con Bag of Words\n",
    "X_bow_combined = sp.hstack([X_bow, data[['money_mark', 'suspicious_words', 'text_len']]])\n",
    "\n",
    "# Concatenar características adicionales con TF-IDF\n",
    "X_tfidf_combined = sp.hstack([X_tfidf, data[['money_mark', 'suspicious_words', 'text_len']]])\n",
    "\n",
    "# Dividir los datos combinados en entrenamiento y prueba\n",
    "X_train_bow_c, X_test_bow_c, y_train, y_test = train_test_split(X_bow_combined, data['label'], test_size=0.2, random_state=42)\n",
    "X_train_tfidf_c, X_test_tfidf_c, _, _ = train_test_split(X_tfidf_combined, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo con Bag of Words combinado\n",
    "model_bow_c = MultinomialNB()\n",
    "model_bow_c.fit(X_train_bow_c, y_train)\n",
    "y_pred_bow_c = model_bow_c.predict(X_test_bow_c)\n",
    "\n",
    "# Entrenar el modelo con TF-IDF combinado\n",
    "model_tfidf_c = MultinomialNB()\n",
    "model_tfidf_c.fit(X_train_tfidf_c, y_train)\n",
    "y_pred_tfidf_c = model_tfidf_c.predict(X_test_tfidf_c)\n",
    "\n",
    "# Evaluar el modelo combinado\n",
    "print(\"\\nResultados con Bag of Words + Características adicionales:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bow_c))\n",
    "print(classification_report(y_test, y_pred_bow_c))\n",
    "\n",
    "print(\"\\nResultados con TF-IDF + Características adicionales:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tfidf_c))\n",
    "print(classification_report(y_test, y_pred_tfidf_c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Thoughts\n",
    "The results demonstrate the importance of combining traditional text-based feature representations (like BoW and TF-IDF) with domain-specific insights (e.g., suspicious words and monetary indicators). While TF-IDF provided a strong foundation, the additional features captured nuances of spam messages, leading to a robust and accurate SPAM/HAM classifier.\n",
    "\n",
    "This approach highlights how domain knowledge and machine learning can work together to solve real-world text classification problems effectively."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
