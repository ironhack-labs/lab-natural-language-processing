{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_13128\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn  as sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
      "1                                           Will do.      0\n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
      "4                                                fyi      0\n"
     ]
    }
   ],
   "source": [
    "#Your Code:\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You Code here:\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_val = train_test_split(data, test_size=0.3, random_state=42, stratify=data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#First we need to import some libraries first:\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, this is a test message with some HTML content. This includes unnecessary words like '' and ''.\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#Sample HTML content\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <p>Hello, this is a <b>test</b> message with some <a href=\"#\">HTML</a> content.</p>\n",
    "        <p>This includes unnecessary words like 'remove' and 'clean'.</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "# Step 1: Remove HTML Tags\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "cleaned_text = soup.get_text()\n",
    "\n",
    "# Step 2: Define the list of unwanted words/phrases to remove\n",
    "unwanted_words = [\"remove\", \"clean\"]\n",
    "\n",
    "# Step 3: Remove unwanted words using regex\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(unwanted_words) + r')\\b', re.IGNORECASE)\n",
    "cleaned_text = pattern.sub('', cleaned_text)\n",
    "\n",
    "# Step 4: Optional - Remove extra spaces left after word removal\n",
    "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "# Print the cleaned content\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_13128\\69838825.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, \"html.parser\")\n",
      "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_13128\\69838825.py:8: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...  \n",
      "1                                           Will do.  \n",
      "2  Nora--Cheryl has emailed dozens of memos about...  \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#Funtion to remove Html Content and unwanted words\n",
    "def clean_html_and_remove_words(html_content, unwanted_words):\n",
    "    #Remove Tags\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    cleaned_text = soup.get_text()\n",
    "    #Remove unwanted words\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(unwanted_words) + r')\\b', re.IGNORECASE)\n",
    "    cleaned_text = pattern.sub('', cleaned_text)\n",
    "    #Remove extra spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "#Define the list of unwanted words/phrases to remove\n",
    "unwanted_words = [\"remove\", \"clean\", \"unnecessary\"]\n",
    "\n",
    "#Define a function that will apply the cleaning process to a column of text\n",
    "data['cleaned_text'] = data['text'].apply(lambda x: clean_html_and_remove_words(x, unwanted_words))\n",
    "\n",
    "#Print the cleaned content\n",
    "print(data[['text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                   no_stopwords_text  \n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...  \n",
      "1                                           Will do.  \n",
      "2  Nora--Cheryl emailed dozens memos Haiti weeken...  \n",
      "3  Dear Sir=2FMadam=2C I know proposal might surp...  \n",
      "4                                                fyi  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') \n",
    "\n",
    "#Get list of stop words in English\n",
    "stop_wrods = set(stopwords.words('english'))\n",
    "\n",
    "#Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filterd_words = [word for word in words if word not in stop_wrods]\n",
    "    return ' '.join(filterd_words)\n",
    "\n",
    "#Apply the function to the clean text column\n",
    "data['no_stopwords_text'] = data['cleaned_text'].apply(remove_stopwords)\n",
    "\n",
    "#Show the original and the cleaned text and stopwords-free text\n",
    "print(data[['text', 'cleaned_text', 'no_stopwords_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install --upgrade spacy typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                   no_stopwords_text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl emailed dozens memos Haiti weeken...   \n",
      "3  Dear Sir=2FMadam=2C I know proposal might surp...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  DEAR SIR , STRICTLY A PRIVATE BUSINESS PROPOSA...  \n",
      "1                                          Will do .  \n",
      "2  Nora -- Cheryl emailed dozen memo Haiti weeken...  \n",
      "3  Dear Sir=2FMadam=2C I know proposal might surp...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK datasets (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform lemmatization on text\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the lemmatized words back into a single string\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply the lemmatization function to the 'no_stopwords_text' column\n",
    "data['lemmatized_text'] = data['no_stopwords_text'].apply(lemmatize_text)\n",
    "\n",
    "# Show the original, cleaned, stopword-free, and lemmatized text\n",
    "print(data[['text', 'cleaned_text', 'no_stopwords_text', 'lemmatized_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Words in Ham Messages:\n",
      "           word  frequency\n",
      "4896         pm        113\n",
      "6044      state        110\n",
      "5029  president         99\n",
      "4266         mr         86\n",
      "4477      obama         84\n",
      "153        2010         82\n",
      "243          30         81\n",
      "6481       time         80\n",
      "4812    percent         77\n",
      "7069       work         75\n",
      "\n",
      "Top 10 Words in Spam Messages:\n",
      "              word  frequency\n",
      "1265            2e       1856\n",
      "12356        money        973\n",
      "1096            2c        923\n",
      "3374       account        866\n",
      "5037          bank        794\n",
      "8865          fund        710\n",
      "17315  transaction        544\n",
      "5554      business        511\n",
      "6549       country        485\n",
      "17330     transfer        419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Separate spam and ham messages\n",
    "ham_messages = data[data['label'] == 0]['lemmatized_text']\n",
    "spam_messages = data[data['label'] == 1]['lemmatized_text']\n",
    "\n",
    "# Initialize separate CountVectorizers for ham and spam\n",
    "ham_vectorizer = CountVectorizer(stop_words='english')\n",
    "spam_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit the vectorizers on ham and spam messages separately and get word counts\n",
    "ham_word_counts = ham_vectorizer.fit_transform(ham_messages)\n",
    "spam_word_counts = spam_vectorizer.fit_transform(spam_messages)\n",
    "\n",
    "# Sum up the word counts\n",
    "ham_word_freq = ham_word_counts.sum(axis=0).A1\n",
    "spam_word_freq = spam_word_counts.sum(axis=0).A1\n",
    "\n",
    "# Get words from the vectorizer vocabulary\n",
    "ham_vocab = ham_vectorizer.get_feature_names_out()\n",
    "spam_vocab = spam_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create DataFrames for easier analysis\n",
    "ham_freq_df = pd.DataFrame({'word': ham_vocab, 'frequency': ham_word_freq}).sort_values(by='frequency', ascending=False)\n",
    "spam_freq_df = pd.DataFrame({'word': spam_vocab, 'frequency': spam_word_freq}).sort_values(by='frequency', ascending=False)\n",
    "\n",
    "# Get the top 10 words for ham and spam\n",
    "top_10_ham = ham_freq_df.head(10)\n",
    "top_10_spam = spam_freq_df.head(10)\n",
    "\n",
    "# Display the top 10 words for ham and spam\n",
    "print(\"Top 10 Words in Ham Messages:\")\n",
    "print(top_10_ham)\n",
    "\n",
    "print(\"\\nTop 10 Words in Spam Messages:\")\n",
    "print(top_10_spam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new colum with preprocessed text\n",
    "data['preprocessed_text'] = data['lemmatized_text']\n",
    "#Do the spliting of the data for train and validation\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>no_stopwords_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>1</td>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>-- -- -- -- -- - REGARDS , MR NELSON SMITH.KIN...</td>\n",
       "      <td>-- -- -- -- -- - REGARDS , MR NELSON SMITH.KIN...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>0</td>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>I able reach oscar am. We supposed send pdb 11...</td>\n",
       "      <td>I able reach oscar am . We supposed send pdb 1...</td>\n",
       "      <td>I able reach oscar am . We supposed send pdb 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>; Huma Abedin B6I'm checking Pat 50k work Jack...</td>\n",
       "      <td>; Huma Abedin B6I 'm checking Pat 50k work Jac...</td>\n",
       "      <td>; Huma Abedin B6I 'm checking Pat 50k work Jac...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>I announced Monday - can't today</td>\n",
       "      <td>I announced Monday - ca n't today</td>\n",
       "      <td>I announced Monday - ca n't today</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>1</td>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...</td>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...</td>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...</td>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
       "535  I have not been able to reach oscar this am. W...      0   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
       "557  I can have it announced here on Monday - can't...      0   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
       "\n",
       "                                          cleaned_text  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...   \n",
       "535  I have not been able to reach oscar this am. W...   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...   \n",
       "557  I can have it announced here on Monday - can't...   \n",
       "836  BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...   \n",
       "\n",
       "                                     no_stopwords_text  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...   \n",
       "535  I able reach oscar am. We supposed send pdb 11...   \n",
       "695  ; Huma Abedin B6I'm checking Pat 50k work Jack...   \n",
       "557                   I announced Monday - can't today   \n",
       "836  BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...   \n",
       "\n",
       "                                       lemmatized_text  \\\n",
       "29   -- -- -- -- -- - REGARDS , MR NELSON SMITH.KIN...   \n",
       "535  I able reach oscar am . We supposed send pdb 1...   \n",
       "695  ; Huma Abedin B6I 'm checking Pat 50k work Jac...   \n",
       "557                  I announced Monday - ca n't today   \n",
       "836  BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "29   -- -- -- -- -- - REGARDS , MR NELSON SMITH.KIN...           1   \n",
       "535  I able reach oscar am . We supposed send pdb 1...           1   \n",
       "695  ; Huma Abedin B6I 'm checking Pat 50k work Jac...           1   \n",
       "557                  I announced Monday - ca n't today           1   \n",
       "836  BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0       116  \n",
       "535                 0        68  \n",
       "695                 0        91  \n",
       "557                 0        33  \n",
       "836                 1      1322  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.00      0.00      0.00         1\n",
      "        spam       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\diego\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\diego\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\diego\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset (Replace with your actual dataset)\n",
    "data = pd.DataFrame({\n",
    "    'preprocessed_text': [\n",
    "        \"Congratulations! You've won a $1000 gift card. Click here to claim.\",\n",
    "        \"Meeting at 5pm today.\",\n",
    "        \"Get rich quickly by investing in this one-time offer.\",\n",
    "        \"Lunch at 12? Let me know if you're free.\",\n",
    "        \"You’ve been selected for a special prize! Don’t miss out!\"\n",
    "    ],\n",
    "    'label': ['spam', 'ham', 'spam', 'ham', 'spam']\n",
    "})\n",
    "\n",
    "# Step 1: Define money symbols and suspicious words\n",
    "money_simbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "# Step 2: Add 'money_mark' feature (1 if money symbol present, 0 otherwise)\n",
    "data['money_mark'] = data['preprocessed_text'].str.contains(money_simbol_list, case=False, regex=True) * 1\n",
    "\n",
    "# Step 3: Add 'suspicious_words' feature (1 if suspicious word present, 0 otherwise)\n",
    "data['suspicious_words'] = data['preprocessed_text'].str.contains(suspicious_words, case=False, regex=True) * 1\n",
    "\n",
    "# Step 4: Add 'text_len' feature (length of the text)\n",
    "data['text_len'] = data['preprocessed_text'].apply(len)\n",
    "\n",
    "# Step 5: Vectorize the text using CountVectorizer (Bag of Words model)\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))  # Use unigrams\n",
    "X_bag_of_words = vectorizer.fit_transform(data['preprocessed_text'])\n",
    "\n",
    "# Step 6: Combine Bag of Words with extra features (money_mark, suspicious_words, text_len)\n",
    "extra_features = data[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "combined_features = np.hstack((X_bag_of_words.toarray(), extra_features))  # Combine BoW and extra features\n",
    "\n",
    "# Step 7: Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features, data['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 8: Train the Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Predict and evaluate the model\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Re-load the test data (since it may not be in memory)\n",
    "test_data = pd.read_csv('../data/kg_test.csv')\n",
    "\n",
    "# Step 2: Transform the test data using the same TF-IDF vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Step 3: Predict using the trained model\n",
    "test_predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "# Step 4: Output the predictions to a CSV file\n",
    "output = pd.DataFrame({'text': test_data['text'], 'prediction': test_predictions})\n",
    "output.to_csv('../data/test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions have been saved to test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data = pd.read_csv('../data/kg_train.csv')\n",
    "# Step 1: Data preparation\n",
    "X = train_data['text']\n",
    "y = train_data['label']\n",
    "\n",
    "# Step 2: Train-test split (if not already done)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Initializing TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Step 4: Fitting TF-IDF on training data and transforming it\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Step 5: Transforming validation data\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "# Now X_train_tfidf and X_val_tfidf are ready for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9731768650461022\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       674\n",
      "           1       1.00      0.94      0.97       519\n",
      "\n",
      "    accuracy                           0.97      1193\n",
      "   macro avg       0.98      0.97      0.97      1193\n",
      "weighted avg       0.97      0.97      0.97      1193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for model training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 6: Initialize Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Step 7: Fit the model on the TF-IDF transformed training data\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Step 8: Predict on the validation set\n",
    "y_pred = model.predict(X_val_tfidf)\n",
    "\n",
    "# Step 9: Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions have been saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Transform the test data using the same TF-IDF vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Step 11: Predict using the trained model\n",
    "test_predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "# Step 12: Output the predictions (e.g., to a CSV file)\n",
    "output = pd.DataFrame({'text': test_data['text'], 'prediction': test_predictions})\n",
    "output.to_csv('../data/test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions have been saved to test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9731768650461022\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       674\n",
      "           1       1.00      0.94      0.97       519\n",
      "\n",
      "    accuracy                           0.97      1193\n",
      "   macro avg       0.98      0.97      0.97      1193\n",
      "weighted avg       0.97      0.97      0.97      1193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Step 1: Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 2: Prepare data (if not already done)\n",
    "X = train_data['text']\n",
    "y = train_data['label']\n",
    "\n",
    "# Step 3: Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize TF-IDF Vectorizer and fit on the training data\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "# Step 5: Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Step 6: Train the classifier on the training data\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Step 7: Predict on the validation data\n",
    "y_pred = model.predict(X_val_tfidf)\n",
    "\n",
    "# Step 8: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred)\n",
    "\n",
    "# Output the evaluation results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
