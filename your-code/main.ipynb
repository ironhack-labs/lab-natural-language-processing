{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speed up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 800, Test size: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = data['text']  # Email text\n",
    "y = data['label']  # Spam/Ham labels\n",
    "\n",
    "# Split into 80% train, 20% test (adjust test_size if needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript/CSS:\n",
    "    text = re.sub(r'<(script|style).*?>.*?</\\1>', '', text, flags=re.DOTALL)\n",
    "    # Remove HTML comments:\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    # Remove remaining HTML tags:\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(clean_html)\n",
    "X_test = X_test.apply(clean_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    # Remove single characters from the start of the text\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove the 'b' character prefix from byte conversions (if present)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "X_train = X_train.apply(remove_stopwords)\n",
    "X_test = X_test.apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rospi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rospi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rospi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442    dear good day hope fine cdear writting mail du...\n",
      "962    mr henry kaborethe chief auditor inchargeforei...\n",
      "971                                                     \n",
      "190    desk dr adamu ismalerauditing accounting manag...\n",
      "551    dear friend name loi estrada wife mr josephest...\n",
      "495    amr ecollince addoattn ai need urgent assistan...\n",
      "845                   pls send call sheet equadoran clue\n",
      "821                                          yes arrange\n",
      "409          fyi limit haiti email major fyi information\n",
      "794    mr zuhair idris jordan kuwait bankcredit depar...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # WordNet data for lemmatization\n",
    "nltk.download('punkt')    # Tokenizer\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "X_train = X_train.apply(lemmatize_text)\n",
    "X_test = X_test.apply(lemmatize_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "847                                     rec printed desk\n",
      "182                                                  fyi\n",
      "372    depart private residence en route white house ...\n",
      "619    devon drive independence lay enugu state urgen...\n",
      "443    sbwhoeoptuesday december pmany truth account s...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y_train.unique())\n",
    "print(X_train.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Vectorize with Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Create a DataFrame from the Bag of Words\n",
    "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "bow_df['label'] = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in HAM messages:\n",
      "state        97\n",
      "president    95\n",
      "would        92\n",
      "mr           90\n",
      "obama        80\n",
      "percent      80\n",
      "call         77\n",
      "work         72\n",
      "time         70\n",
      "one          69\n",
      "dtype: int64\n",
      "\n",
      "Top 10 words in SPAM messages:\n",
      "money          756\n",
      "account        670\n",
      "bank           614\n",
      "fund           600\n",
      "transaction    432\n",
      "business       412\n",
      "country        397\n",
      "nbsp           387\n",
      "mr             383\n",
      "million        361\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ham_words = bow_df[bow_df['label'] == 0].drop('label', axis=1).sum().sort_values(ascending=False)\n",
    "spam_words = bow_df[bow_df['label'] == 1].drop('label', axis=1).sum().sort_values(ascending=False)\n",
    "\n",
    "# Get top 10\n",
    "top_ham = ham_words.head(10)\n",
    "top_spam = spam_words.head(10)\n",
    "\n",
    "print(\"Top 10 words in HAM messages:\")\n",
    "print(top_ham)\n",
    "\n",
    "print(\"\\nTop 10 words in SPAM messages:\")\n",
    "print(top_spam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split original data into train and validation sets\n",
    "data_train, data_val = train_test_split(\n",
    "    data, test_size=0.2, random_state=42, stratify=data['label']\n",
    ")\n",
    "\n",
    "# Apply preprocessing pipeline to create 'preprocessed_text'\n",
    "data_train['preprocessed_text'] = (\n",
    "    data_train['text']\n",
    "    .apply(preprocess_text)        # your cleaning function\n",
    "    .apply(remove_stopwords)       # stopwords removal\n",
    "    .apply(lemmatize_text)         # lemmatization\n",
    ")\n",
    "\n",
    "data_val['preprocessed_text'] = (\n",
    "    data_val['text']\n",
    "    .apply(preprocess_text)\n",
    "    .apply(remove_stopwords)\n",
    "    .apply(lemmatize_text)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear  c good day hope fine  cdear am writting ...</td>\n",
       "      <td>dear good day hope fine cdear writting mail du...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>1</td>\n",
       "      <td>from mr henry kaborethe chief auditor incharge...</td>\n",
       "      <td>mr henry kaborethe chief auditor inchargeforei...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>will do</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>1</td>\n",
       "      <td>from the desk of dr adamu ismalerauditing and ...</td>\n",
       "      <td>desk dr adamu ismalerauditing accounting manag...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>1</td>\n",
       "      <td>dear friend  my name is loi   estrada the wife...</td>\n",
       "      <td>dear friend name loi estrada wife mr josephest...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "442  Dear=2C Good day hope fine=2Cdear am writting ...      1   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...      1   \n",
       "971                                           Will do.      0   \n",
       "190  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...      1   \n",
       "551  Dear Friend, My name is LOI C.ESTRADA,The wife...      1   \n",
       "\n",
       "                                            clean_text  \\\n",
       "442  dear  c good day hope fine  cdear am writting ...   \n",
       "962  from mr henry kaborethe chief auditor incharge...   \n",
       "971                                            will do   \n",
       "190  from the desk of dr adamu ismalerauditing and ...   \n",
       "551  dear friend  my name is loi   estrada the wife...   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "442  dear good day hope fine cdear writting mail du...           1   \n",
       "962  mr henry kaborethe chief auditor inchargeforei...           1   \n",
       "971                                                              1   \n",
       "190  desk dr adamu ismalerauditing accounting manag...           1   \n",
       "551  dear friend name loi estrada wife mr josephest...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "442                 1      1002  \n",
       "962                 1      1946  \n",
       "971                 0         0  \n",
       "190                 1       383  \n",
       "551                 1      1475  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5000\n",
      "Shape of train BoW matrix: (800, 5000)\n",
      "Shape of val BoW matrix: (200, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000)  # limit vocab size for speed\n",
    "\n",
    "# Fit on training preprocessed texts and transform both train and test sets\n",
    "X_train_bow = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_bow = vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Shape of train BoW matrix: {X_train_bow.shape}\")\n",
    "print(f\"Shape of val BoW matrix: {X_val_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF train shape: (800, 5000)\n",
      "TF-IDF validation shape: (200, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # limit vocab size for speed\n",
    "\n",
    "# Fit on training data and transform both train and validation sets\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# Print shapes\n",
    "print(f\"TF-IDF train shape: {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF validation shape: {X_val_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.995\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       112\n",
      "           1       1.00      0.99      0.99        88\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       1.00      0.99      0.99       200\n",
      "weighted avg       1.00      0.99      0.99       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train on the TF-IDF features\n",
    "clf.fit(X_train_tfidf, data_train['label'])\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = clf.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(data_val['label'], y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(data_val['label'], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       112\n",
      "           1       0.96      1.00      0.98        88\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.98      0.98      0.98       200\n",
      "weighted avg       0.98      0.98      0.98       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the classifier with default parameters\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train on BoW features (or your chosen representation)\n",
    "clf.fit(X_train_bow, data_train['label'])\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = clf.predict(X_val_bow)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(data_val['label'], y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(data_val['label'], y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with engineered features: 0.99\n",
      "\n",
      "Classification Report with engineered features:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       112\n",
      "           1       0.98      1.00      0.99        88\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.99      0.99      0.99       200\n",
      "weighted avg       0.99      0.99      0.99       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Extract engineered features as numpy arrays or sparse matrices\n",
    "engineered_train = data_train[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "engineered_val = data_val[['money_mark', 'suspicious_words', 'text_len']].values\n",
    "\n",
    "# Combine with BoW or TF-IDF sparse matrices\n",
    "X_train_combined = hstack([X_train_bow, engineered_train])\n",
    "X_val_combined = hstack([X_val_bow, engineered_val])\n",
    "\n",
    "# Train classifier on combined features\n",
    "clf.fit(X_train_combined, data_train['label'])\n",
    "y_pred_combined = clf.predict(X_val_combined)\n",
    "\n",
    "print(\"Accuracy with engineered features:\", accuracy_score(data_val['label'], y_pred_combined))\n",
    "print(\"\\nClassification Report with engineered features:\\n\", classification_report(data_val['label'], y_pred_combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
