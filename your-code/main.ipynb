{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\AppData\\Local\\Temp\\ipykernel_8268\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(r\"C:\\Users\\Gebruiker\\Desktop\\IronHack\\Labs\\week 7\\lab-natural-language-processing\\data\\kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "#data = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sure -- bottom line - you need a special secur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dear Sir,I am Engr. Ugo Nzego with the Enginee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abedin Huma &lt;AbedinH@state.gov&gt;Saturday Novemb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There is an Oct 16th George Marshall event at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;P&gt;1 25% for you as the account owner &lt;BR&gt;2 65...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>STRONG&gt;&lt;A href=3D\"http://www.cnn.com/2003/WORL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dear Friend,My name is Edward Moore QC.Princip...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Compliment, How are you doing today, Hope you ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Who wrote it?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>accident. &lt;BR&gt;&amp;nbsp;&lt;BR&gt;On further investigat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Email from EricBackground for you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(\"REMITTANCE OF $15 M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Abidjan Cote D'Ivoire=20Dear,=20It is my pleas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;DIV&gt;&lt;BR&gt;&lt;STRONG&gt;I am Hon. Mr. Hope Sithole on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                            Will do.      0\n",
       "2   Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3   Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                 fyi      0\n",
       "5   sure -- bottom line - you need a special secur...      0\n",
       "6   Dear Sir,I am Engr. Ugo Nzego with the Enginee...      1\n",
       "7   Abedin Huma <AbedinH@state.gov>Saturday Novemb...      0\n",
       "8   There is an Oct 16th George Marshall event at ...      0\n",
       "9   <P>1 25% for you as the account owner <BR>2 65...      1\n",
       "10  STRONG><A href=3D\"http://www.cnn.com/2003/WORL...      1\n",
       "11  Dear Friend,My name is Edward Moore QC.Princip...      1\n",
       "12  Compliment, How are you doing today, Hope you ...      1\n",
       "13                                      Who wrote it?      0\n",
       "14   accident. <BR>&nbsp;<BR>On further investigat...      1\n",
       "15                  Email from EricBackground for you      0\n",
       "16                                                ...      1\n",
       "17                           (\"REMITTANCE OF $15 M...      1\n",
       "18  Abidjan Cote D'Ivoire=20Dear,=20It is my pleas...      1\n",
       "19  <DIV><BR><STRONG>I am Hon. Mr. Hope Sithole on...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(r\"C:\\Users\\Gebruiker\\Desktop\\IronHack\\Labs\\week 7\\lab-natural-language-processing\\data\\kg_test.csv\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data.head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (200,), Test size: (200,)\n"
     ]
    }
   ],
   "source": [
    "X_train = data_test['text']\n",
    "Y_train = data_test['label']\n",
    "\n",
    "x_test = data_test['text']\n",
    "\n",
    "# confirm shape\n",
    "print(f\"Train size: {X_train.shape}, Test size: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\gebruiker\\anaconda3\\envs\\nlp-env\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gebruiker\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\gebruiker\\anaconda3\\envs\\nlp-env\\lib\\site-packages (from beautifulsoup4) (4.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw HTML = messy text with lots of tags\n",
    "# Parsing HTML = converting that mess into a readable tree structure, - Reading and analyzing HTML code so a program can understand its structure and extract useful information\n",
    "# Parser = tool that does this (e.g., html.parser, lxml, html5lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html(text):\n",
    "    # Remove HTML comments\n",
    "    no_comments = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    clean = re.sub(r'<[^>]+>', '', no_comments)\n",
    "    # remove remaining HTML tags - removes any remaining HTML tags (e.g., <p>, <br>, <strong>) using regex\n",
    "    #Regex breakdown: <: Start of an HTML tag, [^>]+: One or more characters that are NOT >, >: End of the tag, Leaves only plain text behind.\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    clean = re.sub(r'\\s+', ' ', clean).strip()   \n",
    "    # replace multiple spaces with a single space, \\s+ matches any number of whitespace characters (spaces, newlines, tabs), replaces them with a single space,\n",
    "    #.strip() removes leading/trailing spaces, makes the result tidy and consistent\n",
    "    \n",
    "    return clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\AppData\\Local\\Temp\\ipykernel_8268\\1638377541.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_test['cleaned_text'] = data_test['text'].apply(remove_html)\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning function\n",
    "data['cleaned_text'] = data['text'].apply(remove_html)\n",
    "data_test['cleaned_text'] = data_test['text'].apply(remove_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  label  \n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1  \n",
      "1                                           Will do.      0  \n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0  \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1  \n",
      "4                                                fyi      0  \n",
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...  \n",
      "1                                           Will do.  \n",
      "2  Nora--Cheryl has emailed dozens of memos about...  \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "# Optional: preview\n",
    "print(data[['text', 'cleaned_text', 'label']].head())       # OK for training data\n",
    "print(data_test[['text', 'cleaned_text']].head())           # NO 'label' in test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\AppData\\Local\\Temp\\ipykernel_8268\\3924885812.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_test['preprocessed_text'] = data_test['cleaned_text'].apply(preprocess_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>dear sir strictly private business proposal am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>Will do.</td>\n",
       "      <td>will do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>noracheryl has emailed dozens of memos about h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>dear sirfmadamc know that this proposal might ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
       "1                                           Will do.   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
       "1                                           Will do.   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  dear sir strictly private business proposal am...  \n",
       "1                                            will do  \n",
       "2  noracheryl has emailed dozens of memos about h...  \n",
       "3  dear sirfmadamc know that this proposal might ...  \n",
       "4                                                fyi  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # keep only letters and whitespace\n",
    "\n",
    "    # Remove all single characters (e.g., \"a\", \"b\")\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # Remove single characters from the start (e.g., \"a This\" → \"This\")\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "\n",
    "    # Remove prefix \"b\" (in case of byte strings)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "\n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    return text.lower().strip()\n",
    "\n",
    "# Apply preprocessing function to cleaned text\n",
    "data['preprocessed_text'] = data['cleaned_text'].apply(preprocess_text)\n",
    "data_test['preprocessed_text'] = data_test['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "data[['text', 'cleaned_text', 'preprocessed_text']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gebruiker\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Remove stop words\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    words = text.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gebruiker\\AppData\\Local\\Temp\\ipykernel_8268\\1164293317.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_test['lemmatized_text'] = data_test['preprocessed_text'].apply(lemmatize_words)\n"
     ]
    }
   ],
   "source": [
    "data['lemmatized_text'] = data['preprocessed_text'].apply(lemmatize_words)\n",
    "data_test['lemmatized_text'] = data_test['preprocessed_text'].apply(lemmatize_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert processed_docs to strings\n",
    "texts = [' '.join(doc) for doc in 'lemmatized_text']\n",
    "\n",
    "texts = data['lemmatized_text'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "# Create the Bag of Words model\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Convert to DataFrame so we can see the word counts\n",
    "word_counts = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "word_counts['label'] = labels  # add the labels back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 HAM words:\n",
      " zurich            0\n",
      "aac               0\n",
      "aaclocated        0\n",
      "aae               0\n",
      "aag               0\n",
      "aaronovitchon     0\n",
      "abacha            0\n",
      "zimbabwei         0\n",
      "zimbabwec         0\n",
      "zimbabwebefore    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Find top 10 words in ham\n",
    "ham = word_counts[word_counts['label'] == 'ham']\n",
    "top_ham = ham.drop(columns='label').sum().sort_values(ascending=False).head(10)\n",
    "print(\"Top 10 HAM words:\\n\", top_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 SPAM words:\n",
      " zurich            0\n",
      "aac               0\n",
      "aaclocated        0\n",
      "aae               0\n",
      "aag               0\n",
      "aaronovitchon     0\n",
      "abacha            0\n",
      "zimbabwei         0\n",
      "zimbabwec         0\n",
      "zimbabwebefore    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 SPAM words\n",
    "top_spam = word_counts[word_counts['label'] == 'spam'].drop(columns='label').sum().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 SPAM words:\\n\", top_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(r\"C:\\Users\\Gebruiker\\Desktop\\IronHack\\Labs\\week 7\\lab-natural-language-processing\\data\\kg_train.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_test.copy()\n",
    "#adding features to the cleaned data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>dear sir strictly private business proposal am...</td>\n",
       "      <td>dear sir strictly private business proposal am...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>Will do.</td>\n",
       "      <td>will do</td>\n",
       "      <td>will do</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>noracheryl has emailed dozens of memos about h...</td>\n",
       "      <td>noracheryl ha emailed dozen of memo about hait...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>dear sirfmadamc know that this proposal might ...</td>\n",
       "      <td>dear sirfmadamc know that this proposal might ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "      <td>fyi</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
       "1                                           Will do.      0   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
       "4                                                fyi      0   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
       "1                                           Will do.   \n",
       "2  Nora--Cheryl has emailed dozens of memos about...   \n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                   preprocessed_text  \\\n",
       "0  dear sir strictly private business proposal am...   \n",
       "1                                            will do   \n",
       "2  noracheryl has emailed dozens of memos about h...   \n",
       "3  dear sirfmadamc know that this proposal might ...   \n",
       "4                                                fyi   \n",
       "\n",
       "                                     lemmatized_text  money_mark  \\\n",
       "0  dear sir strictly private business proposal am...           1   \n",
       "1                                            will do           1   \n",
       "2  noracheryl ha emailed dozen of memo about hait...           1   \n",
       "3  dear sirfmadamc know that this proposal might ...           1   \n",
       "4                                                fyi           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 1      2195  \n",
       "1                 0         7  \n",
       "2                 0       192  \n",
       "3                 1      2017  \n",
       "4                 0         3  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "data_train = vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "data_val = vectorizer.transform(data_val['preprocessed_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aac' 'aaclocated' 'aae' ... 'zumadirector' 'zumae' 'zurich']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorized dataset shape: (1000, 20356)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorize all dataset\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['preprocessed_text'])  # Use full dataset or split \n",
    "\n",
    "# Print shape of the vectorized dataset\n",
    "print(\"TF-IDF vectorized dataset shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95       125\n",
      "           1       0.97      0.87      0.92        75\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.95      0.93      0.93       200\n",
      "weighted avg       0.94      0.94      0.94       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_text = data['preprocessed_text']\n",
    "y = data['label']\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
