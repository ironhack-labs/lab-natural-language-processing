{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html(text):\n",
    "    # remove html tags + inline css/js + comments\n",
    "    cleaned_text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # remove html comments\n",
    "    return re.sub(r'<!--.*?-->', \"\", cleaned_text)\n",
    "\n",
    "X_train_cleaned = X_train.apply(remove_html)\n",
    "X_test_cleaned = X_test.apply(remove_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    # remove prefixed 'b'\n",
    "    cleaned_text = text.lstrip(\"b\")\n",
    "    \n",
    "    # convert to lower\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    for char in string.punctuation:\n",
    "        cleaned_text = cleaned_text.replace(char, ' ')\n",
    "\n",
    "    # remove digits\n",
    "    for digit in string.digits:\n",
    "        cleaned_text = cleaned_text.replace(digit, ' ')\n",
    "\n",
    "    # split sentences into words + remove multiple spaces\n",
    "    words = cleaned_text.split()\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return \" \".join(words).strip()\n",
    "\n",
    "\n",
    "X_train_cleaned = X_train_cleaned.apply(clean_text)\n",
    "X_test_cleaned = X_test_cleaned.apply(clean_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done in previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "X_train_tokenized = [[lemmatizer.lemmatize(word) for word in word_tokenize(sentence)] for sentence in X_train_cleaned]\n",
    "X_test_tokenized = [[lemmatizer.lemmatize(word) for word in word_tokenize(sentence)] for sentence in X_test_cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbsp nbsp: 235\n",
      "next kin: 221\n",
      "united state: 115\n",
      "security company: 110\n",
      "bank account: 102\n",
      "state dollar: 100\n",
      "fax number: 100\n",
      "south africa: 85\n",
      "hundred thousand: 83\n",
      "five hundred: 74\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=1, ngram_range=(2,2))\n",
    "\n",
    "X_train_sentences = [\" \".join(words) for words in X_train_tokenized]\n",
    "X_test_sentences = [\" \".join(words) for words in X_test_tokenized]\n",
    "\n",
    "word_freq_matrix = count_vectorizer.fit_transform(X_train_sentences)\n",
    "\n",
    "frequencies = np.asarray(word_freq_matrix.sum(axis=0)).flatten()\n",
    "vocab_items = count_vectorizer.vocabulary_\n",
    "reverse_vocab = {v: k for k, v in vocab_items.items()}\n",
    "\n",
    "# Output frequency and bigram together\n",
    "top_10 = sorted(zip(frequencies, [reverse_vocab[i] for i in range(len(frequencies))]), reverse=True)[:10]\n",
    "for freq, bigram in top_10:\n",
    "    print(f\"{bigram}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thursday february pm sbwhoeopre urgent good ne...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fyimills cheryl monday april pmhfw decision up...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dear cgood day ei know message come suprise co...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goodday thanks response email today business i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   preprocessed_text  money_mark  \\\n",
       "0  thursday february pm sbwhoeopre urgent good ne...           1   \n",
       "1  fyimills cheryl monday april pmhfw decision up...           1   \n",
       "2  dear cgood day ei know message come suprise co...           1   \n",
       "3                                               like           1   \n",
       "4  goodday thanks response email today business i...           1   \n",
       "\n",
       "   suspicious_words  text_len  \n",
       "0                 0        85  \n",
       "1                 0        50  \n",
       "2                 1       865  \n",
       "3                 0         4  \n",
       "4                 1       468  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train = pd.DataFrame({'preprocessed_text': X_train_sentences})\n",
    "data_val = pd.DataFrame({'preprocessed_text': X_test_sentences})\n",
    "\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words simply counts the frequency of each word in the text and creates a vector from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 71586)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(ngram_range=(2, 2), min_df=1)\n",
    "tf_idf_matrix = tf_idf_vectorizer.fit_transform(X_train_sentences)\n",
    "\n",
    "tf_idf_matrix.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = MultinomialNB().fit(tf_idf_matrix, y_train)\n",
    "y_pred = model.predict(tf_idf_vectorizer.transform(X_test_sentences))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Count Unigram ===\n",
      "Accuracy:  0.9700\n",
      "Precision: 0.9432\n",
      "Recall:    0.9881\n",
      "F1-Score:  0.9651\n",
      "\n",
      "=== Count Bigram ===\n",
      "Accuracy:  0.9500\n",
      "Precision: 0.9744\n",
      "Recall:    0.9048\n",
      "F1-Score:  0.9383\n",
      "\n",
      "=== Count Uni+Bi ===\n",
      "Accuracy:  0.9800\n",
      "Precision: 0.9651\n",
      "Recall:    0.9881\n",
      "F1-Score:  0.9765\n",
      "\n",
      "=== TF-IDF Unigram ===\n",
      "Accuracy:  0.9750\n",
      "Precision: 0.9540\n",
      "Recall:    0.9881\n",
      "F1-Score:  0.9708\n",
      "\n",
      "=== TF-IDF Bigram ===\n",
      "Accuracy:  0.9600\n",
      "Precision: 0.9750\n",
      "Recall:    0.9286\n",
      "F1-Score:  0.9512\n",
      "\n",
      "=== TF-IDF Uni+Bi ===\n",
      "Accuracy:  0.9800\n",
      "Precision: 0.9651\n",
      "Recall:    0.9881\n",
      "F1-Score:  0.9765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "vectorizers = {\n",
    "    \"Count Unigram\": CountVectorizer(ngram_range=(1,1), min_df=2),\n",
    "    \"Count Bigram\": CountVectorizer(ngram_range=(2,2), min_df=2),\n",
    "    \"Count Uni+Bi\": CountVectorizer(ngram_range=(1,2), min_df=2),\n",
    "    \"TF-IDF Unigram\": TfidfVectorizer(ngram_range=(1,1), min_df=2),\n",
    "    \"TF-IDF Bigram\": TfidfVectorizer(ngram_range=(2,2), min_df=2),\n",
    "    \"TF-IDF Uni+Bi\": TfidfVectorizer(ngram_range=(1,2), min_df=2, stop_words='english')\n",
    "}\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    X_train_vec = vectorizer.fit_transform(X_train_sentences)\n",
    "    X_test_vec = vectorizer.transform(X_test_sentences)\n",
    "    \n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='binary')\n",
    "    rec = recall_score(y_test, y_pred, average='binary')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
