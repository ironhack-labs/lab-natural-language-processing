{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehak\\AppData\\Local\\Temp\\ipykernel_10136\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "data.fillna(\"\",inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800\n",
      "Validation set size: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the split\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Validation set size:\", X_val.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    # Remove JavaScript/CSS\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Regular expression module\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = clean_html(text)  # Remove HTML content\n",
    "    text = clean_text(text)  # Clean text\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]  # Stemming & remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'text' column of X_train and X_val\n",
    "X_train['text'] = X_train['text'].apply(preprocess_text)\n",
    "X_val['text'] = X_val['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mehak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mehak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mehak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting X_train and X_val structure...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                  text  money_mark  \\\n",
      "29   regard mr nelson smithkind repli privat email ...           0   \n",
      "535            abl reach oscar suppo send pdb u receiv           0   \n",
      "695  huma abedin bim check pat k work jack jake res...           0   \n",
      "557                          announc monday cant today           0   \n",
      "836  bank africaag san pedro bp san pedro cote divo...           1   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "29                  0        72  \n",
      "535                 0        40  \n",
      "695                 0        78  \n",
      "557                 0        25  \n",
      "836                 1       943  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                  text  money_mark  \\\n",
      "521  dear sirc wish go offer consid partnerei mresa...           0   \n",
      "737  take mind balkan second see ã¢ââ great plug ...           0   \n",
      "740                                pls keep updat come           0   \n",
      "660  christ bethel hospit rue aboboteabidjanivori c...           0   \n",
      "411  sbwhoeopfriday februari amhr bravo brava issu ...           0   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "521                 1      1171  \n",
      "737                 0       313  \n",
      "740                 0        19  \n",
      "660                 1      1155  \n",
      "411                 0       154  \n",
      "After transformation:\n",
      "                                                  text  money_mark  \\\n",
      "29   regard mr nelson smithkind repli privat email ...           0   \n",
      "535            abl reach oscar suppo send pdb u receiv           0   \n",
      "695  huma abedin bim check pat k work jack jake res...           0   \n",
      "557                          announc monday cant today           0   \n",
      "836  bank africaag san pedro bp san pedro cote divo...           1   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "29                  0        72  \n",
      "535                 0        40  \n",
      "695                 0        78  \n",
      "557                 0        25  \n",
      "836                 1       943  \n",
      "                                                  text  money_mark  \\\n",
      "521  dear sirc wish go offer consid partnerei mresa...           0   \n",
      "737  take mind balkan second see ã¢ââ great plug ...           0   \n",
      "740                                pls keep updat come           0   \n",
      "660  christ bethel hospit rue aboboteabidjanivori c...           0   \n",
      "411  sbwhoeopfriday februari amhr bravo brava issu ...           0   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "521                 1      1171  \n",
      "737                 0       313  \n",
      "740                 0        19  \n",
      "660                 1      1155  \n",
      "411                 0       154  \n"
     ]
    }
   ],
   "source": [
    "import re  # Regular expression module\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Download required NLTK packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')  # Ensure punkt_tab is downloaded\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Debugging: Inspect the structure of X_train and X_val\n",
    "print(\"Inspecting X_train and X_val structure...\")\n",
    "print(type(X_train))\n",
    "print(X_train.head())\n",
    "print(type(X_val))\n",
    "print(X_val.head())\n",
    "\n",
    "# Apply lemmatization with structure checks\n",
    "if isinstance(X_train, pd.Series):\n",
    "    X_train = X_train.apply(lemmatize_text)\n",
    "    X_val = X_val.apply(lemmatize_text)\n",
    "elif isinstance(X_train, pd.DataFrame) and 'text' in X_train.columns:\n",
    "    X_train['text'] = X_train['text'].apply(lemmatize_text)\n",
    "    X_val['text'] = X_val['text'].apply(lemmatize_text)\n",
    "else:\n",
    "    raise ValueError(\"X_train and X_val should either be Series or DataFrames with a 'text' column.\")\n",
    "\n",
    "# Debugging: Check transformed data\n",
    "print(\"After transformation:\")\n",
    "print(X_train.head())\n",
    "print(X_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in the dataset:\n",
      "        Word  Frequency\n",
      "0     money        815\n",
      "1   account        720\n",
      "2      bank        686\n",
      "3      fund        587\n",
      "4   foreign        457\n",
      "5  transfer        446\n",
      "6      busi        425\n",
      "7   countri        399\n",
      "8   contact        380\n",
      "9  transact        378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorizer for Bag of Words\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform training data\n",
    "bow_matrix = bow_vectorizer.fit_transform(X_train['text'])\n",
    "\n",
    "# Summing up word frequencies\n",
    "word_counts = bow_matrix.sum(axis=0)\n",
    "words_freq = [(word, word_counts[0, idx]) for word, idx in bow_vectorizer.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display top 10 words\n",
    "top_10_words = pd.DataFrame(words_freq[:10], columns=['Word', 'Frequency'])\n",
    "print(\"Top 10 words in the dataset:\\n\", top_10_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'preprocessed_text'], dtype='object')\n",
      "                                                text  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...   \n",
      "1                                           Will do.   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...   \n",
      "4                                                fyi   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  dear sir, strictly a private business proposal...  \n",
      "1                                           will do.  \n",
      "2  nora--cheryl has emailed dozens of memos about...  \n",
      "3  dear sir=2fmadam=2c i know that this proposal ...  \n",
      "4                                                fyi  \n"
     ]
    }
   ],
   "source": [
    "print(data_train.columns)\n",
    "\n",
    "data_train['preprocessed_text'] = data_train['text'].str.lower()  # Adjust based on your actual column name\n",
    "\n",
    "print(data_train[['text', 'preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text'], dtype='object')\n",
      "                                                text  \\\n",
      "0  usiness is for the fact that the deceased man ...   \n",
      "1  They are happy to adjust to the afternoon. I a...   \n",
      "2  Lael Brainard was confirmed 78-19 this afterno...   \n",
      "3  H <hrod17@clintonemail.com>Friday March 26 201...   \n",
      "4  n;\"> Dear Good Friend,<br><br><br>I am happy t...   \n",
      "\n",
      "                                   preprocessed_text  \n",
      "0  usiness is for the fact that the deceased man ...  \n",
      "1  they are happy to adjust to the afternoon. i a...  \n",
      "2  lael brainard was confirmed 78-19 this afterno...  \n",
      "3  h <hrod17@clintonemail.com>friday march 26 201...  \n",
      "4  n;\"> dear good friend,<br><br><br>i am happy t...  \n"
     ]
    }
   ],
   "source": [
    "print(data_val.columns)\n",
    "\n",
    "data_val['preprocessed_text'] = data_val['text'].str.lower()  # Add more preprocessing if needed\n",
    "\n",
    "print(data_val[['text', 'preprocessed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1   \n",
      "1                                           Will do.      0   \n",
      "2  Nora--Cheryl has emailed dozens of memos about...      0   \n",
      "3  Dear Sir=2FMadam=2C I know that this proposal ...      1   \n",
      "4                                                fyi      0   \n",
      "\n",
      "                                   preprocessed_text  money_mark  \\\n",
      "0  dear sir, strictly a private business proposal...           1   \n",
      "1                                           will do.           1   \n",
      "2  nora--cheryl has emailed dozens of memos about...           1   \n",
      "3  dear sir=2fmadam=2c i know that this proposal ...           1   \n",
      "4                                                fyi           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 1      2292  \n",
      "1                 0         8  \n",
      "2                 0       197  \n",
      "3                 1      2199  \n",
      "4                 0         3  \n"
     ]
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "# Check the resulting data\n",
    "print(data_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  money_mark  \\\n",
      "29   regard mr nelson smithkind repli privat email ...           0   \n",
      "535            abl reach oscar suppo send pdb u receiv           0   \n",
      "695  huma abedin bim check pat k work jack jake res...           0   \n",
      "557                          announc monday cant today           0   \n",
      "836  bank africaag san pedro bp san pedro cote divo...           1   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "29                  0        72  \n",
      "535                 0        39  \n",
      "695                 0        78  \n",
      "557                 0        25  \n",
      "836                 1       936  \n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering\n",
    "X_train = add_features(X_train, 'text')\n",
    "X_val = add_features(X_val, 'text')\n",
    "\n",
    "# Preview added features\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BoW training dataset: (800, 14941)\n",
      "Shape of BoW validation dataset: (200, 14941)\n",
      "Top 10 words in the dataset:\n",
      " [('money', 815), ('account', 719), ('bank', 686), ('fund', 580), ('us', 543), ('foreign', 456), ('transfer', 446), ('busi', 418), ('countri', 384), ('contact', 380)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "bow_matrix_train = bow_vectorizer.fit_transform(X_train['text'])\n",
    "\n",
    "# Transform validation data\n",
    "bow_matrix_val = bow_vectorizer.transform(X_val['text'])\n",
    "\n",
    "# Display shape of BoW matrix\n",
    "print(\"Shape of BoW training dataset:\", bow_matrix_train.shape)\n",
    "print(\"Shape of BoW validation dataset:\", bow_matrix_val.shape)\n",
    "\n",
    "# Display top 10 words and their counts\n",
    "word_counts = bow_matrix_train.sum(axis=0)\n",
    "words_freq = [(word, word_counts[0, idx]) for word, idx in bow_vectorizer.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display top 10 words\n",
    "print(\"Top 10 words in the dataset:\\n\", words_freq[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF training dataset: (800, 14941)\n",
      "Shape of TF-IDF validation dataset: (200, 14941)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['text'])\n",
    "X_val_tfidf = vectorizer.transform(X_val['text'])\n",
    "\n",
    "# Display shape of the transformed dataset\n",
    "print(\"Shape of TF-IDF training dataset:\", X_train_tfidf.shape)\n",
    "print(\"Shape of TF-IDF validation dataset:\", X_val_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.915\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93       125\n",
      "           1       0.82      0.99      0.90        75\n",
      "\n",
      "    accuracy                           0.92       200\n",
      "   macro avg       0.91      0.93      0.91       200\n",
      "weighted avg       0.93      0.92      0.92       200\n",
      "\n",
      "Confusion Matrix:\n",
      " [[109  16]\n",
      " [  1  74]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Use MultinomialNB as specified\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
