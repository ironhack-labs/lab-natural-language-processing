{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Q2TBjozk9eOv",
        "outputId": "f18112f8-8e8c-401f-f331-5dd9030d60ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dajWXnEO9eOx"
      },
      "source": [
        "# Lab | Natural Language Processing\n",
        "### SMS: SPAM or HAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JNgbqJe9eOy"
      },
      "source": [
        "### Let's prepare the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-7fnPRN29eOy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSZXvLgW9eOz"
      },
      "source": [
        "- Read Data for the Fraudulent Email Kaggle Challenge\n",
        "- Reduce the training set to speead up development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1m1wewo9eOz",
        "outputId": "9450abe0-6b12-4237-bed2-2b16a5baaa3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2)\n"
          ]
        }
      ],
      "source": [
        "## Read Data for the Fraudulent Email Kaggle Challenge\n",
        "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
        "\n",
        "# Reduce the training set to speed up development.\n",
        "# Modify for final system\n",
        "data = data.head(1000)\n",
        "print(data.shape)\n",
        "data.fillna(\"\",inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LktYeJc99eOz"
      },
      "source": [
        "### Let's divide the training and test set into two partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8DOVV2fU9eO0"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train = data.text\n",
        "data_val = data.drop(columns=['text'])\n",
        "X_train,X_test,y_train,y_test=train_test_split(data_train,data_val,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQWMgNpY9eO0"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wISr2aA9eO1",
        "outputId": "84351db0-144a-4686-cf9d-78ada9014483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(string.punctuation)\n",
        "print(stopwords.words(\"english\")[100:110])\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snowball = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIbuzdoZ9eO1"
      },
      "source": [
        "## Now, we have to clean the html code removing words\n",
        "\n",
        "- First we remove inline JavaScript/CSS\n",
        "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
        "- Next we can remove the remaining tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RYJ_y5k9eO1",
        "outputId": "31c36a77-a5ca-4e95-c46e-1caf52842df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "521    Dear Sir=2C I wish you go through this offer t...\n",
            "737    To take your mind off the Balkans for a second...\n",
            "740                         Pls keep the updates coming!\n",
            "660    </STRONG><STRONG>CHRIST BETHEL HOSPITAL<BR>11 ...\n",
            "411    sbwhoeopFriday February 5 2010 7:11 AMHRe: Bra...\n",
            "                             ...                        \n",
            "408    Sorry yes exactlyWe have shy tomorrow at 10am ...\n",
            "332    DEAR=2CGOOD DAY=2EI KNOW THIS MESSAGE WILL COM...\n",
            "208                                                  FYI\n",
            "613    Greetings Dear Friend Please Permit me to cont...\n",
            "78     No in car on way to airport. Can you talk? Cal...\n",
            "Name: text, Length: 200, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(X_test) #before cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY1haE-T9eO2",
        "outputId": "95003189-fe7f-49c1-9fcc-ed225f878ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "521    Dear Sir=2C I wish you go through this offer t...\n",
            "737    To take your mind off the Balkans for a second...\n",
            "740                         Pls keep the updates coming!\n",
            "660    CHRIST BETHEL HOSPITAL11 RUE ABOBOTE,ABIDJANIV...\n",
            "411    sbwhoeopFriday February 5 2010 7:11 AMHRe: Bra...\n",
            "                             ...                        \n",
            "408    Sorry yes exactlyWe have shy tomorrow at 10am ...\n",
            "332    DEAR=2CGOOD DAY=2EI KNOW THIS MESSAGE WILL COM...\n",
            "208                                                  FYI\n",
            "613    Greetings Dear Friend Please Permit me to cont...\n",
            "78     No in car on way to airport. Can you talk? Cal...\n",
            "Name: text, Length: 200, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "import re\n",
        "\n",
        "def clean_html_regex(html_content):\n",
        "    html_content = re.sub(r'<(script|style).*?>.*?</\\1>', '', html_content, flags=re.S)  # Remove scripts & styles\n",
        "    html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.S)  # Remove comments\n",
        "    html_content = re.sub(r'<[^>]+>', '', html_content)  # Remove tags\n",
        "    return re.sub(r'\\s+', ' ', html_content).strip()  # Remove extra spaces\n",
        "\n",
        "\n",
        "X_train = X_train.apply(clean_html_regex)\n",
        "X_test = X_test.apply(clean_html_regex)\n",
        "\n",
        "print(X_test) #before cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0wpGrg9eO2"
      },
      "source": [
        "- Remove all the special characters\n",
        "    \n",
        "- Remove numbers\n",
        "    \n",
        "- Remove all single characters\n",
        "\n",
        "- Remove single characters from the start\n",
        "\n",
        "- Substitute multiple spaces with single space\n",
        "\n",
        "- Remove prefixed 'b'\n",
        "\n",
        "- Convert to Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0MY6_0p9eO2",
        "outputId": "e87802cd-4a70-4503-f1b2-8ffd633362a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "521    dear sir 2c wish you go through this offer to ...\n",
            "737    to take your mind off the balkans for second s...\n",
            "740                         pls keep the updates coming \n",
            "660    christ bethel hospital11 rue abobote abidjaniv...\n",
            "411    sbwhoeopfriday february 5 2010 7 11 amhre brav...\n",
            "                             ...                        \n",
            "408    sorry yes exactlywe have shy tomorrow at 10am ...\n",
            "332    dear 2cgood day 2ei know this message will com...\n",
            "208                                                  fyi\n",
            "613    greetings dear friend please permit me to cont...\n",
            "78     no in car on way to airport can you talk call ...\n",
            "Name: text, Length: 200, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "\n",
        "    # Remove single characters\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "\n",
        "    # Remove single characters from the start\n",
        "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
        "\n",
        "    # Substitute multiple spaces with single space\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "\n",
        "    # Remove prefixed 'b'\n",
        "    text = re.sub(r'^b\\s+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the function to the text data\n",
        "X_train = X_train.apply(preprocess_text)\n",
        "X_test = X_test.apply(preprocess_text)\n",
        "\n",
        "print(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0J1Ct7c9eO2"
      },
      "source": [
        "## Now let's work on removing stopwords\n",
        "Remove the stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz1sMU0Q9eO3",
        "outputId": "22eb4413-8cf4-469b-831e-1651ec877316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))  # Load NLTK's stopwords\n",
        "    words = word_tokenize(text)  # Tokenize text into words\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "X_train = X_train.apply(remove_stopwords)\n",
        "X_test = X_test.apply(remove_stopwords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAaWyL8u9eO3"
      },
      "source": [
        "## Tame Your Text with Lemmatization\n",
        "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BADw_GpS9eO3",
        "outputId": "8888e484-bd9b-4b31-d119-d9c82a4c3444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "# Apply the function to the text data\n",
        "X_train = X_train.apply(lemmatize_text)\n",
        "X_test = X_test.apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3fjcl59eO3"
      },
      "source": [
        "## Bag Of Words\n",
        "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NdVX8c759eO3"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "X_train_bow = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test_bow = vectorizer.transform(X_test).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxloeGBL9eO4"
      },
      "source": [
        "## Extra features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Di0HBhWm9eO4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2KHoPDa9eO4",
        "outputId": "bd9f4886-3764-44f0-a1a3-c1be7c01976f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    preprocessed_text  label money_mark suspicious_words  text_len\n",
            "521               NaN      1        NaN              NaN         0\n",
            "737               NaN      0        NaN              NaN         0\n",
            "740               NaN      0        NaN              NaN         0\n",
            "660               NaN      1        NaN              NaN         0\n",
            "411               NaN      0        NaN              NaN         0\n",
            "__________________________________________________________________\n",
            "    preprocessed_text  label money_mark suspicious_words  text_len\n",
            "29                NaN      1        NaN              NaN         0\n",
            "535               NaN      0        NaN              NaN         0\n",
            "695               NaN      0        NaN              NaN         0\n",
            "557               NaN      0        NaN              NaN         0\n",
            "836               NaN      1        NaN              NaN         0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Combine X_train and y_train into a single DataFrame for data_train\n",
        "data_train = pd.DataFrame(X_train, columns=['preprocessed_text'])\n",
        "data_train['label'] = y_train\n",
        "\n",
        "# Combine X_test and y_test into a single DataFrame for data_val\n",
        "data_val = pd.DataFrame(X_test, columns=['preprocessed_text'])\n",
        "data_val['label'] = y_test\n",
        "\n",
        "# Add new indicators to data_train\n",
        "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
        "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
        "\n",
        "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(str(x)) if isinstance(x, str) else 0)\n",
        "\n",
        "# Add new indicators to data_val\n",
        "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(str(x)) if isinstance(x, str) else 0)\n",
        "\n",
        "# Check the updated data_train\n",
        "print(data_val.head())\n",
        "print('_'*66)\n",
        "print(data_train.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DOlFFGJ9eO4"
      },
      "source": [
        "## How would work the Bag of Words with Count Vectorizer concept?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "R3Xd0d989eO4"
      },
      "outputs": [],
      "source": [
        "# Your code\n",
        "# The Bag of Words (BoW) model is a way to represent text data in numerical form.\n",
        "# It creates a vocabulary of all the unique words in the text corpus and then represents each document as a vector of word counts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qVss02g9eO5"
      },
      "source": [
        "## TD-IDF\n",
        "\n",
        "- Load the vectorizer\n",
        "\n",
        "- Vectorize all dataset\n",
        "\n",
        "- print the shape of the vetorized dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJG7K23p9eO5",
        "outputId": "1bfc9fb4-e48a-4219-9de1-eaf885fa9d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of TF-IDF vectorized training data: (800, 5000)\n",
            "Shape of TF-IDF vectorized test data: (200, 5000)\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
        "\n",
        "print(\"Shape of TF-IDF vectorized training data:\", X_train_tfidf.shape)\n",
        "print(\"Shape of TF-IDF vectorized test data:\", X_test_tfidf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEZ-sw9W9eO5"
      },
      "source": [
        "## And the Train a Classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPNFtUGU9eO5",
        "outputId": "99dbb414-8cd9-4848-e5fe-85035a3fffd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Accuracy: 0.965\n",
            "BoW Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97       125\n",
            "           1       0.93      0.99      0.95        75\n",
            "\n",
            "    accuracy                           0.96       200\n",
            "   macro avg       0.96      0.97      0.96       200\n",
            "weighted avg       0.97      0.96      0.97       200\n",
            "\n",
            "TF-IDF Accuracy: 0.96\n",
            "TF-IDF Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.94      0.97       125\n",
            "           1       0.91      0.99      0.95        75\n",
            "\n",
            "    accuracy                           0.96       200\n",
            "   macro avg       0.95      0.97      0.96       200\n",
            "weighted avg       0.96      0.96      0.96       200\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "# Your code\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train the classifier using BoW features\n",
        "clf_bow = MultinomialNB()\n",
        "clf_bow.fit(X_train_bow, y_train)\n",
        "y_pred_bow = clf_bow.predict(X_test_bow)\n",
        "\n",
        "print(\"BoW Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(\"BoW Classification Report:\\n\", classification_report(y_test, y_pred_bow))\n",
        "\n",
        "# Train the classifier using TF-IDF features\n",
        "clf_tfidf = MultinomialNB()\n",
        "clf_tfidf.fit(X_train_tfidf, y_train)\n",
        "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(\"TF-IDF Classification Report:\\n\", classification_report(y_test, y_pred_tfidf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShluGe3Q9eO5"
      },
      "source": [
        "### Extra Task - Implement a SPAM/HAM classifier\n",
        "\n",
        "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
        "\n",
        "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
        "\n",
        "Your task is to find the **best feature representation**.\n",
        "\n",
        "You can work with teams of two persons (recommended)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3_J3nTIo9eO5"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}