{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jarian\\AppData\\Local\\Temp\\ipykernel_16912\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:1600, Test size:400\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train size:{train_data.size}, Test size:{test_data.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     ----------- REGARDS, MR NELSON SMITH.KINDLY RE...\n",
      "535    I have not been able to reach oscar this am. W...\n",
      "695    ; Huma Abedin B6I'm checking with Pat on the 5...\n",
      "557    I can have it announced here on Monday - can't...\n",
      "836    BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 San P...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Import re module to use Regex for text cleaning\n",
    "import re\n",
    "\n",
    "# Function for cleaning HTML \n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)\n",
    "    # Remove inline CSS\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)\n",
    "    # Remove HTML comments\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "    \n",
    "# Apply cleaning to the training and testing data\n",
    "train_clean_html = train_data[\"text\"].apply(clean_html)\n",
    "test_clean_html = test_data[\"text\"].apply(clean_html)\n",
    "\n",
    "# Inspection of results for train data\n",
    "print(train_clean_html.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29      regards mr nelson smithkindly reply me on my ...\n",
      "535     have not been able to reach oscar this am we ...\n",
      "695     huma abedin bim checking with pat on the will...\n",
      "557      can have it announced here on monday cant today\n",
      "836    bank of africaagence san pedro bp san pedro co...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def additional_cleaning(text):\n",
    "    # Remove all special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\s\\w\\s', '', text)\n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove prefixed 'b' (common in encoded text)\n",
    "    text = re.sub(r'\\bb\\b', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply additonal cleaning function to train and test data\n",
    "train_cleaned = train_clean_html.apply(additional_cleaning)\n",
    "test_cleaned = test_clean_html.apply(additional_cleaning)\n",
    "\n",
    "# Inspect the cleaned data\n",
    "print(train_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     regards mr nelson smithkindly reply private em...\n",
      "535           able reach oscar supposed send pdb receive\n",
      "695    huma abedin bim checking pat work jack jake re...\n",
      "557                          announced monday cant today\n",
      "836    bank africaagence san pedro bp san pedro cote ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Initialize stop word set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train_cleaned = train_cleaned.apply(lambda text: ' '.join([word for word in text.split() if word not in stop_words]))\n",
    "test_cleaned = test_cleaned.apply(lambda text: ' '.join([word for word in text.split() if word not in stop_words]))\n",
    "\n",
    "# Inspect the cleaned data with stopwords removed\n",
    "print(train_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29     regard mr nelson smithkindly reply private ema...\n",
      "535           able reach oscar supposed send pdb receive\n",
      "695    huma abedin bim checking pat work jack jake re...\n",
      "557                          announced monday cant today\n",
      "836    bank africaagence san pedro bp san pedro cote ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize each word in the cleaned text\n",
    "train_lemmatized = train_cleaned.apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "test_lemmatized = test_cleaned.apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "\n",
    "# Inspect the lemmatized data\n",
    "print(train_lemmatized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['aac' 'aaclocated' 'aae' ... 'zumadirector' 'zumae' 'zurich']\n",
      "Document-Term Matrix:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(train_lemmatized)\n",
    "\n",
    "# Show the Bag of Words feature names and the document-term matrix\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
    "print(\"Document-Term Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  money_mark  \\\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1           1   \n",
      "535  I have not been able to reach oscar this am. W...      0           1   \n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0           1   \n",
      "557  I can have it announced here on Monday - can't...      0           1   \n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1           1   \n",
      "\n",
      "     suspicious_words  text_len  \n",
      "29                  0        75  \n",
      "535                 0        42  \n",
      "695                 0        79  \n",
      "557                 0        27  \n",
      "836                 1      1067  \n"
     ]
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "# Define the regular expressions for money symbols and suspicious words\n",
    "money_symbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"â‚¬\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "# Adding the indicators and text length to the training data\n",
    "train_lemmatized = train_lemmatized.apply(lambda x: x.lower())  # Make sure text is lowercase if not already\n",
    "train_data['money_mark'] = train_lemmatized.str.contains(money_symbol_list)*1\n",
    "train_data['suspicious_words'] = train_lemmatized.str.contains(suspicious_words)*1\n",
    "train_data['text_len'] = train_lemmatized.apply(lambda x: len(x))\n",
    "\n",
    "# Adding the indicators and text length to the test data\n",
    "test_lemmatized = test_lemmatized.apply(lambda x: x.lower())  # Make sure text is lowercase if not already\n",
    "test_data['money_mark'] = test_lemmatized.str.contains(money_symbol_list)*1\n",
    "test_data['suspicious_words'] = test_lemmatized.str.contains(suspicious_words)*1\n",
    "test_data['text_len'] = test_lemmatized.apply(lambda x: len(x))\n",
    "\n",
    "# Inspect the training data\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# The Bag of Words (BoW) model represents text as a collection of unique words, disregarding grammar and word order. \n",
    "# Using CountVectorizer from sklearn, we can convert a set of text documents into a Document-Term Matrix (DTM), where each row corresponds to a document and each column represents a unique word from the vocabulary. \n",
    "# The values in the matrix indicate the frequency of each word in the respective document. \n",
    "# After preprocessing the text (e.g., lemmatization, stopwords removal), we use CountVectorizer to tokenize the text and create the DTM. \n",
    "# This process converts the text data into a numerical format, making it suitable for machine learning models. \n",
    "# Additionally, parameters like stop_words, ngram_range, and max_features can be used to refine the model by excluding common words, including word sequences, or limiting the vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the TF-IDF matrix: (1000, 19349)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the entire dataset (combine train and test datasets)\n",
    "all_data = pd.concat([train_lemmatized, test_lemmatized])  # Concatenate train and test\n",
    "\n",
    "# Transform all text data into TF-IDF features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(all_data)\n",
    "\n",
    "# Print the shape of the resulting TF-IDF matrix\n",
    "print(\"Shape of the TF-IDF matrix:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.94      0.69       110\n",
      "           1       0.42      0.06      0.10        90\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.48      0.50      0.39       200\n",
      "weighted avg       0.49      0.54      0.42       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier on the TF-IDF transformed training data\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
