{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Nekky\n",
      "[nltk_data]     Lung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Nekky\n",
      "[nltk_data]     Lung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Nekky\n",
      "[nltk_data]     Lung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Read Data for the Fraudulent Email Kaggle Challenge\n",
    "# Reduce the training set to speed up development\n",
    "data = pd.read_csv(\"../data/kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "data.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (800, 2)\n",
      "Validation set size: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42, stratify=data['label'])\n",
    "print(f\"Training set size: {data_train.shape}\")\n",
    "print(f\"Validation set size: {data_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cleaned_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nekky Lung\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'cleaned_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Apply text preprocessing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m data_train[\u001b[33m'\u001b[39m\u001b[33mpreprocessed_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcleaned_text\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(preprocess_text)\n\u001b[32m     37\u001b[39m data_val[\u001b[33m'\u001b[39m\u001b[33mpreprocessed_text\u001b[39m\u001b[33m'\u001b[39m] = data_val[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m].apply(preprocess_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nekky Lung\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nekky Lung\\anaconda3\\envs\\ML\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'cleaned_text'"
     ]
    }
   ],
   "source": [
    "# HTML CLEANING SECTION\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "# TEXT PREPROCESSING SECTION\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove all special characters (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b' (if any)\n",
    "    text = re.sub(r'^b\\s+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text preprocessing\n",
    "data_train['preprocessed_text'] = data_train['cleaned_text'].apply(preprocess_text)\n",
    "data_val['preprocessed_text'] = data_val['cleaned_text'].apply(preprocess_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Before: Dear=2C Good day hope fine=2Cdear am writting this mail with due respect and heartful of tears since...\n",
      "After:  dearc good day hope finecdear am writting this mail with due respect and heartful of tears since we ...\n",
      "\n",
      "Example 2:\n",
      "Before: FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGEFOREIGN REMITTANCE UNIT,AFRICAN DEVELOPMENT BANK{ADB}....\n",
      "After:  from mr henry kaborethe chief auditor inchargeforeign remittance unitafrican development bankadbouag...\n",
      "\n",
      "Example 3:\n",
      "Before: Will do....\n",
      "After:  will do...\n",
      "\n",
      "Example 4:\n",
      "Before: FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND ACCOUNTING MANAGER,BANK OF AFRICA (B.O.A)OUAGADOUGOU-...\n",
      "After:  from the desk of dradamu ismalerauditing and accounting managerbank of africa boaouagadougouburkina ...\n",
      "\n",
      "Example 5:\n",
      "Before: Dear Friend, My name is LOI C.ESTRADA,The wife of Mr. JOSEPHESTRADA, the former President of Philipp...\n",
      "After:  dear friend my name is loi cestradathe wife of mr josephestrada the former president of philippines ...\n",
      "\n",
      "Example 6:\n",
      "Before: FROM =3AMR=2ECOLLINCE ADDOATTN =3AI NEED YOUR URGENT ASSISTANCESir=2C I want you to patiently read t...\n",
      "After:  from amrecollince addoattn ai need your urgent assistancesirc want you to patiently read this offere...\n",
      "\n",
      "Example 7:\n",
      "Before: Pls send me call sheet for Equadoran--i don't have a clue!...\n",
      "After:  pls send me call sheet for equadorani dont have clue...\n",
      "\n",
      "Example 8:\n",
      "Before: Yes I will arrange....\n",
      "After:  yes will arrange...\n",
      "\n",
      "Example 9:\n",
      "Before: FYI Ã¢ÂÂ I will limit Haiti emails to major fyi information for you....\n",
      "After:  fyi will limit haiti emails to major fyi information for you...\n",
      "\n",
      "Example 10:\n",
      "Before: Mr=2E Zuhair Idris Jordan Kuwait BankCredit Department Amman=2C 11191 Jordan Dear Sir=2FMadam=2CI am...\n",
      "After:  mre zuhair idris jordan kuwait bankcredit department ammanc jordan dear sirfmadamci am zuhair idris ...\n"
     ]
    }
   ],
   "source": [
    "# show the differences\n",
    "for i in range(10):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Before: {data_train['cleaned_text'].iloc[i][:100]}...\")\n",
    "    print(f\"After:  {data_train['preprocessed_text'].iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stopwords removal: dearc good day hope finecdear am writting this mail with due respect and heartful of tears since we have not known or met ourselves previously am asking for your assistanceci have will be very glad if you can render me assistance to my situation nowe will make my proposal well known if am given the opportunitye would like to use this opportunity to introduce myself to youe am miss johana johnpaul years old girl from liberia cthe only daughter of late godwin johnpaul the deputy minister of national security under the leadership of president charles taylor of liberia who is now in exile after many innocent soul were killede my father was killed by the government of charlestaylorcheaccuse my father of coup attempt and after month my mother cynthia was also killede the main reason why am contacting you now is to seek for your assistance in the area of my future investment and also want to hand over some huge amount of money to youe this money is ten millon five hundred thousand us dollars which was deposited some years ago by my father he made me the sole beneficiaryfnext of kin to the moneye am now asking you to stand on my behalf to make this claim for the bankcam girl and too young in age cannt handle this can of transactionci want you to stand as my foregin partner oversea and also to help me investment the money as well send me your telephone number and pictures okeeee have all original copies of the documents concering this money with me herecso please am expecting to hear from you soone my regardsc miss johana\n",
      "After stopwords removal: dearc good day hope finecdear writting mail due respect heartful tears since known met previously asking assistanceci glad render assistance situation nowe make proposal well known given opportunitye would like use opportunity introduce youe miss johana johnpaul years old girl liberia cthe daughter late godwin johnpaul deputy minister national security leadership president charles taylor liberia exile many innocent soul killede father killed government charlestaylorcheaccuse father coup attempt month mother cynthia also killede main reason contacting seek assistance area future investment also want hand huge amount money youe money ten millon five hundred thousand us dollars deposited years ago father made sole beneficiaryfnext kin moneye asking stand behalf make claim bankcam girl young age cannt handle transactionci want stand foregin partner oversea also help investment money well send telephone number pictures okeeee original copies documents concering money herecso please expecting hear soone regardsc miss johana\n"
     ]
    }
   ],
   "source": [
    "# STOPWORDS REMOVAL SECTION\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply stopwords removal\n",
    "data_train['no_stopwords'] = data_train['preprocessed_text'].apply(remove_stopwords)\n",
    "data_val['no_stopwords'] = data_val['preprocessed_text'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "print(f\"Before stopwords removal: {data_train['preprocessed_text'].iloc[0]}\")\n",
    "print(f\"After stopwords removal: {data_train['no_stopwords'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final preprocessed text sample: dearc good day hope finecdear writting mail due respect heartful tear since known met previously asking assistanceci glad render assistance situation nowe make proposal well known given opportunitye would like use opportunity introduce youe miss johana johnpaul year old girl liberia cthe daughter late godwin johnpaul deputy minister national security leadership president charles taylor liberia exile many innocent soul killede father killed government charlestaylorcheaccuse father coup attempt month mother cynthia also killede main reason contacting seek assistance area future investment also want hand huge amount money youe money ten millon five hundred thousand u dollar deposited year ago father made sole beneficiaryfnext kin moneye asking stand behalf make claim bankcam girl young age cannt handle transactionci want stand foregin partner oversea also help investment money well send telephone number picture okeeee original copy document concering money herecso please expecting hear soone regardsc miss johana\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZATION SECTION\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Apply lemmatization to reduce words to their base form\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply lemmatization\n",
    "data_train['lemmatized_text'] = data_train['no_stopwords'].apply(lemmatize_text)\n",
    "data_val['lemmatized_text'] = data_val['no_stopwords'].apply(lemmatize_text)\n",
    "\n",
    "# Update preprocessed_text to include all cleaning steps\n",
    "data_train['preprocessed_text'] = data_train['lemmatized_text']\n",
    "data_val['preprocessed_text'] = data_val['lemmatized_text']\n",
    "\n",
    "\n",
    "print(f\"Final preprocessed text sample: {data_train['preprocessed_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words in HAM messages:\n",
      "would        91\n",
      "president    90\n",
      "u            88\n",
      "percent      76\n",
      "call         75\n",
      "state        72\n",
      "work         70\n",
      "mr           70\n",
      "one          62\n",
      "time         60\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 words in SPAM messages:\n",
      "money          708\n",
      "account        597\n",
      "bank           571\n",
      "fund           542\n",
      "u              441\n",
      "business       384\n",
      "transaction    327\n",
      "country        320\n",
      "million        306\n",
      "company        300\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# BAG OF WORDS - EXPLORATORY DATA ANALYSIS\n",
    "# the 10 top words in ham and spam messages (EXPLORATORY DATA ANALYSIS)\n",
    "def get_top_words(data, label, n=10):\n",
    "    \"\"\"\n",
    "    Get top n words for a specific label\n",
    "    \"\"\"\n",
    "    label_data = data[data['label'] == label]\n",
    "    all_text = ' '.join(label_data['preprocessed_text'].tolist())\n",
    "    words = all_text.split()\n",
    "    word_freq = pd.Series(words).value_counts()\n",
    "    return word_freq.head(n)\n",
    "\n",
    "print(\"\\nTop 10 words in HAM messages:\")\n",
    "print(get_top_words(data_train, 0))\n",
    "\n",
    "print(\"\\nTop 10 words in SPAM messages:\")\n",
    "print(get_top_words(data_train, 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear=2C Good day hope fine=2Cdear am writting ...</td>\n",
       "      <td>dearc good day hope finecdear writting mail du...</td>\n",
       "      <td>dearc good day hope finecdear writting mail du...</td>\n",
       "      <td>dearc good day hope finecdear writting mail du...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>1</td>\n",
       "      <td>FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...</td>\n",
       "      <td>mr henry kaborethe chief auditor inchargeforei...</td>\n",
       "      <td>mr henry kaborethe chief auditor inchargeforei...</td>\n",
       "      <td>mr henry kaborethe chief auditor inchargeforei...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "      <td>Will do.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>1</td>\n",
       "      <td>FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...</td>\n",
       "      <td>desk dradamu ismalerauditing accounting manage...</td>\n",
       "      <td>desk dradamu ismalerauditing accounting manage...</td>\n",
       "      <td>desk dradamu ismalerauditing accounting manage...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Friend, My name is LOI C.ESTRADA,The wife...</td>\n",
       "      <td>dear friend name loi cestradathe wife mr josep...</td>\n",
       "      <td>dear friend name loi cestradathe wife mr josep...</td>\n",
       "      <td>dear friend name loi cestradathe wife mr josep...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "442  Dear=2C Good day hope fine=2Cdear am writting ...      1   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...      1   \n",
       "971                                           Will do.      0   \n",
       "190  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...      1   \n",
       "551  Dear Friend, My name is LOI C.ESTRADA,The wife...      1   \n",
       "\n",
       "                                          cleaned_text  \\\n",
       "442  Dear=2C Good day hope fine=2Cdear am writting ...   \n",
       "962  FROM MR HENRY KABORETHE CHIEF AUDITOR INCHARGE...   \n",
       "971                                           Will do.   \n",
       "190  FROM THE DESK OF DR.ADAMU  ISMALERAUDITING AND...   \n",
       "551  Dear Friend, My name is LOI C.ESTRADA,The wife...   \n",
       "\n",
       "                                     preprocessed_text  \\\n",
       "442  dearc good day hope finecdear writting mail du...   \n",
       "962  mr henry kaborethe chief auditor inchargeforei...   \n",
       "971                                                      \n",
       "190  desk dradamu ismalerauditing accounting manage...   \n",
       "551  dear friend name loi cestradathe wife mr josep...   \n",
       "\n",
       "                                          no_stopwords  \\\n",
       "442  dearc good day hope finecdear writting mail du...   \n",
       "962  mr henry kaborethe chief auditor inchargeforei...   \n",
       "971                                                      \n",
       "190  desk dradamu ismalerauditing accounting manage...   \n",
       "551  dear friend name loi cestradathe wife mr josep...   \n",
       "\n",
       "                                       lemmatized_text  money_mark  \\\n",
       "442  dearc good day hope finecdear writting mail du...           1   \n",
       "962  mr henry kaborethe chief auditor inchargeforei...           0   \n",
       "971                                                              0   \n",
       "190  desk dradamu ismalerauditing accounting manage...           1   \n",
       "551  dear friend name loi cestradathe wife mr josep...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "442                 1      1024  \n",
       "962                 1      1954  \n",
       "971                 0         0  \n",
       "190                 1       390  \n",
       "551                 1      1507  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words shape - Train: (800, 1000), Validation: (200, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words vectorizer\n",
    "# Creating Bag of Words features\n",
    "bow_vectorizer = CountVectorizer(max_features=1000)\n",
    "X_train_bow = bow_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_bow = bow_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "print(f\"Bag of Words shape - Train: {X_train_bow.shape}, Validation: {X_val_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "TF-IDF shape - Train: (800, 1000), Validation: (200, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Load the vectorizer\n",
    "print(\"Creating TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val['preprocessed_text'])\n",
    "\n",
    "# print the shape of the vectorized dataset\n",
    "print(f\"TF-IDF shape - Train: {X_train_tfidf.shape}, Validation: {X_val_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_val, y_train, y_val, feature_name):\n",
    "    \"\"\"\n",
    "    Train MultinomialNB classifier and evaluate performance\n",
    "    \"\"\"\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = classifier.predict(X_val)\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"\\n{feature_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words Only Results:\n",
      "Accuracy: 0.9300\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93       112\n",
      "           1       0.87      0.99      0.93        88\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.93      0.94      0.93       200\n",
      "weighted avg       0.94      0.93      0.93       200\n",
      "\n",
      "\n",
      "TF-IDF Only Results:\n",
      "Accuracy: 0.9550\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       112\n",
      "           1       0.92      0.98      0.95        88\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.95      0.96      0.95       200\n",
      "weighted avg       0.96      0.95      0.96       200\n",
      "\n",
      "\n",
      "Bag of Words + Extra Features Results:\n",
      "Accuracy: 0.9300\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93       112\n",
      "           1       0.87      0.99      0.93        88\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.93      0.94      0.93       200\n",
      "weighted avg       0.94      0.93      0.93       200\n",
      "\n",
      "\n",
      "TF-IDF + Extra Features Results:\n",
      "Accuracy: 0.9250\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       112\n",
      "           1       0.89      0.94      0.92        88\n",
      "\n",
      "    accuracy                           0.93       200\n",
      "   macro avg       0.92      0.93      0.92       200\n",
      "weighted avg       0.93      0.93      0.93       200\n",
      "\n",
      "Bag of Words Only: 0.9300\n",
      "TF-IDF Only: 0.9550\n",
      "Bag of Words + Extra Features: 0.9300\n",
      "TF-IDF + Extra Features: 0.9250\n",
      "\n",
      "Best performing feature combination: TF-IDF Only with accuracy 0.9550\n"
     ]
    }
   ],
   "source": [
    "# Prepare target variables\n",
    "y_train = data_train['label']\n",
    "y_val = data_val['label']\n",
    "\n",
    "# Test different feature combinations\n",
    "results = {}\n",
    "\n",
    "# 1. Bag of Words only\n",
    "accuracy_bow = train_and_evaluate(X_train_bow, X_val_bow, y_train, y_val, \"Bag of Words Only\")\n",
    "results['Bag of Words Only'] = accuracy_bow\n",
    "\n",
    "# 2. TF-IDF only\n",
    "accuracy_tfidf = train_and_evaluate(X_train_tfidf, X_val_tfidf, y_train, y_val, \"TF-IDF Only\")\n",
    "results['TF-IDF Only'] = accuracy_tfidf\n",
    "\n",
    "# 3. Bag of Words + extra features\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "X_train_bow_extra = hstack([X_train_bow, csr_matrix(data_train[['money_mark', 'suspicious_words', 'text_len']].values)])\n",
    "X_val_bow_extra = hstack([X_val_bow, csr_matrix(data_val[['money_mark', 'suspicious_words', 'text_len']].values)])\n",
    "\n",
    "accuracy_bow_extra = train_and_evaluate(X_train_bow_extra, X_val_bow_extra, y_train, y_val, \"Bag of Words + Extra Features\")\n",
    "results['Bag of Words + Extra Features'] = accuracy_bow_extra\n",
    "\n",
    "# 4. TF-IDF + extra features\n",
    "X_train_tfidf_extra = hstack([X_train_tfidf, csr_matrix(data_train[['money_mark', 'suspicious_words', 'text_len']].values)])\n",
    "X_val_tfidf_extra = hstack([X_val_tfidf, csr_matrix(data_val[['money_mark', 'suspicious_words', 'text_len']].values)])\n",
    "\n",
    "accuracy_tfidf_extra = train_and_evaluate(X_train_tfidf_extra, X_val_tfidf_extra, y_train, y_val, \"TF-IDF + Extra Features\")\n",
    "results['TF-IDF + Extra Features'] = accuracy_tfidf_extra\n",
    "\n",
    "# Display results comparison\n",
    "for feature, accuracy in results.items():\n",
    "    print(f\"{feature}: {accuracy:.4f}\")\n",
    "\n",
    "# Find the best performing feature combination\n",
    "best_feature = max(results, key=results.get)\n",
    "print(f\"\\nBest performing feature combination: {best_feature} with accuracy {results[best_feature]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
