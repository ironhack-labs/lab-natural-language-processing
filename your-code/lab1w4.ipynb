{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "PJ8iRlNOdf4i",
        "outputId": "8e119bd2-337f-4e18-8be9-851c14e78e75"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfIxAq8gdf4l"
      },
      "source": [
        "# Lab | Natural Language Processing\n",
        "### SMS: SPAM or HAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42lbpjP3df4p"
      },
      "source": [
        "### Let's prepare the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vxM9H8m_df4q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq1mzd7mdf4q"
      },
      "source": [
        "- Read Data for the Fraudulent Email Kaggle Challenge\n",
        "- Reduce the training set to speead up development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKY5iT9ydf4r",
        "outputId": "9036cd41-ae5a-4c50-9664-aef593c41994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2)\n"
          ]
        }
      ],
      "source": [
        "## Read Data for the Fraudulent Email Kaggle Challenge\n",
        "data = pd.read_csv(\"kg_train.csv\",encoding='latin-1')\n",
        "\n",
        "# Reduce the training set to speed up development.\n",
        "# Modify for final system\n",
        "data = data.head(1000)\n",
        "print(data.shape)\n",
        "data.fillna(\"\",inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVaZqdRNdf4s"
      },
      "source": [
        "### Let's divide the training and test set into two partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecb4F-c5df4t",
        "outputId": "7ec40b3d-0c93-42e5-f7db-f5dbf3a0bdbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 800\n",
            "Test set size: 200\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data['text']  # Your feature (text data)\n",
        "y = data['label']  # Your target (label: SPAM or HAM)\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the size of the resulting splits\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgppQAxXdf4u"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhpHyd5SeVHO",
        "outputId": "cb651b78-8945-41b4-a823-b3b4c7444735"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFYzWKybdf4u",
        "outputId": "80bd2a33-43cd-4a90-d18c-6d4a346784f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "print(string.punctuation)\n",
        "print(stopwords.words(\"english\")[100:110])\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snowball = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p58CN3s6df4w"
      },
      "source": [
        "## Now, we have to clean the html code removing words\n",
        "\n",
        "- First we remove inline JavaScript/CSS\n",
        "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
        "- Next we can remove the remaining tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyCP2iT7df4w",
        "outputId": "7bead8da-6afb-40d0-bbe5-2535342ce750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e1087a023540>:13: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
            "\n",
            "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
            "\n",
            "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
            "\n",
            "    from bs4 import MarkupResemblesLocatorWarning\n",
            "    import warnings\n",
            "\n",
            "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
            "    \n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...\n",
            "1                                             Will do.\n",
            "2    Nora--Cheryl has emailed dozens of memos about...\n",
            "3    Dear Sir=2FMadam=2C I know that this proposal ...\n",
            "4                                                  fyi\n",
            "Name: cleaned_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "# Function to clean the HTML content\n",
        "def clean_html(text):\n",
        "    # 1. Remove inline JavaScript and CSS\n",
        "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)  # Remove <script> tags\n",
        "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)    # Remove <style> tags\n",
        "\n",
        "    # 2. Remove HTML comments\n",
        "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)  # Remove comments (<!-- -->)\n",
        "\n",
        "    # 3. Remove remaining HTML tags using BeautifulSoup\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    cleaned_text = soup.get_text()  # Extract text without tags\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Example usage on your dataset (assuming 'data' is your DataFrame with a 'text' column)\n",
        "data['cleaned_text'] = data['text'].apply(clean_html)\n",
        "\n",
        "# Now, `data['cleaned_text']` will contain the HTML-free cleaned text\n",
        "print(data['cleaned_text'].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4cwiGHfdf4x"
      },
      "source": [
        "- Remove all the special characters\n",
        "    \n",
        "- Remove numbers\n",
        "    \n",
        "- Remove all single characters\n",
        "\n",
        "- Remove single characters from the start\n",
        "\n",
        "- Substitute multiple spaces with single space\n",
        "\n",
        "- Remove prefixed 'b'\n",
        "\n",
        "- Convert to Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNAxi961df4y",
        "outputId": "74071b1d-e47e-45a8-eb4d-3f49811728fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    dear sir strictly private business proposal am...\n",
            "1                                              will do\n",
            "2    noracheryl has emailed dozens of memos about h...\n",
            "3    dear sirfmadamc know that this proposal might ...\n",
            "4                                                  fyi\n",
            "Name: cleaned_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def clean_text(text):\n",
        "    # 1. Remove all special characters (keeping only alphabets and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # 2. Remove all numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 3. Remove all single characters (but keep important short words, e.g., \"a\", \"I\")\n",
        "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
        "\n",
        "    # 4. Remove single characters from the start of the text\n",
        "    text = re.sub(r'^\\b[a-zA-Z]\\b', '', text)\n",
        "\n",
        "    # 5. Substitute multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 6. Remove any prefixed 'b' (for byte literals like b'abc')\n",
        "    text = re.sub(r\"^b'\", '', text)\n",
        "\n",
        "    # 7. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Strip any extra leading/trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage on your dataset (assuming 'data' is your DataFrame with a 'text' column)\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)\n",
        "\n",
        "# Check the cleaned text\n",
        "print(data['cleaned_text'].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljFI1pG_df4z"
      },
      "source": [
        "## Now let's work on removing stopwords\n",
        "Remove the stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID56HlYSdf4z",
        "outputId": "f4593ac7-5d71-412d-d353-f74d51c7e56f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    dear sir strictly private business proposal mi...\n",
            "1                                                     \n",
            "2    noracheryl emailed dozens memos haiti weekend ...\n",
            "3    dear sirfmadamc know proposal might surprise e...\n",
            "4                                                  fyi\n",
            "Name: cleaned_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "# Download stopwords if not already available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the list of stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text_with_stopwords(text):\n",
        "    # 1. Remove all special characters (keeping only alphabets and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # 2. Remove all numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 3. Remove all single characters (but keep important short words, e.g., \"a\", \"I\")\n",
        "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
        "\n",
        "    # 4. Remove single characters from the start of the text\n",
        "    text = re.sub(r'^\\b[a-zA-Z]\\b', '', text)\n",
        "\n",
        "    # 5. Substitute multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 6. Remove any prefixed 'b' (for byte literals like b'abc')\n",
        "    text = re.sub(r\"^b'\", '', text)\n",
        "\n",
        "    # 7. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 8. Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # Strip any extra leading/trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage on your dataset (assuming 'data' is your DataFrame with a 'text' column)\n",
        "data['cleaned_text'] = data['text'].apply(clean_text_with_stopwords)\n",
        "\n",
        "# Check the cleaned text after removing stopwords\n",
        "print(data['cleaned_text'].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8FQJWwNdf40"
      },
      "source": [
        "## Tame Your Text with Lemmatization\n",
        "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cLM59zLdf40",
        "outputId": "f87927db-064f-409d-cd08-f50b4a263362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    dear sir strictly private business proposal mi...\n",
            "1                                                     \n",
            "2    noracheryl emailed dozen memo haiti weekend pl...\n",
            "3    dear sirfmadamc know proposal might surprise e...\n",
            "4                                                  fyi\n",
            "Name: cleaned_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Download necessary NLTK data if not already available\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # For multilingual support in WordNet\n",
        "\n",
        "# Initialize the lemmatizer and get the stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text_with_lemmatization(text):\n",
        "    # 1. Remove all special characters (keeping only alphabets and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # 2. Remove all numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # 3. Remove all single characters (but keep important short words, e.g., \"a\", \"I\")\n",
        "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
        "\n",
        "    # 4. Remove single characters from the start of the text\n",
        "    text = re.sub(r'^\\b[a-zA-Z]\\b', '', text)\n",
        "\n",
        "    # 5. Substitute multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 6. Remove any prefixed 'b' (for byte literals like b'abc')\n",
        "    text = re.sub(r\"^b'\", '', text)\n",
        "\n",
        "    # 7. Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 8. Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # 9. Lemmatization - Reduce words to their base form\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "    # Strip any extra leading/trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage on your dataset (assuming 'data' is your DataFrame with a 'text' column)\n",
        "data['cleaned_text'] = data['text'].apply(clean_text_with_lemmatization)\n",
        "\n",
        "# Check the cleaned text after lemmatization\n",
        "print(data['cleaned_text'].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_mAgz2bdf41"
      },
      "source": [
        "## Bag Of Words\n",
        "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFUSSzbudf41",
        "outputId": "83b5dc68-7950-46c3-c88b-743b83b4929e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Load data and process text\n",
        "data_train = pd.read_csv('kg_train.csv').fillna('')\n",
        "data_test = pd.read_csv('kg_test.csv').fillna('')\n",
        "\n",
        "ham_train = data_train[data_train['label'] == 'ham']['text'].apply(clean_and_tokenize)\n",
        "spam_train = data_train[data_train['label'] == 'spam']['text'].apply(clean_and_tokenize)\n",
        "\n",
        "ham_words = [word for msg in ham_train for word in msg]\n",
        "spam_words = [word for msg in spam_train for word in msg]\n",
        "\n",
        "# Count frequencies and get top 10 most common words\n",
        "top_10_ham = Counter(ham_words).most_common(10)\n",
        "top_10_spam = Counter(spam_words).most_common(10)\n",
        "\n",
        "def plot_top_words(top_words, title):\n",
        "    if top_words:\n",
        "        words, frequencies = zip(*top_words)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.barh(words, frequencies, color='skyblue')\n",
        "        plt.xlabel('Frequency')\n",
        "        plt.title(title)\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.show()\n",
        "\n",
        "plot_top_words(top_10_ham, 'Top 10 Words in Ham Messages (Train)')\n",
        "plot_top_words(top_10_spam, 'Top 10 Words in Spam Messages (Train)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDl3ZCmWdf42"
      },
      "source": [
        "## Extra features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goYKsl2Edf43"
      },
      "outputs": [],
      "source": [
        "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
        "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
        "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
        "\n",
        "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
        "\n",
        "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
        "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
        "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x))\n",
        "\n",
        "data_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcjlB-yodf45"
      },
      "source": [
        "## How would work the Bag of Words with Count Vectorizer concept?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYnGurX7df45",
        "outputId": "0601f974-1182-4c1d-eb96-6d0e40800319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['fun' 'is' 'love' 'programming']\n",
            "Bag of Words Representation:\n",
            " [[0 0 1 1]\n",
            " [1 1 0 1]\n",
            " [1 0 1 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents (corpus)\n",
        "documents = [\n",
        "    \"I love programming\",\n",
        "    \"Programming is fun\",\n",
        "    \"I love fun programming\"\n",
        "]\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the documents into a matrix\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert to a dense array to view the result\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words Representation:\\n\", X.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZvMNO3Zdf46"
      },
      "source": [
        "## TD-IDF\n",
        "\n",
        "- Load the vectorizer\n",
        "\n",
        "- Vectorize all dataset\n",
        "\n",
        "- print the shape of the vetorized dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHy8B5iGdf47",
        "outputId": "f4be966e-7f1b-4c1e-b9b5-8a2816227a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of TF-IDF matrix: (3, 4)\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.78980693 0.61335554]\n",
            " [0.54783215 0.72033345 0.         0.42544054]\n",
            " [0.61980538 0.         0.61980538 0.48133417]]\n",
            "Vocabulary: ['fun' 'is' 'love' 'programming']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample data\n",
        "documents = [\n",
        "    \"I love programming\",\n",
        "    \"Programming is fun\",\n",
        "    \"I love fun programming\"\n",
        "]\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents into a TF-IDF matrix\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Print the shape of the TF-IDF matrix (documents x features)\n",
        "print(\"Shape of TF-IDF matrix:\", X.shape)\n",
        "\n",
        "# If you want to view the actual TF-IDF values as an array\n",
        "print(\"TF-IDF Matrix:\\n\", X.toarray())\n",
        "\n",
        "# Vocabulary of terms used in the TF-IDF matrix\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAq2vrrqdf48"
      },
      "source": [
        "## And the Train a Classifier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPrzXecjdf48",
        "outputId": "a1ef6fc3-d471-497b-d053-a37fe05a5c30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9748603351955307\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98      1002\n",
            "           1       1.00      0.94      0.97       788\n",
            "\n",
            "    accuracy                           0.97      1790\n",
            "   macro avg       0.98      0.97      0.97      1790\n",
            "weighted avg       0.98      0.97      0.97      1790\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.datasets import load_files\n",
        "\n",
        "# Step 1: Prepare the Data\n",
        "X = data_train['text']  # Text data\n",
        "y = data_train['label']  # Labels (ham/spam)\n",
        "\n",
        "# Step 2: Split the Data into Training and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Vectorize the Text Data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 4: Train a Classifier (Logistic Regression as an example)\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Step 5: Evaluate the Classifier\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Print Evaluation Metrics\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM5DqOgydf49"
      },
      "source": [
        "### Extra Task - Implement a SPAM/HAM classifier\n",
        "\n",
        "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
        "\n",
        "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
        "\n",
        "Your task is to find the **best feature representation**.\n",
        "\n",
        "You can work with teams of two persons (recommended)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load datasets\n",
        "data_train = pd.read_csv(\"kg_train.csv\")\n",
        "data_test = pd.read_csv(\"kg_test.csv\")\n",
        "\n",
        "# Fill missing values\n",
        "data_train['text'].fillna('', inplace=True)\n",
        "data_test['text'].fillna('', inplace=True)\n",
        "\n",
        "# Splitting train data for evaluation\n",
        "X_train, X_val, y_train, y_val = train_test_split(data_train['text'], data_train['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction methods\n",
        "vectorizers = {\n",
        "    \"BoW (Unigram)\": CountVectorizer(stop_words='english', max_features=5000),\n",
        "    \"TF-IDF\": TfidfVectorizer(stop_words='english', max_features=5000),\n",
        "    \"BoW (Bigram)\": CountVectorizer(ngram_range=(2,2), stop_words='english', max_features=5000),\n",
        "    \"BoW (Trigram)\": CountVectorizer(ngram_range=(3,3), stop_words='english', max_features=5000),\n",
        "    \"BoW (Unigram + Bigram)\": CountVectorizer(ngram_range=(1,2), stop_words='english', max_features=5000),\n",
        "    \"TF-IDF (Bigram)\": TfidfVectorizer(ngram_range=(1,2), stop_words='english', max_features=5000)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for name, vectorizer in vectorizers.items():\n",
        "    # Transform data\n",
        "    X_train_vect = vectorizer.fit_transform(X_train)\n",
        "    X_val_vect = vectorizer.transform(X_val)\n",
        "\n",
        "    # Train MultinomialNB\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train_vect, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_val_vect)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    report = classification_report(y_val, y_pred, output_dict=True)\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        \"Feature Representation\": name,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision (Spam)\": report['1']['precision'],\n",
        "        \"Recall (Spam)\": report['1']['recall'],\n",
        "        \"F1-Score (Spam)\": report['1']['f1-score']\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1AZDhZPjbvx",
        "outputId": "57c5c257-7f75-4f9e-ad92-cb675a9bfe9b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-0f2f2282f262>:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data_train['text'].fillna('', inplace=True)\n",
            "<ipython-input-27-0f2f2282f262>:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data_test['text'].fillna('', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Feature Representation  Accuracy  Precision (Spam)  Recall (Spam)  \\\n",
            "0           BoW (Unigram)  0.970662          0.954887       0.978805   \n",
            "1                  TF-IDF  0.981559          0.980658       0.976879   \n",
            "2            BoW (Bigram)  0.970662          0.982072       0.949904   \n",
            "3           BoW (Trigram)  0.939648          1.000000       0.861272   \n",
            "4  BoW (Unigram + Bigram)  0.972339          0.970930       0.965318   \n",
            "5         TF-IDF (Bigram)  0.978206          0.986193       0.963391   \n",
            "\n",
            "   F1-Score (Spam)  \n",
            "0         0.966698  \n",
            "1         0.978764  \n",
            "2         0.965720  \n",
            "3         0.925466  \n",
            "4         0.968116  \n",
            "5         0.974659  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}