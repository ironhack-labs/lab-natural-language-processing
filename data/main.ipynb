{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"kg_train.csv\", encoding='latin-1')\n",
    "\n",
    "# Reduce the dataset size for faster development\n",
    "data = data.head(1000)\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(\"Original data shape:\", data.shape)\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "data.fillna(\"\", inplace=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# Using 20% of the data for testing\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the shapes of the resulting datasets\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Test set shape:\", test_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "def clean_html(html):\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Remove inline JavaScript and CSS\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()  # Remove the tag and its contents\n",
    "\n",
    "    # Remove HTML comments\n",
    "    comments = soup.findAll(text=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment.extract()  # Remove comment\n",
    "\n",
    "    # Get the text without the remaining tags\n",
    "    clean_text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "# Example HTML input\n",
    "html_input = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Page</title>\n",
    "    <style>\n",
    "        body { font-family: Arial; }\n",
    "    </style>\n",
    "    <script>\n",
    "        console.log('Hello, world!');\n",
    "    </script>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Welcome to the Sample Page</h1>\n",
    "    <!-- This is a comment -->\n",
    "    <p>This is a sample paragraph.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Clean the HTML\n",
    "cleaned_text = clean_html(html_input)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove all special characters (keeping only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove all numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\s*\\w\\s*', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = text.lstrip('b')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example input text\n",
    "input_text = \"b This is a sample text 123! With some special characters @# and numbers 1, 2, 3.\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(input_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Make sure to download the stopwords if you haven't already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove all special characters (keeping only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove all numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\s*\\w\\s*', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = text.lstrip('b')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Get the list of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Example input text\n",
    "input_text = \"b This is a sample text 123! With some special characters @# and numbers 1, 2, 3.\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(input_text)\n",
    "\n",
    "# Remove stopwords\n",
    "final_text = remove_stopwords(cleaned_text)\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # For multilingual support\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove all special characters (keeping only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove all numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\s*\\w\\s*', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b'\n",
    "    text = text.lstrip('b')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Strip leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Get the list of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Example input text\n",
    "input_text = \"b The cats are running and jumping over the lazy dogs.\"\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(input_text)\n",
    "\n",
    "# Remove stopwords\n",
    "no_stopwords_text = remove_stopwords(cleaned_text)\n",
    "\n",
    "# Lemmatize the words\n",
    "final_text = lemmatize_words(no_stopwords_text)\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample dataset (replace with your dataset)\n",
    "data = {\n",
    "    'message': [\n",
    "        \"Hey, how are you?\",\n",
    "        \"Win a free iPhone now!\",\n",
    "        \"Call me when you're free.\",\n",
    "        \"Congratulations! You've won a prize.\",\n",
    "        \"Let's meet for lunch.\",\n",
    "        \"Claim your reward today!\"\n",
    "    ],\n",
    "    'label': ['ham', 'spam', 'ham', 'spam', 'ham', 'spam']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_messages = df[df['label'] == 'ham']['message']\n",
    "spam_messages = df[df['label'] == 'spam']['message']\n",
    "\n",
    "# Get word frequencies for ham and spam\n",
    "ham_words = []\n",
    "spam_words = []\n",
    "\n",
    "for message in ham_messages:\n",
    "    ham_words.extend(preprocess_text(message))\n",
    "\n",
    "for message in spam_messages:\n",
    "    spam_words.extend(preprocess_text(message))\n",
    "\n",
    "# Count word frequencies\n",
    "ham_word_counts = Counter(ham_words)\n",
    "spam_word_counts = Counter(spam_words)\n",
    "\n",
    "# Get top 10 words for ham and spam\n",
    "top_10_ham = ham_word_counts.most_common(10)\n",
    "top_10_spam = spam_word_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 words in Ham messages:\", top_10_ham)\n",
    "print(\"Top 10 words in Spam messages:\", top_10_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: Create a sample DataFrame (replace this with your actual data)\n",
    "data_train = pd.DataFrame({\n",
    "    'preprocessed_text': [\n",
    "        \"win a free iphone now\",\n",
    "        \"call me when you're free\",\n",
    "        \"claim your reward today\",\n",
    "        \"transfer money to your bank account\",\n",
    "        \"let's meet for lunch\"\n",
    "    ]\n",
    "})\n",
    "data_val = pd.DataFrame({\n",
    "    'preprocessed_text': [\n",
    "        \"get a free euro trip\",\n",
    "        \"deposit money into your account\",\n",
    "        \"congratulations you won a prize\",\n",
    "        \"let's discuss the project\",\n",
    "        \"cheap deals on electronics\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Define money symbols and suspicious words\n",
    "money_simbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "# Add indicators to data_train\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list) * 1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words) * 1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data_train = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"win a free iphone now\",\n",
    "        \"call me when you're free\",\n",
    "        \"claim your reward today\",\n",
    "        \"transfer money to your bank account\",\n",
    "        \"let's meet for lunch\"\n",
    "    ],\n",
    "    'label': ['spam', 'ham', 'spam', 'spam', 'ham']\n",
    "})\n",
    "\n",
    "data_val = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"get a free euro trip\",\n",
    "        \"deposit money into your account\",\n",
    "        \"congratulations you won a prize\",\n",
    "        \"let's discuss the project\",\n",
    "        \"cheap deals on electronics\"\n",
    "    ],\n",
    "    'label': ['spam', 'spam', 'spam', 'ham', 'spam']\n",
    "})\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = vectorizer.fit_transform(data_train['text'])\n",
    "\n",
    "# Transform the validation data\n",
    "X_val = vectorizer.transform(data_val['text'])\n",
    "\n",
    "# Convert the results to DataFrames for better visualization\n",
    "X_train_df = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "X_val_df = pd.DataFrame(X_val.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the results\n",
    "print(\"Training Data BoW Representation:\")\n",
    "print(X_train_df)\n",
    "\n",
    "print(\"\\nValidation Data BoW Representation:\")\n",
    "print(X_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data_train = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"win a free iphone now\",\n",
    "        \"call me when you're free\",\n",
    "        \"claim your reward today\",\n",
    "        \"transfer money to your bank account\",\n",
    "        \"let's meet for lunch\"\n",
    "    ],\n",
    "    'label': ['spam', 'ham', 'spam', 'spam', 'ham']\n",
    "})\n",
    "\n",
    "data_val = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"get a free euro trip\",\n",
    "        \"deposit money into your account\",\n",
    "        \"congratulations you won a prize\",\n",
    "        \"let's discuss the project\",\n",
    "        \"cheap deals on electronics\"\n",
    "    ],\n",
    "    'label': ['spam', 'spam', 'spam', 'ham', 'spam']\n",
    "})\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = vectorizer.fit_transform(data_train['text'])\n",
    "\n",
    "# Transform the validation data\n",
    "X_val = vectorizer.transform(data_val['text'])\n",
    "\n",
    "# Print the shape of the vectorized datasets\n",
    "print(\"Shape of vectorized training data:\", X_train.shape)\n",
    "print(\"Shape of vectorized validation data:\", X_val.shape)\n",
    "\n",
    "# Optional: Convert to DataFrame for better visualization\n",
    "X_train_df = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "X_val_df = pd.DataFrame(X_val.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTraining Data TF-IDF Representation:\")\n",
    "print(X_train_df)\n",
    "\n",
    "print(\"\\nValidation Data TF-IDF Representation:\")\n",
    "print(X_val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data_train = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"win a free iphone now\",\n",
    "        \"call me when you're free\",\n",
    "        \"claim your reward today\",\n",
    "        \"transfer money to your bank account\",\n",
    "        \"let's meet for lunch\"\n",
    "    ],\n",
    "    'label': ['spam', 'ham', 'spam', 'spam', 'ham']\n",
    "})\n",
    "\n",
    "data_val = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"get a free euro trip\",\n",
    "        \"deposit money into your account\",\n",
    "        \"congratulations you won a prize\",\n",
    "        \"let's discuss the project\",\n",
    "        \"cheap deals on electronics\"\n",
    "    ],\n",
    "    'label': ['spam', 'spam', 'spam', 'ham', 'spam']\n",
    "})\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = vectorizer.fit_transform(data_train['text'])\n",
    "y_train = data_train['label']\n",
    "\n",
    "# Transform the validation data\n",
    "X_val = vectorizer.transform(data_val['text'])\n",
    "y_val = data_val['label']\n",
    "\n",
    "# Initialize and train the classifier (Logistic Regression)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation data\n",
    "y_pred = classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text  # Ensure this is a string, not a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv(\"kg_train.csv\")  \n",
    "test_data = pd.read_csv(\"kg_test.csv\")   \n",
    "\n",
    "# Preprocess the text data\n",
    "train_data['preprocessed_text'] = train_data['text'].apply(preprocess_text)\n",
    "test_data['preprocessed_text'] = test_data['text'].apply(preprocess_text)\n",
    "\n",
    "# Vectorize the text data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['preprocessed_text'])\n",
    "y_train = train_data['label']\n",
    "X_test = vectorizer.transform(test_data['preprocessed_text'])\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = classifier.predict(X_val_split)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_val_split, y_val_pred)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val_split, y_val_pred))\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# Save predictions for submission\n",
    "submission = pd.DataFrame({\n",
    "    'test': test_data['test'],  # Ensure 'id' is the correct column name\n",
    "    'label': y_test_pred      # Predicted labels\n",
    "})\n",
    "\n",
    "# Save to a CSV file\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
