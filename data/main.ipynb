{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/h4x8twq57394t3xmh53ddyv40000gn/T/ipykernel_6071/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4771, 2), (1193, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first load the data from the CSV files provided and inspect the content to understand how to divide them into training and test sets.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the provided datasets\n",
    "train_data_path = \"kg_train.csv\"\n",
    "test_data_path = \"kg_test.csv\"\n",
    "\n",
    "# Read the data into pandas DataFrame\n",
    "train_data = pd.read_csv(train_data_path, encoding='latin-1')\n",
    "test_data = pd.read_csv(test_data_path, encoding='latin-1')\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "train_data.head(), test_data.head()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_set, validation_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "train_set.shape, validation_set.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nThis is a paragraph.\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript/CSS by removing anything between <script>...</script> and <style>...</style>\n",
    "    text = re.sub(r'<(script|style).*?>.*?(<\\/\\1>)', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove HTML comments (anything between <!-- and -->)\n",
    "    text = re.sub(r'<!--.*?-->', '', text)\n",
    "\n",
    "    # Remove any remaining HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Sample HTML data for testing\n",
    "sample_html = \"\"\"\n",
    "<html>\n",
    "<head><script>var x = 10;</script><style>body {background-color: #fff;}</style></head>\n",
    "<body>\n",
    "<!-- This is a comment -->\n",
    "<p>This is a paragraph.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "cleaned_text = clean_html(sample_html)\n",
    "cleaned_text  # Cleaned text output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello this is test lets remove pecial charcters'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters (everything except letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove any prefixed 'b' (sometimes appears with byte string representation)\n",
    "    text = re.sub(r'\\bb\\b', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Strip leading/trailing spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Sample text for preprocessing\n",
    "sample_text = \"Hello, b2! This is a 100 test b. Let's remove $pecial char@cters!\"\n",
    "preprocessed_text = preprocess_text(sample_text)\n",
    "preprocessed_text  # Preprocessed text output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/keremsenler/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'example sentence demonstrate removal stopwords.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    # Remove stopwords from the text\n",
    "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "# Sample text for removing stopwords\n",
    "sample_text = \"This is an example sentence to demonstrate the removal of stopwords.\"\n",
    "text_without_stopwords = remove_stopwords(sample_text)\n",
    "text_without_stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/keremsenler/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The boy are playing with the running dogs.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    # Lemmatize each word in the text\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "# Sample text for lemmatization\n",
    "sample_text = \"The boys are playing with the running dogs.\"\n",
    "lemmatized_text = lemmatize_text(sample_text)\n",
    "lemmatized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(state        1096\n",
       " pm            875\n",
       " secretary     626\n",
       " said          599\n",
       " obama         558\n",
       " president     537\n",
       " new           503\n",
       " house         473\n",
       " 30            450\n",
       " 2009          442\n",
       " dtype: int64,\n",
       " 2e             10376\n",
       " money           5650\n",
       " br              5454\n",
       " 2c              5249\n",
       " bank            4681\n",
       " account         4533\n",
       " nbsp            3618\n",
       " div             3001\n",
       " transaction     2842\n",
       " business        2658\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine ham and spam messages into one series\n",
    "combined_messages = pd.concat([ham_messages, spam_messages])\n",
    "\n",
    "# Fit the vectorizer on the combined messages\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "combined_bow = vectorizer.fit_transform(combined_messages)\n",
    "\n",
    "# Separate the bow into ham and spam again\n",
    "ham_bow = combined_bow[:len(ham_messages), :]\n",
    "spam_bow = combined_bow[len(ham_messages):, :]\n",
    "\n",
    "# Convert to DataFrames\n",
    "ham_words = pd.DataFrame(ham_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "spam_words = pd.DataFrame(spam_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get the top 10 words in ham and spam\n",
    "top_ham_words = ham_words.sum().sort_values(ascending=False).head(10)\n",
    "top_spam_words = spam_words.sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "top_ham_words, top_spam_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a function preprocess_text() for text cleaning (from previous steps)\n",
    "\n",
    "# Apply text preprocessing to both train and test data\n",
    "train_data['preprocessed_text'] = train_data['text'].apply(preprocess_text)\n",
    "test_data['preprocessed_text'] = test_data['text'].apply(preprocess_text)\n",
    "\n",
    "# Now you can use 'preprocessed_text' for further processing\n",
    "money_simbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \"deposit\", \"password\"])\n",
    "\n",
    "train_data['money_mark'] = train_data['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "train_data['suspicious_words'] = train_data['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "train_data['text_len'] = train_data['preprocessed_text'].apply(lambda x: len(x))\n",
    "\n",
    "test_data['money_mark'] = test_data['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "test_data['suspicious_words'] = test_data['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "test_data['text_len'] = test_data['preprocessed_text'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5964, 79210)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example: apply CountVectorizer on the 'text' column of the training data\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_bow = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Print the shape of the resulting Bag of Words matrix\n",
    "print(X_bow.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5964, 79210)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Print the shape of the resulting TF-IDF matrix\n",
    "print(X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use TF-IDF or Bag of Words (BoW) features, depending on your choice\n",
    "X = X_tfidf  # or X_bow for Bag of Words\n",
    "y = train_data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Classifier accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
