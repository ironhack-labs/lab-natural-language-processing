{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_23728\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "# Handle missing values\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure 'preprocessed_text' exists (modify this to match your actual column name)\n",
    "data_train[\"preprocessed_text\"] = data_train[\"text\"].astype(str)\n",
    "data_val[\"preprocessed_text\"] = data_val[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
      "535  I have not been able to reach oscar this am. W...      0   \n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
      "557  I can have it announced here on Monday - can't...      0   \n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
      "\n",
      "                                            clean_text  \n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...  \n",
      "535  I have not been able to reach oscar this am. W...  \n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...  \n",
      "557  I can have it announced here on Monday - can't...  \n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...  \n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "def remove_js_css(text):\n",
    "    # Remove inline JavaScript and CSS (everything inside <script> and <style> tags)\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)\n",
    "    return text\n",
    "def remove_html_comments(text):\n",
    "    # Remove HTML comments (e.g., <!-- comment -->)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    return text\n",
    "def remove_html_tags(text):\n",
    "    # Remove remaining HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return text\n",
    "def clean_html(text):\n",
    "    # Remove inline JavaScript and CSS\n",
    "    text = remove_js_css(text)\n",
    "    # Remove HTML comments\n",
    "    text = remove_html_comments(text)\n",
    "    # Remove HTML tags\n",
    "    text = remove_html_tags(text)\n",
    "    return text\n",
    "train_data[\"clean_text\"] = train_data[\"text\"].apply(clean_html)\n",
    "test_data[\"clean_text\"] = test_data[\"text\"].apply(clean_html)\n",
    "\n",
    "print(train_data.head())  # View the cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
      "535  I have not been able to reach oscar this am. W...      0   \n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
      "557  I can have it announced here on Monday - can't...      0   \n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
      "\n",
      "                                            clean_text  \n",
      "29   regards mr nelson smithkindly reply me on my p...  \n",
      "535  have not een able to reach oscar this am we ar...  \n",
      "695  huma abedin bim checking with pat on the will ...  \n",
      "557    can have it announced here on monday cant today  \n",
      "836  bank of africaagence san pedro bp san pedro co...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove all special characters (except spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    text = re.sub(r'^\\w', '', text)\n",
    "    \n",
    "    # Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove prefixed 'b' (if it appears at the beginning of words)\n",
    "    text = re.sub(r'\\bb', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text.strip()  # Remove leading and trailing spaces\n",
    "# Apply the cleaning function to the dataset\n",
    "train_data[\"clean_text\"] = train_data[\"text\"].apply(clean_text)\n",
    "test_data[\"clean_text\"] = test_data[\"text\"].apply(clean_text)\n",
    "\n",
    "# Display the first few rows\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
      "535  I have not been able to reach oscar this am. W...      0   \n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
      "557  I can have it announced here on Monday - can't...      0   \n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
      "\n",
      "                                            clean_text  \n",
      "29   regards mr nelson smithkindly reply private em...  \n",
      "535     een able reach oscar supposed send pdb receive  \n",
      "695  huma abedin bim checking pat work jack jake re...  \n",
      "557                        announced monday cant today  \n",
      "836  bank africaagence san pedro bp san pedro cote ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Tokenize text into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return \" \".join(filtered_words)  # Join the words back into a string\n",
    "# Apply the function to the cleaned text column\n",
    "train_data[\"clean_text\"] = train_data[\"clean_text\"].apply(remove_stopwords)\n",
    "test_data[\"clean_text\"] = test_data[\"clean_text\"].apply(remove_stopwords)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label  \\\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
      "535  I have not been able to reach oscar this am. W...      0   \n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
      "557  I can have it announced here on Monday - can't...      0   \n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
      "\n",
      "                                            clean_text  \n",
      "29   regard mr nelson smithkindly reply private ema...  \n",
      "535     een able reach oscar supposed send pdb receive  \n",
      "695  huma abedin bim checking pat work jack jake re...  \n",
      "557                        announced monday cant today  \n",
      "836  bank africaagence san pedro bp san pedro cote ...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "nltk.download('omw-1.4')  # For better word understanding\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)  # Tokenize text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Apply lemmatization\n",
    "    return \" \".join(lemmatized_words)  # Join words back into a sentence\n",
    "# Apply the function to the cleaned text column\n",
    "train_data[\"clean_text\"] = train_data[\"clean_text\"].apply(lemmatize_text)\n",
    "test_data[\"clean_text\"] = test_data[\"clean_text\"].apply(lemmatize_text)\n",
    "\n",
    "# Display the cleaned text\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Spam Words: []\n",
      "Top 10 Ham Words: []\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from collections import Counter\n",
    "\n",
    "# Separate spam and ham messages\n",
    "spam_messages = train_data[train_data[\"label\"] == \"spam\"][\"clean_text\"]\n",
    "ham_messages = train_data[train_data[\"label\"] == \"ham\"][\"clean_text\"]\n",
    "def get_top_words(text_series, n=10):\n",
    "    all_words = \" \".join(text_series).split()  # Combine all text and split into words\n",
    "    word_counts = Counter(all_words)  # Count word frequencies\n",
    "    return word_counts.most_common(n)  # Return top N words\n",
    "top_spam_words = get_top_words(spam_messages, 10)\n",
    "top_ham_words = get_top_words(ham_messages, 10)\n",
    "\n",
    "print(\"Top 10 Spam Words:\", top_spam_words)\n",
    "print(\"Top 10 Ham Words:\", top_ham_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>1</td>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>0</td>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>1</td>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
       "535  I have not been able to reach oscar this am. W...      0   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
       "557  I can have it announced here on Monday - can't...      0   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
       "\n",
       "                                     preprocessed_text  money_mark  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...           1   \n",
       "535  I have not been able to reach oscar this am. W...           1   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...           1   \n",
       "557  I can have it announced here on Monday - can't...           1   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...           1   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0       106  \n",
       "535                 0       101  \n",
       "695                 0       141  \n",
       "557                 0        52  \n",
       "836                 1      1750  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1]]\n",
      "Vocabulary: ['are' 'click' 'coffee' 'congratulations' 'discount' 'exclusive' 'for'\n",
      " 'free' 'get' 'hello' 'here' 'how' 'let' 'meet' 'money' 'now' 'offer'\n",
      " 'prize' 'to' 'tomorrow' 'win' 'won' 'you']\n",
      "Shape of Train Data: (800, 23374)\n",
      "Shape of Validation Data: (200, 23374)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample dataset (use your actual dataset instead)\n",
    "text_samples = [\n",
    "    \"Free money now! Click here to win\",\n",
    "    \"Hello, how are you?\",\n",
    "    \"Exclusive offer! Get a discount now\",\n",
    "    \"Let's meet tomorrow for coffee\",\n",
    "    \"Congratulations, you won a prize!\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(text_samples)\n",
    "\n",
    "# Convert sparse matrix to array for readability\n",
    "X_array = X.toarray()\n",
    "print(X_array)\n",
    "# Print feature names (unique words in corpus)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "# Assuming `data_train['preprocessed_text']` contains cleaned text\n",
    "X_train = vectorizer.fit_transform(data_train[\"preprocessed_text\"])\n",
    "X_val = vectorizer.transform(data_val[\"preprocessed_text\"])\n",
    "\n",
    "print(\"Shape of Train Data:\", X_train.shape)\n",
    "print(\"Shape of Validation Data:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF Train Data: (800, 23374)\n",
      "Shape of TF-IDF Validation Data: (200, 23374)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data_train[\"preprocessed_text\"])\n",
    "\n",
    "# Transform the validation data (do NOT fit again)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(data_val[\"preprocessed_text\"])\n",
    "print(\"Shape of TF-IDF Train Data:\", X_train_tfidf.shape)\n",
    "print(\"Shape of TF-IDF Validation Data:\", X_val_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       125\n",
      "           1       0.97      0.89      0.93        75\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.95      0.94      0.95       200\n",
      "weighted avg       0.95      0.95      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the model\n",
    "classifier = LogisticRegression()\n",
    "# Train the model using the TF-IDF features\n",
    "classifier.fit(X_train_tfidf, data_train[\"label\"])  # Assuming \"label\" column contains spam (1) and ham (0)\n",
    "# Predict on validation set\n",
    "y_pred = classifier.predict(X_val_tfidf)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(data_val[\"label\"], y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(data_val[\"label\"], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
