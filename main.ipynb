{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiago\\AppData\\Local\\Temp\\ipykernel_25972\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(r\"C:\\Users\\tiago\\OneDrive\\Ambiente de Trabalho\\NovaPasta\\lab-natural-language-processing\\data\\kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data=data\n",
    "test_data=pd.read_csv(r\"C:\\Users\\tiago\\OneDrive\\Ambiente de Trabalho\\NovaPasta\\lab-natural-language-processing\\data\\kg_test.csv\",encoding='latin-1')\n",
    "\n",
    "# Split the training set into two parts (80% for training, 20% for validation)\n",
    "X_train, X_val = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the test set into two parts (80% for testing, 20% for evaluation)\n",
    "#X_test, X_val_test = train_test_split(test_data, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tiago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tiago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean HTML content\n",
    "def clean_html(html_code):\n",
    "    # Step 1: Remove inline JavaScript and CSS (inside <script> and <style> tags)\n",
    "    html_code = re.sub(r'<script.*?>.*?</script>', '', html_code, flags=re.DOTALL)\n",
    "    html_code = re.sub(r'<style.*?>.*?</style>', '', html_code, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 2: Remove HTML comments\n",
    "    html_code = re.sub(r'<!--.*?-->', '', html_code, flags=re.DOTALL)\n",
    "    \n",
    "    # Step 3: Remove all remaining HTML tags\n",
    "    html_code = re.sub(r'<.*?>', '', html_code)\n",
    "    \n",
    "    return html_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_html function to the relevant column of X_train\n",
    "\n",
    "X_train['cleaned_content'] = X_train['text'].apply(clean_html)\n",
    "X_val['cleaned_content'] = X_val['text'].apply(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # 1. Remove special characters (excluding alphabets and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # 2. Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 3. Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "\n",
    "    # 4. Remove single characters from the start\n",
    "    text = re.sub(r'^\\b\\w\\b', '', text)\n",
    "\n",
    "    # 5. Substitute multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # 6. Remove prefixed 'b' (if it's part of a string like 'bhello')\n",
    "    text = re.sub(r'^\\bb', '', text)\n",
    "\n",
    "    # 7. Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove any leading or trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_text function to the relevant column (replace 'text_column' with actual column name)\n",
    "\n",
    "X_train['cleaned_content'] = X_train['cleaned_content'].apply(clean_text)\n",
    "X_val['cleaned_content'] = X_val['cleaned_content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    \"\"\"Remove punctuation from a tokenized sentence.\"\"\"\n",
    "    return [word for word in sentence if word not in string.punctuation]\n",
    "\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"Remove stopwords from a list of tokens.\"\"\"\n",
    "    return [word for word in sentence if word not in stopwords.words('english')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tiago\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "X_train['cleaned_content'] = X_train['cleaned_content'].apply(\n",
    "    lambda x: word_tokenize(x) if isinstance(x, str) else x)\n",
    "\n",
    "X_train['cleaned_content'] = X_train['cleaned_content'].apply(remove_punctuation)\n",
    "X_train['cleaned_content'] = X_train['cleaned_content'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val['cleaned_content'] = X_val['cleaned_content'].apply(\n",
    "    lambda x: word_tokenize(x) if isinstance(x, str) else x)\n",
    "\n",
    "X_val['cleaned_content'] = X_val['cleaned_content'].apply(remove_punctuation)\n",
    "X_val['cleaned_content'] = X_val['cleaned_content'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:                                                   text  label  \\\n",
      "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
      "535  I have not been able to reach oscar this am. W...      0   \n",
      "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
      "557  I can have it announced here on Monday - can't...      0   \n",
      "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
      "..                                                 ...    ...   \n",
      "106  7653 2612ADAMA IBRAHIM________________________...      1   \n",
      "270             What does that mean for our schedules?      0   \n",
      "860  Dear Friend,My Compliment to you,I guess this ...      1   \n",
      "435  Dear PRESIDENT=2FDIRECTOR=2C My name is Mr=2E ...      1   \n",
      "102  Let me know if today or tomorrow works for you...      0   \n",
      "\n",
      "                                       cleaned_content  \n",
      "29   [regards, mr, nelson, smithkindly, reply, priv...  \n",
      "535  [able, reach, oscar, supposed, send, pdb, rece...  \n",
      "695  [huma, abedin, bim, checking, pat, work, jack,...  \n",
      "557                   [announced, monday, cant, today]  \n",
      "836  [bank, africaagence, san, pedro, bp, san, pedr...  \n",
      "..                                                 ...  \n",
      "106  [adama, ibrahimtout, savoir, sur, la, scurit, ...  \n",
      "270                                  [mean, schedules]  \n",
      "860  [dear, friendmy, compliment, youi, guess, lett...  \n",
      "435  [dear, presidentfdirectorc, name, mre, micheal...  \n",
      "102  [let, know, today, tomorrow, works, would, rat...  \n",
      "\n",
      "[800 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"Apply lemmatization to a list of tokens.\"\"\"\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "lemmatized_X_train=X_train['cleaned_content'].apply(lemmatize_tokens)\n",
    "lemmatized_X_val=X_val['cleaned_content'].apply(lemmatize_tokens)\n",
    "\n",
    "# Print the original filtered_X_train and their lemmatized versions\n",
    "print(\"Original Tokens:\", X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized Tokens train: 29     [regard, mr, nelson, smithkindly, reply, priva...\n",
      "535    [able, reach, oscar, supposed, send, pdb, rece...\n",
      "695    [huma, abedin, bim, checking, pat, work, jack,...\n",
      "557                     [announced, monday, cant, today]\n",
      "836    [bank, africaagence, san, pedro, bp, san, pedr...\n",
      "                             ...                        \n",
      "106    [adama, ibrahimtout, savoir, sur, la, scurit, ...\n",
      "270                                     [mean, schedule]\n",
      "860    [dear, friendmy, compliment, youi, guess, lett...\n",
      "435    [dear, presidentfdirectorc, name, mre, micheal...\n",
      "102    [let, know, today, tomorrow, work, would, rath...\n",
      "Name: cleaned_content, Length: 800, dtype: object\n",
      "lemmatized Tokens Val: 521    [dear, sirc, wish, go, offer, consider, partne...\n",
      "737    [take, mind, balkan, second, see, great, plug,...\n",
      "740                          [pls, keep, update, coming]\n",
      "660    [christ, bethel, hospital, rue, aboboteabidjan...\n",
      "411    [sbwhoeopfriday, february, amhre, bravo, brava...\n",
      "                             ...                        \n",
      "408               [sorry, yes, exactlywe, shy, tomorrow]\n",
      "332    [dearcgood, dayei, know, message, come, supris...\n",
      "208                                                [fyi]\n",
      "613    [greeting, dear, friend, please, permit, conta...\n",
      "78                [car, way, airport, talk, call, berry]\n",
      "Name: cleaned_content, Length: 200, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"lemmatized Tokens train:\", lemmatized_X_train)\n",
    "print(\"lemmatized Tokens Val:\", lemmatized_X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (Words in BoW):\n",
      " ['aac' 'aaclocated' 'aae' ... 'zumadirector' 'zumae' 'zurich']\n",
      "BoW Matrix (Word Frequency Representation):\n",
      "      aac  aaclocated  aae  aag  aaronovitchon  abacha  abachabefore  abachac  \\\n",
      "0      0           0    0    0              0       0             0        0   \n",
      "1      0           0    0    0              0       0             0        0   \n",
      "2      0           0    0    0              0       0             0        0   \n",
      "3      0           0    0    0              0       0             0        0   \n",
      "4      0           0    0    0              0       0             0        0   \n",
      "..   ...         ...  ...  ...            ...     ...           ...      ...   \n",
      "779    0           0    0    0              0       0             0        0   \n",
      "780    0           0    0    0              0       0             0        0   \n",
      "781    0           0    0    0              0       0             0        0   \n",
      "782    0           0    0    0              0       0             0        0   \n",
      "783    0           0    0    0              0       0             0        0   \n",
      "\n",
      "     abachace  abachaco  ...  zongo  zongothe  zuhair  \\\n",
      "0           0         0  ...      0         0       0   \n",
      "1           0         0  ...      0         0       0   \n",
      "2           0         0  ...      0         0       0   \n",
      "3           0         0  ...      0         0       0   \n",
      "4           0         0  ...      0         0       0   \n",
      "..        ...       ...  ...    ...       ...     ...   \n",
      "779         0         0  ...      0         0       0   \n",
      "780         0         0  ...      0         0       0   \n",
      "781         0         0  ...      0         0       0   \n",
      "782         0         0  ...      0         0       0   \n",
      "783         0         0  ...      0         0       0   \n",
      "\n",
      "     zulatoebikozulatonetscapeenet  zulatoffffffffffffffffff  zuma  zumac  \\\n",
      "0                                0                         0     0      0   \n",
      "1                                0                         0     0      0   \n",
      "2                                0                         0     0      0   \n",
      "3                                0                         0     0      0   \n",
      "4                                0                         0     0      0   \n",
      "..                             ...                       ...   ...    ...   \n",
      "779                              0                         0     0      0   \n",
      "780                              0                         0     0      0   \n",
      "781                              0                         0     0      0   \n",
      "782                              0                         0     0      0   \n",
      "783                              0                         0     0      0   \n",
      "\n",
      "     zumadirector  zumae  zurich  \n",
      "0               0      0       0  \n",
      "1               0      0       0  \n",
      "2               0      0       0  \n",
      "3               0      0       0  \n",
      "4               0      0       0  \n",
      "..            ...    ...     ...  \n",
      "779             0      0       0  \n",
      "780             0      0       0  \n",
      "781             0      0       0  \n",
      "782             0      0       0  \n",
      "783             0      0       0  \n",
      "\n",
      "[784 rows x 16933 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# Convert each list of tokens into a string\n",
    "lemmatized_X_train_str = [\" \".join(words) for words in lemmatized_X_train if len(words) > 0]  \n",
    "lemmatized_X_val_str = [\" \".join(words) for words in lemmatized_X_val if len(words) > 0]  \n",
    "#  Fix: Remove empty lists\n",
    "# Step 1: Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a BoW representation\n",
    "bow_matrix = vectorizer.fit_transform(lemmatized_X_train_str)\n",
    "\n",
    "# Get the vocabulary (words)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "#Convert Sparse Matrix to Array for Better Visualization\n",
    "bow_array = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print Results\n",
    "print(\"Vocabulary (Words in BoW):\\n\", words)\n",
    "print(\"BoW Matrix (Word Frequency Representation):\\n\", bow_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert this varable like we did before \n",
    "#X_val_str = [\" \".join(words) for words in X_val]\n",
    "#X_val_df= pd.DataFrame({'preprocessed_text': X_val_str})\n",
    "\n",
    "# Convert lists into DataFrames (ENSURE THIS STEP)\n",
    "#bow_array_X_train = pd.DataFrame(bow_array, columns=vectorizer.get_feature_names_out())  # Convert BoW to DataFrame\n",
    "\n",
    "#  Add preprocessed text to DataFrame (so `.str.contains()` works)\n",
    "#bow_array_X_train['preprocessed_text'] = lemmatized_X_train_str  # Ensure original text is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>money_mark</th>\n",
       "      <th>suspicious_words</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>----------- REGARDS, MR NELSON SMITH.KINDLY RE...</td>\n",
       "      <td>1</td>\n",
       "      <td>[regards, mr, nelson, smithkindly, reply, priv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>I have not been able to reach oscar this am. W...</td>\n",
       "      <td>0</td>\n",
       "      <td>[able, reach, oscar, supposed, send, pdb, rece...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>; Huma Abedin B6I'm checking with Pat on the 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>[huma, abedin, bim, checking, pat, work, jack,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>I can have it announced here on Monday - can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>[announced, monday, cant, today]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...</td>\n",
       "      <td>1</td>\n",
       "      <td>[bank, africaagence, san, pedro, bp, san, pedr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  \\\n",
       "29   ----------- REGARDS, MR NELSON SMITH.KINDLY RE...      1   \n",
       "535  I have not been able to reach oscar this am. W...      0   \n",
       "695  ; Huma Abedin B6I'm checking with Pat on the 5...      0   \n",
       "557  I can have it announced here on Monday - can't...      0   \n",
       "836      BANK OF AFRICAAGENCE SAN PEDRO14 BP 1210 S...      1   \n",
       "\n",
       "                                       cleaned_content  money_mark  \\\n",
       "29   [regards, mr, nelson, smithkindly, reply, priv...           0   \n",
       "535  [able, reach, oscar, supposed, send, pdb, rece...           0   \n",
       "695  [huma, abedin, bim, checking, pat, work, jack,...           0   \n",
       "557                   [announced, monday, cant, today]           0   \n",
       "836  [bank, africaagence, san, pedro, bp, san, pedr...           0   \n",
       "\n",
       "     suspicious_words  text_len  \n",
       "29                  0         8  \n",
       "535                 0         7  \n",
       "695                 0        14  \n",
       "557                 0         4  \n",
       "836                 0       140  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "X_train['money_mark'] = X_train['cleaned_content'].str.contains(money_simbol_list, regex=True, na=False) * 1\n",
    "X_train['suspicious_words'] = X_train['cleaned_content'].str.contains(suspicious_words, regex=True, na=False) * 1\n",
    "X_train['text_len'] = X_train['cleaned_content'].apply(lambda x: len(x))\n",
    "\n",
    "X_val['money_mark'] = X_val['cleaned_content'].str.contains(money_simbol_list, regex=True, na=False) * 1\n",
    "X_val['suspicious_words'] = X_val['cleaned_content'].str.contains(suspicious_words, regex=True, na=False) * 1\n",
    "X_val['text_len'] = X_val['cleaned_content'].apply(lambda x: len(x))\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to Implement BoW with CountVectorizer\n",
    "#Prepare the text data (list of sentences/documents).\n",
    "#Initialize CountVectorizer from sklearn.feature_extraction.text.\n",
    "#Fit and transform the text data into a BoW matrix.\n",
    "#Convert the sparse matrix to an array for better readability.\n",
    "#Display the words and the BoW matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose lemmatized_X_train is a Series of token lists\n",
    "valid_mask = lemmatized_X_train.apply(lambda tokens: len(tokens) > 0)\n",
    "filtered_X_train = X_train[valid_mask]\n",
    "filtered_lemmatized_train = lemmatized_X_train[valid_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask based on whether the cleaned content yields tokens\n",
    "valid_mask = lemmatized_X_val.apply(lambda tokens: len(tokens) > 0)\n",
    "filtered_X_val = X_val[valid_mask]\n",
    "filtered_lemmatized_val = lemmatized_X_val[valid_mask]\n",
    "\n",
    "# Convert token lists to strings without dropping any rows (they are already filtered)\n",
    "lemmatized_X_val_str = [\" \".join(words) for words in filtered_lemmatized_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (Words in TF-IDF):\n",
      " ['aaclocated' 'aae' 'abacha' ... 'zone' 'zuhair' 'zuma']\n",
      "TF-IDF Matrix (Word Weight Representation):\n",
      "      aaclocated  aae  abacha  abachac  abachae  abad  abandoned  abbas  abdul  \\\n",
      "0           0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "1           0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "2           0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "3           0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "4           0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "..          ...  ...     ...      ...      ...   ...        ...    ...    ...   \n",
      "779         0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "780         0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "781         0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "782         0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "783         0.0  0.0     0.0      0.0      0.0   0.0        0.0    0.0    0.0   \n",
      "\n",
      "     abdull  ...  yung  zahrulmalaysian  zenith  zimbabwe  zimbabwean  \\\n",
      "0       0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "1       0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "2       0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "3       0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "4       0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "..      ...  ...   ...              ...     ...       ...         ...   \n",
      "779     0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "780     0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "781     0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "782     0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "783     0.0  ...   0.0              0.0     0.0       0.0         0.0   \n",
      "\n",
      "     zimbabwei  zimbabwethis  zone  zuhair  zuma  \n",
      "0          0.0           0.0   0.0     0.0   0.0  \n",
      "1          0.0           0.0   0.0     0.0   0.0  \n",
      "2          0.0           0.0   0.0     0.0   0.0  \n",
      "3          0.0           0.0   0.0     0.0   0.0  \n",
      "4          0.0           0.0   0.0     0.0   0.0  \n",
      "..         ...           ...   ...     ...   ...  \n",
      "779        0.0           0.0   0.0     0.0   0.0  \n",
      "780        0.0           0.0   0.0     0.0   0.0  \n",
      "781        0.0           0.0   0.0     0.0   0.0  \n",
      "782        0.0           0.0   0.0     0.0   0.0  \n",
      "783        0.0           0.0   0.0     0.0   0.0  \n",
      "\n",
      "[784 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "#  Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Convert BoW into TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform([\" \".join(words) for words in filtered_lemmatized_train])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(lemmatized_X_val_str)\n",
    "\n",
    "# Get the vocabulary (words)\n",
    "words_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Convert Sparse Matrix to DataFrame for Better Visualization\n",
    "\n",
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "X_val_tfidf_df = pd.DataFrame(X_val_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print Results\n",
    "print(\"Vocabulary (Words in TF-IDF):\\n\", words_tfidf)\n",
    "print(\"TF-IDF Matrix (Word Weight Representation):\\n\", X_train_tfidf_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       124\n",
      "           1       1.00      0.91      0.95        75\n",
      "\n",
      "    accuracy                           0.96       199\n",
      "   macro avg       0.97      0.95      0.96       199\n",
      "weighted avg       0.97      0.96      0.96       199\n",
      "\n",
      "First 5 predictions: [1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming your training and validation DataFrames include a 'label' column.\n",
    "# If the column name is different, adjust accordingly.\n",
    "\n",
    "# Extract labels from your training and validation datasets\n",
    "y_train = filtered_X_train['label']\n",
    "y_val_filtered = filtered_X_val['label']\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the classifier using the TF-IDF features\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "val_predictions = logreg.predict(X_val_tfidf)\n",
    "\n",
    "print(classification_report(y_val_filtered, val_predictions))\n",
    "\n",
    "# Print out the first 3 predictions from the validation set\n",
    "print(\"First 5 predictions:\", val_predictions[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
