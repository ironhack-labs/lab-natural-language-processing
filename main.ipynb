{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macat\\AppData\\Local\\Temp\\ipykernel_18364\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5964, 2) (5964, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will do.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nora--Cheryl has emailed dozens of memos about...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Sir=2FMadam=2C I know that this proposal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fyi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...      1\n",
       "1                                           Will do.      0\n",
       "2  Nora--Cheryl has emailed dozens of memos about...      0\n",
       "3  Dear Sir=2FMadam=2C I know that this proposal ...      1\n",
       "4                                                fyi      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"kg_train.csv\", encoding=\"latin-1\")\n",
    "test = pd.read_csv(\"kg_test.csv\",  encoding=\"latin-1\")\n",
    "\n",
    "print(data.shape, test.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"kg_train.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 800\n",
      "Validation size: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,        \n",
    "    stratify=y,            \n",
    "    random_state=42        \n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_file(fname: str):\n",
    "    \"\"\"Search for a file name in a few likely places. Returns Path or None.\"\"\"\n",
    "    candidates = [\n",
    "        Path(fname),\n",
    "        Path.cwd() / fname,\n",
    "        Path(\"/mnt/data\") / fname,\n",
    "        Path(\"data\") / fname,\n",
    "        Path(\"dataset\") / fname,\n",
    "        Path(\"datasets\") / fname,\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.is_file():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _read_csv_safely(fname: str, encoding=\"latin-1\"):\n",
    "    p = _find_file(fname)\n",
    "    if p is None:\n",
    "        return None, None\n",
    "    try:\n",
    "        df = pd.read_csv(p, encoding=encoding)\n",
    "        return df, str(p)\n",
    "    except Exception as e:\n",
    "        return None, f\"Found at {p} but failed to read: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training data source: kg_train.csv\n",
      "[INFO] Test data source:     kg_test.csv\n",
      "[INFO] Train shape: (5964, 2) | Test shape: (5964, 1)\n"
     ]
    }
   ],
   "source": [
    "data, data_path_info = _read_csv_safely(\"kg_train.csv\")\n",
    "test, test_path_info = _read_csv_safely(\"kg_test.csv\")\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "if data is None:\n",
    "    # Tiny fallback so the notebook still runs.\n",
    "    # Two \"spam-ish\" examples and two \"ham\" (legit) examples.\n",
    "    data = pd.DataFrame({\n",
    "        TEXT_COL: [\n",
    "            \"CONGRATS!!! You won $1000. Click here to claim your prize now!!!\",\n",
    "            \"<html><body>Dear user, your account needs verification. Send your password.</body></html>\",\n",
    "            \"Hi team, here are the minutes from today's meeting attached.\",\n",
    "            \"Lunch tomorrow at 12? Let's meet by the cafeteria.\"\n",
    "        ],\n",
    "        LABEL_COL: [1, 1, 0, 0],\n",
    "    })\n",
    "    data_path_info = \"FAKE (in-notebook) sample data used because kg_train.csv was not found.\"\n",
    "else:\n",
    "    # Ensure the expected columns exist; if the original CSV uses other names, try to map them.\n",
    "    lowered_cols = {c.lower(): c for c in data.columns}\n",
    "    if TEXT_COL not in data.columns:\n",
    "        # Best guess mappings\n",
    "        for guess in [\"email_text\", \"message\", \"content\", \"body\", \"text_\"]:\n",
    "            if guess in lowered_cols:\n",
    "                data.rename(columns={lowered_cols[guess]: TEXT_COL}, inplace=True)\n",
    "                break\n",
    "    if LABEL_COL not in data.columns:\n",
    "        for guess in [\"label\", \"spam\", \"target\", \"class\", \"is_spam\"]:\n",
    "            if guess in lowered_cols:\n",
    "                data.rename(columns={lowered_cols[guess]: LABEL_COL}, inplace=True)\n",
    "                break\n",
    "    # If still missing, raise a friendly error\n",
    "    if TEXT_COL not in data.columns or LABEL_COL not in data.columns:\n",
    "        raise ValueError(f\"Your training CSV must have '{TEXT_COL}' and '{LABEL_COL}' columns (or recognizable equivalents). \"\n",
    "                         f\"Present columns: {list(data.columns)}\")\n",
    "\n",
    "if test is None:\n",
    "    test = pd.DataFrame({TEXT_COL: [\"This is a placeholder test email.\"], LABEL_COL: [0]})\n",
    "    test_path_info = \"FAKE (in-notebook) sample test data used because kg_test.csv was not found.\"\n",
    "\n",
    "print(f\"[INFO] Training data source: {data_path_info}\")\n",
    "print(f\"[INFO] Test data source:     {test_path_info}\")\n",
    "print(f\"[INFO] Train shape: {data.shape} | Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# detect which text column exists\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_train\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     34\u001b[0m         TEXT_COL \u001b[38;5;241m=\u001b[39m candidate\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "def strip_html(raw_html: str) -> str:\n",
    "    \"\"\"Remove HTML tags, scripts, styles, and comments from a string.\"\"\"\n",
    "    if not isinstance(raw_html, str):\n",
    "        return \"\"\n",
    "    \n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    \n",
    "    # remove script and style\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    # remove HTML comments\n",
    "    for c in soup.find_all(string=lambda t: isinstance(t, Comment)):\n",
    "        c.extract()\n",
    "    \n",
    "    # get visible text\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    return text.strip()\n",
    "\n",
    "# --------------------------\n",
    "# Apply to your dataframes\n",
    "# --------------------------\n",
    "\n",
    "def add_clean_text(df: pd.DataFrame, text_col: str) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"text_nohtml\"] = df[text_col].fillna(\"\").astype(str).map(strip_html)\n",
    "    return df\n",
    "\n",
    "# detect which text column exists\n",
    "for candidate in [\"text\", \"message\", \"body\", \"content\"]:\n",
    "    if candidate in data_train.columns:\n",
    "        TEXT_COL = candidate\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(f\"No suitable text column found in {list(data_train.columns)}\")\n",
    "\n",
    "# apply to train and val sets\n",
    "data_train = add_clean_text(data_train, TEXT_COL)\n",
    "data_val   = add_clean_text(data_val, TEXT_COL)\n",
    "\n",
    "# quick check\n",
    "data_train[[\"text_nohtml\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_ in (data, test):\n",
    "    df_[TEXT_COL] = df_[TEXT_COL].fillna(\"\").astype(str)\n",
    "    if LABEL_COL in df_.columns:\n",
    "        df_[LABEL_COL] = df_[LABEL_COL].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: This is just a simple example to show how stopword removal works!\n",
      "After: simple example show stopword removal works\n"
     ]
    }
   ],
   "source": [
    "def get_stopwords():\n",
    "    try:\n",
    "        from nltk.corpus import stopwords\n",
    "        try:\n",
    "            return set(stopwords.words(\"english\"))\n",
    "        except LookupError:\n",
    "            # Try downloading if missing\n",
    "            import nltk\n",
    "            nltk.download(\"stopwords\", quiet=True)\n",
    "            return set(stopwords.words(\"english\"))\n",
    "    except Exception:\n",
    "        # Fallback minimal list\n",
    "        return set(\"\"\"a an and are as at be by for from has have he her hers him his i in is it its \n",
    "                      me my not of on or our ours she that the their them they this to was we were will with you your\"\"\"\n",
    "                   .split())\n",
    "\n",
    "STOP_WORDS = get_stopwords()\n",
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower().translate(PUNCT_TABLE)\n",
    "    tokens = re.sub(r\"[^a-z\\s]\", \" \", text).split()\n",
    "    return \" \".join([t for t in tokens if t not in STOP_WORDS])\n",
    "sample = \"This is just a simple example to show how stopword removal works!\"\n",
    "print(\"Before:\", sample)\n",
    "print(\"After:\", remove_stopwords(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatizer():\n",
    "    try:\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        import nltk\n",
    "    \n",
    "        try:\n",
    "            nltk.data.find(\"corpora/wordnet\")\n",
    "        except LookupError:\n",
    "            nltk.download(\"wordnet\", quiet=True)\n",
    "            nltk.download(\"omw-1.4\", quiet=True)  # for more lemmas\n",
    "        return WordNetLemmatizer().lemmatize\n",
    "    except Exception:\n",
    "        \n",
    "        return lambda w: w\n",
    "\n",
    "LEMMATIZE = get_lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: The cats are running faster than the dogs\n",
      "After: the cat are running faster than the dog\n"
     ]
    }
   ],
   "source": [
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def lemmatize_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Lowercase + remove punctuation\n",
    "    text = text.lower().translate(PUNCT_TABLE)\n",
    "    # Keep only letters and spaces\n",
    "    tokens = re.sub(r\"[^a-z\\s]\", \" \", text).split()\n",
    "    # Lemmatize each token\n",
    "    return \" \".join([LEMMATIZE(t) for t in tokens])\n",
    "sample = \"The cats are running faster than the dogs\"\n",
    "print(\"Before:\", sample)\n",
    "print(\"After:\", lemmatize_text(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using dataframe: train_df with columns: ['text', 'label', 'text_nohtml', 'text_clean']\n",
      "[INFO] Using TEXT_COL='text_nohtml', LABEL_COL='label'\n",
      "\n",
      "Top 10 words in HAM messages:\n",
      "the                1602\n",
      "to                 929\n",
      "and                737\n",
      "of                 694\n",
      "a                  570\n",
      "in                 548\n",
      "that               379\n",
      "s                  341\n",
      "for                340\n",
      "is                 336\n",
      "\n",
      "Top 10 words in SPAM messages:\n",
      "the                5633\n",
      "to                 4359\n",
      "of                 3973\n",
      "and                3138\n",
      "i                  2859\n",
      "in                 2604\n",
      "you                2577\n",
      "a                  2387\n",
      "e                  2266\n",
      "this               2082\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "DF = None\n",
    "for df_name in [\"data_train\", \"train_df\", \"data\", \"df\", \"dataset\"]:\n",
    "    if df_name in globals():\n",
    "        cand = globals()[df_name]\n",
    "        if isinstance(cand, pd.DataFrame) and len(cand.columns) > 0:\n",
    "            DF = cand\n",
    "            print(f\"[INFO] Using dataframe: {df_name} with columns: {list(DF.columns)}\")\n",
    "            break\n",
    "\n",
    "if DF is None:\n",
    "    raise ValueError(\"Couldn't find a dataframe. Expected one of: data_train, train_df, data, df, dataset.\")\n",
    "\n",
    "TEXT_CANDIDATES  = [\n",
    "    \"preprocessed_text\", \"text_nohtml\", \"text\",\n",
    "    \"message\", \"email_text\", \"content\", \"body\", \"sms\", \"message_text\"\n",
    "]\n",
    "LABEL_CANDIDATES = [\"label\", \"target\", \"spam\", \"class\", \"is_spam\", \"Category\", \"category\", \"Label\"]\n",
    "\n",
    "TEXT_COL = next((c for c in TEXT_CANDIDATES  if c in DF.columns), None)\n",
    "LABEL_COL = next((c for c in LABEL_CANDIDATES if c in DF.columns), None)\n",
    "\n",
    "if TEXT_COL is None:\n",
    "    raise ValueError(f\"No text column found. Looked for: {TEXT_CANDIDATES}. \"\n",
    "                     f\"Available columns: {list(DF.columns)}\")\n",
    "\n",
    "if LABEL_COL is None:\n",
    "    raise ValueError(f\"No label column found. Looked for: {LABEL_CANDIDATES}. \"\n",
    "                     f\"Available columns: {list(DF.columns)}\")\n",
    "\n",
    "print(f\"[INFO] Using TEXT_COL='{TEXT_COL}', LABEL_COL='{LABEL_COL}'\")\n",
    "\n",
    "labels = DF[LABEL_COL]\n",
    "\n",
    "# If labels are already numeric binary (0/1), keep them.\n",
    "if labels.dtype.kind in \"iu\" and set(labels.unique()) <= {0,1}:\n",
    "    y = labels.astype(int)\n",
    "else:\n",
    "    # Convert common string labels\n",
    "    mapping = {}\n",
    "    uniq = set(str(v).strip().lower() for v in labels.unique())\n",
    "    if \"spam\" in uniq and \"ham\" in uniq:\n",
    "        mapping = {\"spam\": 1, \"ham\": 0}\n",
    "    elif \"spam\" in uniq and \"not spam\" in uniq:\n",
    "        mapping = {\"spam\": 1, \"not spam\": 0}\n",
    "    elif \"pos\" in uniq and \"neg\" in uniq:\n",
    "        mapping = {\"pos\": 1, \"neg\": 0}\n",
    "    else:\n",
    "        # Fallback: try to infer by name of LABEL_COL\n",
    "        # If column name suggests spam, treat nonzero/True as spam\n",
    "        y = labels.astype(str).str.lower().isin([\"1\",\"true\",\"spam\"]).astype(int)\n",
    "        print(\"[WARN] Unrecognized label values. Interpreting '1/true/spam' as spam.\")\n",
    "    if mapping:\n",
    "        y = labels.astype(str).str.strip().str.lower().map(mapping)\n",
    "        if y.isna().any():\n",
    "            raise ValueError(f\"Some labels couldn't be mapped with {mapping}. Unique labels: {uniq}\")\n",
    "\n",
    "def tokenize(text):\n",
    "    # Lowercase, keep only letters, split by whitespace\n",
    "    return re.findall(r\"[a-zA-Z]+\", str(text).lower())\n",
    "\n",
    "ham_texts  = DF.loc[y == 0, TEXT_COL].fillna(\"\").tolist()\n",
    "spam_texts = DF.loc[y == 1, TEXT_COL].fillna(\"\").tolist()\n",
    "\n",
    "ham_counter  = Counter()\n",
    "spam_counter = Counter()\n",
    "\n",
    "for msg in ham_texts:\n",
    "    ham_counter.update(tokenize(msg))\n",
    "for msg in spam_texts:\n",
    "    spam_counter.update(tokenize(msg))\n",
    "\n",
    "top_ham  = ham_counter.most_common(10)\n",
    "top_spam = spam_counter.most_common(10)\n",
    "\n",
    "print(\"\\nTop 10 words in HAM messages:\")\n",
    "for word, count in top_ham:\n",
    "    print(f\"{word:<18} {count}\")\n",
    "\n",
    "print(\"\\nTop 10 words in SPAM messages:\")\n",
    "for word, count in top_spam:\n",
    "    print(f\"{word:<18} {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpick_text_col\u001b[39m(df: \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_nohtml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def pick_text_col(df: pd.DataFrame):\n",
    "    for c in [\"preprocessed_text\", \"text_nohtml\", \"text\", \"message\", \"body\", \"content\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(f\"No suitable text column found in: {list(df.columns)}\")\n",
    "\n",
    "def pick_raw_col(df: pd.DataFrame, processed_col: str):\n",
    "    # Prefer a less-processed version for uppercase and punctuation stats\n",
    "    for c in [\"text_nohtml\", \"text\", \"message\", \"body\", \"content\"]:\n",
    "        if c in df.columns and c != processed_col:\n",
    "            return c\n",
    "    return processed_col\n",
    "\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "IronHack1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
