{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1000, 2)\n",
      "Test data shape: (250, 1)\n",
      "Training Data Columns:\n",
      "Index(['text', 'label'], dtype='object')\n",
      "Test Data Columns:\n",
      "Index(['text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Read Data \n",
    "\n",
    "# Load the datasets\n",
    "training_data = pd.read_csv(r\"C:\\Users\\sombe\\Downloads\\kg_train.csv\")\n",
    "test_data = pd.read_csv(r\"C:\\Users\\sombe\\Downloads\\kg_test.csv\")\n",
    "\n",
    "# Subset the datasets\n",
    "training_data = training_data.head(1000)  # Use the first 1000 rows for training\n",
    "test_data = test_data.head(250)  # Use the first 250 rows for testing\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Training data shape:\", training_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "print(\"Training Data Columns:\")\n",
    "print(training_data.columns)\n",
    "\n",
    "print(\"Test Data Columns:\")\n",
    "print(test_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def add_spaces_around_special_chars(text):\n",
    "    \"\"\"\n",
    "    Adds spaces around special characters to ensure proper tokenization.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'([.,!?;()=<>-])', r' \\1 ', text)  # Add spaces around punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (First 5 Rows):\n",
      "                                                text  label\n",
      "0  DEAR SIR , STRICTLY PRIVATE BUSINESS PROPOSAL ...      1\n",
      "1                                                  .      0\n",
      "2  Nora - - Cheryl emailed dozens memos Haiti wee...      0\n",
      "3  Dear Sir = 2FMadam = 2C know proposal might su...      1\n",
      "4                                                fyi      0\n",
      "\n",
      "Test Data (First 5 Rows):\n",
      "                                                text\n",
      "0  usiness fact deceased man foreigner not = auth...\n",
      "1  happy adjust afternoon . going suggest 3:00pm ...\n",
      "2  Lael Brainard confirmed 78 - 19 afternoon . Mi...\n",
      "3  H < hrod17@clintonemail . com > Friday March 2...\n",
      "4  n ; \" > Dear Good Friend , < br > < br > < br ...\n"
     ]
    }
   ],
   "source": [
    "# Apply to both datasets\n",
    "training_data['text'] = training_data['text'].astype(str).apply(add_spaces_around_special_chars)\n",
    "test_data['text'] = test_data['text'].astype(str).apply(add_spaces_around_special_chars)\n",
    "\n",
    "# Confirm changes\n",
    "print(\"Training Data (First 5 Rows):\")\n",
    "print(training_data.head())\n",
    "\n",
    "print(\"\\nTest Data (First 5 Rows):\")\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "# Step 1: Remove inline JavaScript and CSS\n",
    "def remove_js_css(html_content):\n",
    "    return re.sub(r'<(script|style).*?>.*?</\\1>', '', html_content, flags=re.DOTALL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Remove HTML comments\n",
    "def remove_html_comments(html_content):\n",
    "    return re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Remove all remaining HTML tags\n",
    "def remove_html_tags(html_content):\n",
    "    return re.sub(r'<[^>]+>', '', html_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Decode HTML entities and clean spaces\n",
    "def clean_text(html_content):\n",
    "    text = unescape(html_content)  # Decode HTML entities (e.g., &amp; → &, &lt; → <)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Columns:\n",
      "Index(['text', 'label'], dtype='object')\n",
      "Test Data Columns:\n",
      "Index(['text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Columns:\")\n",
    "print(training_data.columns)\n",
    "\n",
    "print(\"Test Data Columns:\")\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (First 5 Rows):\n",
      "                                                text  label\n",
      "0  dear sir strictly private business proposal mi...      1\n",
      "1                                                         0\n",
      "2  nora cheryl emailed dozens memos haiti weekend...      0\n",
      "3  dear sir fmadam know proposal might surprise e...      1\n",
      "4                                                fyi      0\n",
      "\n",
      "Test Data (First 5 Rows):\n",
      "                                                text\n",
      "0  usiness fact deceased man foreigner authorized...\n",
      "1  happy adjust afternoon going suggest pm start ...\n",
      "2  lael brainard confirmed afternoon miguel rodri...\n",
      "3  hrodclintonemail com friday march amsbwhoeop r...\n",
      "4  dear good friend br br br happy inform succe s...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Performs multiple text preprocessing steps in a single function.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Step 1: Remove special characters (except letters, numbers, and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Step 2: Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Step 3: Remove all single characters\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)\n",
    "\n",
    "    # Step 4: Remove single characters at the start of the text\n",
    "    text = re.sub(r'^[a-zA-Z]\\s+', '', text)\n",
    "\n",
    "    # Step 5: Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Step 6: Remove prefixed 'b' (e.g., from byte conversion artifacts)\n",
    "    text = re.sub(r'\\bb\\s+', '', text)\n",
    "\n",
    "    # Step 7: Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Ensure text column is a string before applying the function\n",
    "training_data['text'] = training_data['text'].astype(str).apply(clean_text)\n",
    "test_data['text'] = test_data['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Confirm changes\n",
    "print(\"Training Data (First 5 Rows):\")\n",
    "print(training_data.head())\n",
    "\n",
    "print(\"\\nTest Data (First 5 Rows):\")\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Columns:\n",
      "Index(['text', 'label'], dtype='object')\n",
      "Test Data Columns:\n",
      "Index(['text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Columns:\")\n",
    "print(training_data.columns)\n",
    "\n",
    "print(\"Test Data Columns:\")\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Tokenize text into words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)  # Join back into a single string\n",
    "\n",
    "# Apply stopword removal to both datasets\n",
    "training_data['text'] = training_data['text'].apply(remove_stopwords)\n",
    "test_data['text'] = test_data['text'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (First 5 Rows):\n",
      "                                                text  label\n",
      "0  dear sir strictly private business proposal mi...      1\n",
      "1                                                         0\n",
      "2  nora cheryl emailed dozens memos haiti weekend...      0\n",
      "3  dear sir fmadam know proposal might surprise e...      1\n",
      "4                                                fyi      0\n",
      "\n",
      "Test Data (First 5 Rows):\n",
      "                                                text\n",
      "0  usiness fact deceased man foreigner authorized...\n",
      "1  happy adjust afternoon going suggest pm start ...\n",
      "2  lael brainard confirmed afternoon miguel rodri...\n",
      "3  hrodclintonemail com friday march amsbwhoeop r...\n",
      "4  dear good friend br br br happy inform succe s...\n"
     ]
    }
   ],
   "source": [
    "# Confirm changes\n",
    "print(\"Training Data (First 5 Rows):\")\n",
    "print(training_data.head())\n",
    "\n",
    "print(\"\\nTest Data (First 5 Rows):\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download resources if not already installed\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\" Map POS tag to first character for lemmatizer \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if tag not found\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\" Tokenize text and apply lemmatization \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)  # Tokenize sentence into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\sombe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')  # All to resolve the error of nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (First 5 Rows):\n",
      "                                                text  label\n",
      "0  dear sir strictly private business proposal mi...      1\n",
      "1                                                         0\n",
      "2  nora cheryl email dozen memo haiti weekend ple...      0\n",
      "3  dear sir fmadam know proposal might surprise e...      1\n",
      "4                                                fyi      0\n",
      "\n",
      "Test Data (First 5 Rows):\n",
      "                                                text\n",
      "0  usiness fact decease man foreigner authorize l...\n",
      "1  happy adjust afternoon go suggest pm start tim...\n",
      "2  lael brainard confirm afternoon miguel rodrigu...\n",
      "3  hrodclintonemail com friday march amsbwhoeop r...\n",
      "4  dear good friend br br br happy inform succe s...\n"
     ]
    }
   ],
   "source": [
    "# Apply lemmatization to both datasets\n",
    "training_data['text'] = training_data['text'].astype(str).apply(lemmatize_text)\n",
    "test_data['text'] = test_data['text'].astype(str).apply(lemmatize_text)\n",
    "\n",
    "# Confirm changes\n",
    "print(\"Training Data (First 5 Rows):\")\n",
    "print(training_data.head())\n",
    "\n",
    "print(\"\\nTest Data (First 5 Rows):\")\n",
    "print(test_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  dear sir strictly private business proposal mi...\n",
      "1                                                   \n",
      "2  nora cheryl email dozen memo haiti weekend ple...\n",
      "3  dear sir fmadam know proposal might surprise e...\n",
      "4                                                fyi\n",
      "5  sure bottom line need special security code ge...\n",
      "6  dear sir engr ugo nzego engineering store depa...\n",
      "7  abedin huma abedinhstate gov saturday november...\n",
      "8  oct th george marshall event department tentat...\n",
      "9  account owner br colleague br set aside defray...\n",
      "                                                text\n",
      "0  usiness fact decease man foreigner authorize l...\n",
      "1  happy adjust afternoon go suggest pm start tim...\n",
      "2  lael brainard confirm afternoon miguel rodrigu...\n",
      "3  hrodclintonemail com friday march amsbwhoeop r...\n",
      "4  dear good friend br br br happy inform succe s...\n",
      "5  faithfully cdr abdul siop edirector fgeneral e...\n",
      "6  need time thursday friday see holbrooke mark h...\n",
      "7  div dear div div sorry hear news understand fa...\n",
      "8  dr ea smith natwest bank plc frith street clon...\n",
      "9  dear friend political unrest country philippin...\n"
     ]
    }
   ],
   "source": [
    "print(training_data[['text']].head(10))\n",
    "print(test_data[['text']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\sombe\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\sombe\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sombe\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sombe\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sombe\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# Separate ham and spam messages\n",
    "ham_messages = training_data[training_data['label'] == 0]['text']\n",
    "spam_messages = training_data[training_data['label'] == 1]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Ham Messages:\n",
      " 1                                                     \n",
      "2    nora cheryl email dozen memo haiti weekend ple...\n",
      "4                                                  fyi\n",
      "5    sure bottom line need special security code ge...\n",
      "7    abedin huma abedinhstate gov saturday november...\n",
      "Name: text, dtype: object\n",
      "Sample Spam Messages:\n",
      " 0     dear sir strictly private business proposal mi...\n",
      "3     dear sir fmadam know proposal might surprise e...\n",
      "6     dear sir engr ugo nzego engineering store depa...\n",
      "9     account owner br colleague br set aside defray...\n",
      "10    strong href dhttpwww cnn comworldafricakenya c...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Ham Messages:\\n\", ham_messages.head())\n",
    "print(\"Sample Spam Messages:\\n\", spam_messages.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(text_series, n=10):\n",
    "    print(\"Number of messages in input:\", len(text_series))  # Check input size\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english')  # Remove stopwords\n",
    "    X = vectorizer.fit_transform(text_series)\n",
    "    \n",
    "    if X.shape[0] == 0:  # Check if the output is empty\n",
    "        print(\"No words found after vectorization.\")\n",
    "        return []\n",
    "\n",
    "    # Sum word occurrences\n",
    "    word_counts = X.toarray().sum(axis=0)\n",
    "\n",
    "    # Get words and their frequencies\n",
    "    word_freq = dict(zip(vectorizer.get_feature_names_out(), word_counts))\n",
    "\n",
    "    # Get the top N words\n",
    "    top_words = Counter(word_freq).most_common(n)\n",
    "\n",
    "    print(\"\\n**Top Words Found:**\", top_words)  # Debugging output\n",
    "    return top_words\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of messages in input: 558\n",
      "\n",
      "📌 **Top Words Found:** [('pm', 116), ('state', 111), ('say', 109), ('work', 107), ('president', 99), ('time', 94), ('mr', 90), ('percent', 81), ('secretary', 77), ('obama', 64)]\n",
      "Number of messages in input: 442\n",
      "\n",
      "📌 **Top Words Found:** [('br', 1102), ('money', 986), ('account', 897), ('bank', 801), ('fund', 782), ('transfer', 573), ('transaction', 551), ('country', 511), ('business', 510), ('mr', 480)]\n",
      "\n",
      "📌 **Top 10 Words in Ham Messages:**\n",
      "[('pm', 116), ('state', 111), ('say', 109), ('work', 107), ('president', 99), ('time', 94), ('mr', 90), ('percent', 81), ('secretary', 77), ('obama', 64)]\n",
      "\n",
      "📌 **Top 10 Words in Spam Messages:**\n",
      "[('br', 1102), ('money', 986), ('account', 897), ('bank', 801), ('fund', 782), ('transfer', 573), ('transaction', 551), ('country', 511), ('business', 510), ('mr', 480)]\n"
     ]
    }
   ],
   "source": [
    "top_ham_words = get_top_words(ham_messages)\n",
    "top_spam_words = get_top_words(spam_messages)\n",
    "\n",
    "print(\"\\n📌 **Top 10 Words in Ham Messages:**\")\n",
    "print(top_ham_words)\n",
    "\n",
    "print(\"\\n📌 **Top 10 Words in Spam Messages:**\")\n",
    "print(top_spam_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  money_mark  \\\n",
      "0  dear sir strictly private business proposal mi...           1   \n",
      "1                                                              1   \n",
      "2  nora cheryl email dozen memo haiti weekend ple...           1   \n",
      "3  dear sir fmadam know proposal might surprise e...           1   \n",
      "4                                                fyi           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 1      1460  \n",
      "1                 0         0  \n",
      "2                 0       109  \n",
      "3                 1      1315  \n",
      "4                 0         3  \n",
      "                                                text  money_mark  \\\n",
      "0  usiness fact decease man foreigner authorize l...           1   \n",
      "1  happy adjust afternoon go suggest pm start tim...           1   \n",
      "2  lael brainard confirm afternoon miguel rodrigu...           1   \n",
      "3  hrodclintonemail com friday march amsbwhoeop r...           1   \n",
      "4  dear good friend br br br happy inform succe s...           1   \n",
      "\n",
      "   suspicious_words  text_len  \n",
      "0                 1       813  \n",
      "1                 0        99  \n",
      "2                 0       154  \n",
      "3                 0        64  \n",
      "4                 1       955  \n"
     ]
    }
   ],
   "source": [
    "money_symbol_list = \"|\".join([\"euro\", \"dollar\", \"pound\", \"€\", \"$\"])\n",
    "suspicious_words = \"|\".join([\"free\", \"cheap\", \"sex\", \"money\", \"account\", \n",
    "                             \"bank\", \"fund\", \"transfer\", \"transaction\", \"win\", \n",
    "                             \"deposit\", \"password\"])\n",
    "\n",
    "# Apply indicators using 'text' column\n",
    "training_data['money_mark'] = training_data['text'].fillna(\"\").str.contains(money_symbol_list).astype(int)\n",
    "training_data['suspicious_words'] = training_data['text'].fillna(\"\").str.contains(suspicious_words).astype(int)\n",
    "training_data['text_len'] = training_data['text'].apply(lambda x: len(str(x))) \n",
    "\n",
    "test_data['money_mark'] = test_data['text'].fillna(\"\").str.contains(money_symbol_list).astype(int)\n",
    "test_data['suspicious_words'] = test_data['text'].fillna(\"\").str.contains(suspicious_words).astype(int)\n",
    "test_data['text_len'] = test_data['text'].apply(lambda x: len(str(x))) \n",
    "\n",
    "# Confirm the changes\n",
    "print(training_data[['text', 'money_mark', 'suspicious_words', 'text_len']].head())\n",
    "print(test_data[['text', 'money_mark', 'suspicious_words', 'text_len']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (1000, 15092)\n",
      "\n",
      "Bag of Words Matrix (First 5 Rows):\n",
      "   aa  aaccount  aaffed  aag  aagre  aand  aaronovitchon  ab  abacha  \\\n",
      "0   0         0       0    0      0     0              0   0       0   \n",
      "1   0         0       0    0      0     0              0   0       0   \n",
      "2   0         0       0    0      0     0              0   0       0   \n",
      "3   0         0       0    0      0     0              0   0       0   \n",
      "4   0         0       0    0      0     0              0   0       0   \n",
      "\n",
      "   abachaco  ...  zona  zone  zong  zongothe  zuhair  zulato  zuma  \\\n",
      "0         0  ...     0     0     0         0       0       0     0   \n",
      "1         0  ...     0     0     0         0       0       0     0   \n",
      "2         0  ...     0     0     0         0       0       0     0   \n",
      "3         0  ...     0     0     0         0       0       0     3   \n",
      "4         0  ...     0     0     0         0       0       0     0   \n",
      "\n",
      "   zumadirector  zurich  zwallet  \n",
      "0             0       0        0  \n",
      "1             0       0        0  \n",
      "2             0       0        0  \n",
      "3             0       0        0  \n",
      "4             0       0        0  \n",
      "\n",
      "[5 rows x 15092 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "# Initialize CountVectorizer (removes English stopwords)\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the text data into a matrix\n",
    "X_train = vectorizer.fit_transform(training_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])  # Transform test data using the same vocabulary\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "bow_train_df = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the shape of the matrix (rows = messages, columns = unique words)\n",
    "print(\"Training Data Shape:\", X_train.shape)  # Example: (5000, 3000) → 5000 messages, 3000 unique words\n",
    "print(\"\\nBag of Words Matrix (First 5 Rows):\")\n",
    "print(bow_train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF Vectorizer (removes stopwords)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(training_data['text'])\n",
    "\n",
    "# Transform the test data using the same vectorizer (do not fit again)\n",
    "X_test_tfidf = vectorizer.transform(test_data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data (TF-IDF): (1000, 15092)\n",
      "Shape of Test Data (TF-IDF): (250, 15092)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training Data (TF-IDF):\", X_train_tfidf.shape)\n",
    "print(\"Shape of Test Data (TF-IDF):\", X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (800, 15092)\n",
      "Validation Data Shape: (200, 15092)\n"
     ]
    }
   ],
   "source": [
    "# Define features (TF-IDF matrix) and labels (Spam=1, Ham=0)\n",
    "X = X_train_tfidf  # Already vectorized training data\n",
    "y = training_data['label']  # Spam = 1, Ham = 0\n",
    "\n",
    "# Split data into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Validation Data Shape:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Naive Bayes Classifier (Good for Text Classification)\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "y_pred = model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 0.955\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       112\n",
      "           1       0.91      1.00      0.95        88\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.95      0.96      0.95       200\n",
      "weighted avg       0.96      0.95      0.96       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"\\nModel Accuracy:\", accuracy)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
